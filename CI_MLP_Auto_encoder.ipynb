{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsQuS6yJTj5bZI/8bXgBsE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elman295/Deep-Learning-Course-INFO/blob/main/CI_MLP_Auto_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#`MNIST classification with MLP`"
      ],
      "metadata": {
        "id": "3chJexYgQVp4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S3qDocwY-OS4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = datasets.MNIST(\n",
        "\n",
        "     root = \"data\",\n",
        "     train = True,\n",
        "     download = True,\n",
        "     transform = transforms.ToTensor()\n",
        ")\n",
        "\n",
        "test_ds = datasets.MNIST(\n",
        "\n",
        "     root = \"data\",\n",
        "     download = True,\n",
        "     train = False,\n",
        "     transform = transforms.ToTensor()\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tL4nTYUsQnFc",
        "outputId": "1e0539b9-17de-4e27-84dc-4b801b8f636d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 149784599.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 62057220.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 46207290.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11854716.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_ds[0]\n",
        "\n",
        "print(image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PoNdsCsRebZ",
        "outputId": "c508b48f-a5f0-4115-e4ab-4c07f8ee3a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.reshape(28,28))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "VYe9fcgDRlim",
        "outputId": "d8475131-fbd6-40c5-87c0-828b97b918a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DataLoader(train_ds, batch_size = 256, shuffle = True)\n",
        "\n",
        "test_dl = DataLoader(test_ds, batch_size = 256, shuffle = False)"
      ],
      "metadata": {
        "id": "G2tu6ULwSCp9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl.dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4CHZpvISWtE",
        "outputId": "66ed8c4e-a8d9-43e5-b8a4-8b9604de06e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = next(iter(train_dl))\n",
        "\n",
        "outs = torchvision.utils.make_grid(images)\n",
        "\n",
        "plt.imshow(np.transpose(outs, (1,2,0)))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "UKH_hKdsRrOA",
        "outputId": "907d896e-5799-4497-bd6f-5ed765479134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "  def __init__(self, output_node):\n",
        "    super(MLP, self).__init__()\n",
        "\n",
        "    self.linear_1 = nn.Linear(1*28*28, 512)\n",
        "    self.linear_2 = nn.Linear(512, output_node)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view((-1,1*28*28))\n",
        "    x = F.relu(self.linear_1(x))\n",
        "    x = self.linear_2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "9mf1WwR5R_oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLP(10)"
      ],
      "metadata": {
        "id": "gz34g9lgTHRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(mlp, input_size=(1,28,28), batch_size=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3nhVbgTTPQQ",
        "outputId": "49fdb35d-5fd7-4f77-8c91-e801edca80e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [256, 512]         401,920\n",
            "            Linear-2                  [256, 10]           5,130\n",
            "================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.77\n",
            "Forward/backward pass size (MB): 1.02\n",
            "Params size (MB): 1.55\n",
            "Estimated Total Size (MB): 3.34\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(Data, Model, Loss_fn, Optimizer):\n",
        "\n",
        "  size = len(Data.dataset)\n",
        "\n",
        "  Model.train()\n",
        "\n",
        "  for batch, (x,y) in enumerate(Data):\n",
        "    y_pred = Model(x)\n",
        "    loss = Loss_fn(y_pred, y)\n",
        "    Optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    Optimizer.step()\n",
        "\n",
        "    if size % 100 ==0 :\n",
        "      loss, current = loss.item(), batch * len(x)\n",
        "      print(f\"Loss: {loss} [{current} / {size}]\")"
      ],
      "metadata": {
        "id": "2mjZFVVeTWCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(Data, Model, Loss_fn):\n",
        "\n",
        "  size = len(Data.dataset)\n",
        "  num_batch = len(Data)\n",
        "  correct, test_loss = 0,0\n",
        "  Model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x,y in Data:\n",
        "      y_pred = Model(x)\n",
        "      test_loss += Loss_fn(y_pred, y).item()\n",
        "      correct += (y_pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batch\n",
        "    correct /= size\n",
        "    print(f\"Test Loss: {test_loss} Accuracy:{correct * 100}\")"
      ],
      "metadata": {
        "id": "1m03dXWHT-Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "opt = torch.optim.SGD(params = mlp.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "8y6eSuUaU2No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"epoch:{epoch+1}=-=-=-=-=-=--=-==--=-=-=\")\n",
        "  train(train_dl, mlp,loss_fn, opt)\n",
        "  test(test_dl, mlp, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCe2FYngUrXi",
        "outputId": "402f4563-9420-4847-9e4a-97fa8b4e0529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 2.3129875659942627 [0 / 60000]\n",
            "Loss: 2.303269624710083 [256 / 60000]\n",
            "Loss: 2.3018877506256104 [512 / 60000]\n",
            "Loss: 2.310478448867798 [768 / 60000]\n",
            "Loss: 2.303879499435425 [1024 / 60000]\n",
            "Loss: 2.3041343688964844 [1280 / 60000]\n",
            "Loss: 2.3070945739746094 [1536 / 60000]\n",
            "Loss: 2.3076682090759277 [1792 / 60000]\n",
            "Loss: 2.304940938949585 [2048 / 60000]\n",
            "Loss: 2.3043646812438965 [2304 / 60000]\n",
            "Loss: 2.2926948070526123 [2560 / 60000]\n",
            "Loss: 2.3093550205230713 [2816 / 60000]\n",
            "Loss: 2.307098150253296 [3072 / 60000]\n",
            "Loss: 2.299382448196411 [3328 / 60000]\n",
            "Loss: 2.305635690689087 [3584 / 60000]\n",
            "Loss: 2.305169105529785 [3840 / 60000]\n",
            "Loss: 2.2978084087371826 [4096 / 60000]\n",
            "Loss: 2.3029468059539795 [4352 / 60000]\n",
            "Loss: 2.299638509750366 [4608 / 60000]\n",
            "Loss: 2.3138034343719482 [4864 / 60000]\n",
            "Loss: 2.307345390319824 [5120 / 60000]\n",
            "Loss: 2.301560878753662 [5376 / 60000]\n",
            "Loss: 2.306701421737671 [5632 / 60000]\n",
            "Loss: 2.2958359718322754 [5888 / 60000]\n",
            "Loss: 2.3016998767852783 [6144 / 60000]\n",
            "Loss: 2.2992091178894043 [6400 / 60000]\n",
            "Loss: 2.2985997200012207 [6656 / 60000]\n",
            "Loss: 2.295891523361206 [6912 / 60000]\n",
            "Loss: 2.299238681793213 [7168 / 60000]\n",
            "Loss: 2.300830602645874 [7424 / 60000]\n",
            "Loss: 2.2961225509643555 [7680 / 60000]\n",
            "Loss: 2.3011207580566406 [7936 / 60000]\n",
            "Loss: 2.296769857406616 [8192 / 60000]\n",
            "Loss: 2.2997939586639404 [8448 / 60000]\n",
            "Loss: 2.302497625350952 [8704 / 60000]\n",
            "Loss: 2.291977643966675 [8960 / 60000]\n",
            "Loss: 2.297630548477173 [9216 / 60000]\n",
            "Loss: 2.2926456928253174 [9472 / 60000]\n",
            "Loss: 2.3025314807891846 [9728 / 60000]\n",
            "Loss: 2.306816577911377 [9984 / 60000]\n",
            "Loss: 2.2936627864837646 [10240 / 60000]\n",
            "Loss: 2.2959914207458496 [10496 / 60000]\n",
            "Loss: 2.290910005569458 [10752 / 60000]\n",
            "Loss: 2.2985293865203857 [11008 / 60000]\n",
            "Loss: 2.289613962173462 [11264 / 60000]\n",
            "Loss: 2.2944960594177246 [11520 / 60000]\n",
            "Loss: 2.289017915725708 [11776 / 60000]\n",
            "Loss: 2.295187473297119 [12032 / 60000]\n",
            "Loss: 2.2926793098449707 [12288 / 60000]\n",
            "Loss: 2.2915854454040527 [12544 / 60000]\n",
            "Loss: 2.2930963039398193 [12800 / 60000]\n",
            "Loss: 2.288342237472534 [13056 / 60000]\n",
            "Loss: 2.295499324798584 [13312 / 60000]\n",
            "Loss: 2.296971559524536 [13568 / 60000]\n",
            "Loss: 2.284531593322754 [13824 / 60000]\n",
            "Loss: 2.295620918273926 [14080 / 60000]\n",
            "Loss: 2.2988617420196533 [14336 / 60000]\n",
            "Loss: 2.2864036560058594 [14592 / 60000]\n",
            "Loss: 2.2928197383880615 [14848 / 60000]\n",
            "Loss: 2.2896337509155273 [15104 / 60000]\n",
            "Loss: 2.279437303543091 [15360 / 60000]\n",
            "Loss: 2.2880821228027344 [15616 / 60000]\n",
            "Loss: 2.288318395614624 [15872 / 60000]\n",
            "Loss: 2.2952306270599365 [16128 / 60000]\n",
            "Loss: 2.287097930908203 [16384 / 60000]\n",
            "Loss: 2.2882206439971924 [16640 / 60000]\n",
            "Loss: 2.290985345840454 [16896 / 60000]\n",
            "Loss: 2.287996530532837 [17152 / 60000]\n",
            "Loss: 2.285097360610962 [17408 / 60000]\n",
            "Loss: 2.2827587127685547 [17664 / 60000]\n",
            "Loss: 2.279869318008423 [17920 / 60000]\n",
            "Loss: 2.2775750160217285 [18176 / 60000]\n",
            "Loss: 2.289520502090454 [18432 / 60000]\n",
            "Loss: 2.2806127071380615 [18688 / 60000]\n",
            "Loss: 2.289604902267456 [18944 / 60000]\n",
            "Loss: 2.2774658203125 [19200 / 60000]\n",
            "Loss: 2.29341459274292 [19456 / 60000]\n",
            "Loss: 2.2884602546691895 [19712 / 60000]\n",
            "Loss: 2.2935564517974854 [19968 / 60000]\n",
            "Loss: 2.2889773845672607 [20224 / 60000]\n",
            "Loss: 2.2877914905548096 [20480 / 60000]\n",
            "Loss: 2.2844901084899902 [20736 / 60000]\n",
            "Loss: 2.284916400909424 [20992 / 60000]\n",
            "Loss: 2.2799124717712402 [21248 / 60000]\n",
            "Loss: 2.2822265625 [21504 / 60000]\n",
            "Loss: 2.2899327278137207 [21760 / 60000]\n",
            "Loss: 2.284633159637451 [22016 / 60000]\n",
            "Loss: 2.2863986492156982 [22272 / 60000]\n",
            "Loss: 2.286747932434082 [22528 / 60000]\n",
            "Loss: 2.2882893085479736 [22784 / 60000]\n",
            "Loss: 2.276500940322876 [23040 / 60000]\n",
            "Loss: 2.2848968505859375 [23296 / 60000]\n",
            "Loss: 2.282540798187256 [23552 / 60000]\n",
            "Loss: 2.279548168182373 [23808 / 60000]\n",
            "Loss: 2.277045965194702 [24064 / 60000]\n",
            "Loss: 2.27900767326355 [24320 / 60000]\n",
            "Loss: 2.2787487506866455 [24576 / 60000]\n",
            "Loss: 2.2792980670928955 [24832 / 60000]\n",
            "Loss: 2.282867431640625 [25088 / 60000]\n",
            "Loss: 2.281160354614258 [25344 / 60000]\n",
            "Loss: 2.2801177501678467 [25600 / 60000]\n",
            "Loss: 2.280893087387085 [25856 / 60000]\n",
            "Loss: 2.2761735916137695 [26112 / 60000]\n",
            "Loss: 2.2737619876861572 [26368 / 60000]\n",
            "Loss: 2.2783496379852295 [26624 / 60000]\n",
            "Loss: 2.2825746536254883 [26880 / 60000]\n",
            "Loss: 2.27860689163208 [27136 / 60000]\n",
            "Loss: 2.282620906829834 [27392 / 60000]\n",
            "Loss: 2.2781920433044434 [27648 / 60000]\n",
            "Loss: 2.2816243171691895 [27904 / 60000]\n",
            "Loss: 2.2801504135131836 [28160 / 60000]\n",
            "Loss: 2.278698205947876 [28416 / 60000]\n",
            "Loss: 2.270125389099121 [28672 / 60000]\n",
            "Loss: 2.278627395629883 [28928 / 60000]\n",
            "Loss: 2.282278537750244 [29184 / 60000]\n",
            "Loss: 2.2809066772460938 [29440 / 60000]\n",
            "Loss: 2.2810864448547363 [29696 / 60000]\n",
            "Loss: 2.2760140895843506 [29952 / 60000]\n",
            "Loss: 2.2748565673828125 [30208 / 60000]\n",
            "Loss: 2.2754344940185547 [30464 / 60000]\n",
            "Loss: 2.2814722061157227 [30720 / 60000]\n",
            "Loss: 2.2752857208251953 [30976 / 60000]\n",
            "Loss: 2.272066354751587 [31232 / 60000]\n",
            "Loss: 2.2788820266723633 [31488 / 60000]\n",
            "Loss: 2.2886271476745605 [31744 / 60000]\n",
            "Loss: 2.2815237045288086 [32000 / 60000]\n",
            "Loss: 2.27842378616333 [32256 / 60000]\n",
            "Loss: 2.280841827392578 [32512 / 60000]\n",
            "Loss: 2.2777633666992188 [32768 / 60000]\n",
            "Loss: 2.2761363983154297 [33024 / 60000]\n",
            "Loss: 2.2754576206207275 [33280 / 60000]\n",
            "Loss: 2.2739689350128174 [33536 / 60000]\n",
            "Loss: 2.2815887928009033 [33792 / 60000]\n",
            "Loss: 2.2722129821777344 [34048 / 60000]\n",
            "Loss: 2.269976854324341 [34304 / 60000]\n",
            "Loss: 2.2736005783081055 [34560 / 60000]\n",
            "Loss: 2.271430015563965 [34816 / 60000]\n",
            "Loss: 2.2710070610046387 [35072 / 60000]\n",
            "Loss: 2.276042938232422 [35328 / 60000]\n",
            "Loss: 2.2702534198760986 [35584 / 60000]\n",
            "Loss: 2.270137310028076 [35840 / 60000]\n",
            "Loss: 2.2772624492645264 [36096 / 60000]\n",
            "Loss: 2.2726824283599854 [36352 / 60000]\n",
            "Loss: 2.2687389850616455 [36608 / 60000]\n",
            "Loss: 2.2722601890563965 [36864 / 60000]\n",
            "Loss: 2.275667905807495 [37120 / 60000]\n",
            "Loss: 2.2711808681488037 [37376 / 60000]\n",
            "Loss: 2.2729337215423584 [37632 / 60000]\n",
            "Loss: 2.266139030456543 [37888 / 60000]\n",
            "Loss: 2.27154541015625 [38144 / 60000]\n",
            "Loss: 2.2704062461853027 [38400 / 60000]\n",
            "Loss: 2.263521909713745 [38656 / 60000]\n",
            "Loss: 2.2670156955718994 [38912 / 60000]\n",
            "Loss: 2.2758123874664307 [39168 / 60000]\n",
            "Loss: 2.2715039253234863 [39424 / 60000]\n",
            "Loss: 2.270172119140625 [39680 / 60000]\n",
            "Loss: 2.273912191390991 [39936 / 60000]\n",
            "Loss: 2.2625532150268555 [40192 / 60000]\n",
            "Loss: 2.2625372409820557 [40448 / 60000]\n",
            "Loss: 2.2709619998931885 [40704 / 60000]\n",
            "Loss: 2.2741658687591553 [40960 / 60000]\n",
            "Loss: 2.2707834243774414 [41216 / 60000]\n",
            "Loss: 2.2752060890197754 [41472 / 60000]\n",
            "Loss: 2.2658307552337646 [41728 / 60000]\n",
            "Loss: 2.256382703781128 [41984 / 60000]\n",
            "Loss: 2.269369602203369 [42240 / 60000]\n",
            "Loss: 2.263861656188965 [42496 / 60000]\n",
            "Loss: 2.2606117725372314 [42752 / 60000]\n",
            "Loss: 2.2644290924072266 [43008 / 60000]\n",
            "Loss: 2.2686100006103516 [43264 / 60000]\n",
            "Loss: 2.2668135166168213 [43520 / 60000]\n",
            "Loss: 2.268610954284668 [43776 / 60000]\n",
            "Loss: 2.2652761936187744 [44032 / 60000]\n",
            "Loss: 2.265794277191162 [44288 / 60000]\n",
            "Loss: 2.26692795753479 [44544 / 60000]\n",
            "Loss: 2.2620108127593994 [44800 / 60000]\n",
            "Loss: 2.263253927230835 [45056 / 60000]\n",
            "Loss: 2.267778158187866 [45312 / 60000]\n",
            "Loss: 2.264890432357788 [45568 / 60000]\n",
            "Loss: 2.2683072090148926 [45824 / 60000]\n",
            "Loss: 2.26716947555542 [46080 / 60000]\n",
            "Loss: 2.264392375946045 [46336 / 60000]\n",
            "Loss: 2.2655539512634277 [46592 / 60000]\n",
            "Loss: 2.26570463180542 [46848 / 60000]\n",
            "Loss: 2.2620952129364014 [47104 / 60000]\n",
            "Loss: 2.267265796661377 [47360 / 60000]\n",
            "Loss: 2.2649571895599365 [47616 / 60000]\n",
            "Loss: 2.2623279094696045 [47872 / 60000]\n",
            "Loss: 2.259885787963867 [48128 / 60000]\n",
            "Loss: 2.2571921348571777 [48384 / 60000]\n",
            "Loss: 2.2578511238098145 [48640 / 60000]\n",
            "Loss: 2.259615659713745 [48896 / 60000]\n",
            "Loss: 2.2651824951171875 [49152 / 60000]\n",
            "Loss: 2.2626326084136963 [49408 / 60000]\n",
            "Loss: 2.2568533420562744 [49664 / 60000]\n",
            "Loss: 2.2503206729888916 [49920 / 60000]\n",
            "Loss: 2.2662816047668457 [50176 / 60000]\n",
            "Loss: 2.2685251235961914 [50432 / 60000]\n",
            "Loss: 2.2529473304748535 [50688 / 60000]\n",
            "Loss: 2.2631354331970215 [50944 / 60000]\n",
            "Loss: 2.257854461669922 [51200 / 60000]\n",
            "Loss: 2.2570292949676514 [51456 / 60000]\n",
            "Loss: 2.2642529010772705 [51712 / 60000]\n",
            "Loss: 2.2608301639556885 [51968 / 60000]\n",
            "Loss: 2.265885353088379 [52224 / 60000]\n",
            "Loss: 2.2606201171875 [52480 / 60000]\n",
            "Loss: 2.256535530090332 [52736 / 60000]\n",
            "Loss: 2.2643136978149414 [52992 / 60000]\n",
            "Loss: 2.254255771636963 [53248 / 60000]\n",
            "Loss: 2.261333703994751 [53504 / 60000]\n",
            "Loss: 2.266231060028076 [53760 / 60000]\n",
            "Loss: 2.2613136768341064 [54016 / 60000]\n",
            "Loss: 2.2511966228485107 [54272 / 60000]\n",
            "Loss: 2.2551469802856445 [54528 / 60000]\n",
            "Loss: 2.2487566471099854 [54784 / 60000]\n",
            "Loss: 2.248567581176758 [55040 / 60000]\n",
            "Loss: 2.250836133956909 [55296 / 60000]\n",
            "Loss: 2.257993221282959 [55552 / 60000]\n",
            "Loss: 2.253873825073242 [55808 / 60000]\n",
            "Loss: 2.256991386413574 [56064 / 60000]\n",
            "Loss: 2.253392457962036 [56320 / 60000]\n",
            "Loss: 2.254188299179077 [56576 / 60000]\n",
            "Loss: 2.2591447830200195 [56832 / 60000]\n",
            "Loss: 2.2640724182128906 [57088 / 60000]\n",
            "Loss: 2.2547354698181152 [57344 / 60000]\n",
            "Loss: 2.2539286613464355 [57600 / 60000]\n",
            "Loss: 2.2495131492614746 [57856 / 60000]\n",
            "Loss: 2.256408452987671 [58112 / 60000]\n",
            "Loss: 2.262380838394165 [58368 / 60000]\n",
            "Loss: 2.2493484020233154 [58624 / 60000]\n",
            "Loss: 2.2579188346862793 [58880 / 60000]\n",
            "Loss: 2.2536346912384033 [59136 / 60000]\n",
            "Loss: 2.249183177947998 [59392 / 60000]\n",
            "Loss: 2.2643520832061768 [59648 / 60000]\n",
            "Loss: 2.246253252029419 [22464 / 60000]\n",
            "Test Loss: 2.249308097362518 Accuracy:25.259999999999998\n",
            "epoch:2=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 2.250242233276367 [0 / 60000]\n",
            "Loss: 2.26019287109375 [256 / 60000]\n",
            "Loss: 2.2501583099365234 [512 / 60000]\n",
            "Loss: 2.250133514404297 [768 / 60000]\n",
            "Loss: 2.250237226486206 [1024 / 60000]\n",
            "Loss: 2.2454402446746826 [1280 / 60000]\n",
            "Loss: 2.242119550704956 [1536 / 60000]\n",
            "Loss: 2.2434825897216797 [1792 / 60000]\n",
            "Loss: 2.247227907180786 [2048 / 60000]\n",
            "Loss: 2.249342441558838 [2304 / 60000]\n",
            "Loss: 2.2484323978424072 [2560 / 60000]\n",
            "Loss: 2.252765417098999 [2816 / 60000]\n",
            "Loss: 2.2557451725006104 [3072 / 60000]\n",
            "Loss: 2.2455129623413086 [3328 / 60000]\n",
            "Loss: 2.2518274784088135 [3584 / 60000]\n",
            "Loss: 2.252965211868286 [3840 / 60000]\n",
            "Loss: 2.2592380046844482 [4096 / 60000]\n",
            "Loss: 2.2547202110290527 [4352 / 60000]\n",
            "Loss: 2.245013952255249 [4608 / 60000]\n",
            "Loss: 2.2442498207092285 [4864 / 60000]\n",
            "Loss: 2.2472639083862305 [5120 / 60000]\n",
            "Loss: 2.25134539604187 [5376 / 60000]\n",
            "Loss: 2.2405221462249756 [5632 / 60000]\n",
            "Loss: 2.238935708999634 [5888 / 60000]\n",
            "Loss: 2.241925001144409 [6144 / 60000]\n",
            "Loss: 2.2492854595184326 [6400 / 60000]\n",
            "Loss: 2.254164457321167 [6656 / 60000]\n",
            "Loss: 2.2424893379211426 [6912 / 60000]\n",
            "Loss: 2.238539218902588 [7168 / 60000]\n",
            "Loss: 2.24569034576416 [7424 / 60000]\n",
            "Loss: 2.2425756454467773 [7680 / 60000]\n",
            "Loss: 2.237436056137085 [7936 / 60000]\n",
            "Loss: 2.2426981925964355 [8192 / 60000]\n",
            "Loss: 2.240917444229126 [8448 / 60000]\n",
            "Loss: 2.2482452392578125 [8704 / 60000]\n",
            "Loss: 2.2420730590820312 [8960 / 60000]\n",
            "Loss: 2.2431371212005615 [9216 / 60000]\n",
            "Loss: 2.242736339569092 [9472 / 60000]\n",
            "Loss: 2.2358646392822266 [9728 / 60000]\n",
            "Loss: 2.2407026290893555 [9984 / 60000]\n",
            "Loss: 2.2405762672424316 [10240 / 60000]\n",
            "Loss: 2.243858575820923 [10496 / 60000]\n",
            "Loss: 2.236300230026245 [10752 / 60000]\n",
            "Loss: 2.2547688484191895 [11008 / 60000]\n",
            "Loss: 2.238908052444458 [11264 / 60000]\n",
            "Loss: 2.246828317642212 [11520 / 60000]\n",
            "Loss: 2.250678300857544 [11776 / 60000]\n",
            "Loss: 2.2433125972747803 [12032 / 60000]\n",
            "Loss: 2.2421391010284424 [12288 / 60000]\n",
            "Loss: 2.2324025630950928 [12544 / 60000]\n",
            "Loss: 2.235064744949341 [12800 / 60000]\n",
            "Loss: 2.243049144744873 [13056 / 60000]\n",
            "Loss: 2.2438464164733887 [13312 / 60000]\n",
            "Loss: 2.241673469543457 [13568 / 60000]\n",
            "Loss: 2.2438528537750244 [13824 / 60000]\n",
            "Loss: 2.241356134414673 [14080 / 60000]\n",
            "Loss: 2.234700918197632 [14336 / 60000]\n",
            "Loss: 2.241764783859253 [14592 / 60000]\n",
            "Loss: 2.238699197769165 [14848 / 60000]\n",
            "Loss: 2.236567735671997 [15104 / 60000]\n",
            "Loss: 2.231192111968994 [15360 / 60000]\n",
            "Loss: 2.243525743484497 [15616 / 60000]\n",
            "Loss: 2.236309289932251 [15872 / 60000]\n",
            "Loss: 2.2304329872131348 [16128 / 60000]\n",
            "Loss: 2.2324612140655518 [16384 / 60000]\n",
            "Loss: 2.2383534908294678 [16640 / 60000]\n",
            "Loss: 2.240133285522461 [16896 / 60000]\n",
            "Loss: 2.236825942993164 [17152 / 60000]\n",
            "Loss: 2.231107234954834 [17408 / 60000]\n",
            "Loss: 2.234971046447754 [17664 / 60000]\n",
            "Loss: 2.2387208938598633 [17920 / 60000]\n",
            "Loss: 2.2342779636383057 [18176 / 60000]\n",
            "Loss: 2.2303762435913086 [18432 / 60000]\n",
            "Loss: 2.232436180114746 [18688 / 60000]\n",
            "Loss: 2.2317967414855957 [18944 / 60000]\n",
            "Loss: 2.2330522537231445 [19200 / 60000]\n",
            "Loss: 2.238105535507202 [19456 / 60000]\n",
            "Loss: 2.2347424030303955 [19712 / 60000]\n",
            "Loss: 2.2402920722961426 [19968 / 60000]\n",
            "Loss: 2.239302158355713 [20224 / 60000]\n",
            "Loss: 2.229032039642334 [20480 / 60000]\n",
            "Loss: 2.2351632118225098 [20736 / 60000]\n",
            "Loss: 2.2339723110198975 [20992 / 60000]\n",
            "Loss: 2.231851100921631 [21248 / 60000]\n",
            "Loss: 2.225653648376465 [21504 / 60000]\n",
            "Loss: 2.2286553382873535 [21760 / 60000]\n",
            "Loss: 2.223662853240967 [22016 / 60000]\n",
            "Loss: 2.2272133827209473 [22272 / 60000]\n",
            "Loss: 2.2406232357025146 [22528 / 60000]\n",
            "Loss: 2.229705810546875 [22784 / 60000]\n",
            "Loss: 2.234534502029419 [23040 / 60000]\n",
            "Loss: 2.227522373199463 [23296 / 60000]\n",
            "Loss: 2.2268810272216797 [23552 / 60000]\n",
            "Loss: 2.23085355758667 [23808 / 60000]\n",
            "Loss: 2.236610174179077 [24064 / 60000]\n",
            "Loss: 2.233494997024536 [24320 / 60000]\n",
            "Loss: 2.221297264099121 [24576 / 60000]\n",
            "Loss: 2.2354419231414795 [24832 / 60000]\n",
            "Loss: 2.224802255630493 [25088 / 60000]\n",
            "Loss: 2.2336525917053223 [25344 / 60000]\n",
            "Loss: 2.2372772693634033 [25600 / 60000]\n",
            "Loss: 2.235267162322998 [25856 / 60000]\n",
            "Loss: 2.235626220703125 [26112 / 60000]\n",
            "Loss: 2.2341935634613037 [26368 / 60000]\n",
            "Loss: 2.231410026550293 [26624 / 60000]\n",
            "Loss: 2.2302141189575195 [26880 / 60000]\n",
            "Loss: 2.2132339477539062 [27136 / 60000]\n",
            "Loss: 2.231558084487915 [27392 / 60000]\n",
            "Loss: 2.2314984798431396 [27648 / 60000]\n",
            "Loss: 2.2281088829040527 [27904 / 60000]\n",
            "Loss: 2.2243635654449463 [28160 / 60000]\n",
            "Loss: 2.225442886352539 [28416 / 60000]\n",
            "Loss: 2.2295498847961426 [28672 / 60000]\n",
            "Loss: 2.229236364364624 [28928 / 60000]\n",
            "Loss: 2.2336411476135254 [29184 / 60000]\n",
            "Loss: 2.223604679107666 [29440 / 60000]\n",
            "Loss: 2.2277016639709473 [29696 / 60000]\n",
            "Loss: 2.2311935424804688 [29952 / 60000]\n",
            "Loss: 2.226550817489624 [30208 / 60000]\n",
            "Loss: 2.2263808250427246 [30464 / 60000]\n",
            "Loss: 2.2175049781799316 [30720 / 60000]\n",
            "Loss: 2.2146739959716797 [30976 / 60000]\n",
            "Loss: 2.2188103199005127 [31232 / 60000]\n",
            "Loss: 2.2232608795166016 [31488 / 60000]\n",
            "Loss: 2.220255136489868 [31744 / 60000]\n",
            "Loss: 2.211602210998535 [32000 / 60000]\n",
            "Loss: 2.21976375579834 [32256 / 60000]\n",
            "Loss: 2.2147514820098877 [32512 / 60000]\n",
            "Loss: 2.223179578781128 [32768 / 60000]\n",
            "Loss: 2.2166426181793213 [33024 / 60000]\n",
            "Loss: 2.2273786067962646 [33280 / 60000]\n",
            "Loss: 2.220426321029663 [33536 / 60000]\n",
            "Loss: 2.2203917503356934 [33792 / 60000]\n",
            "Loss: 2.2199158668518066 [34048 / 60000]\n",
            "Loss: 2.216902256011963 [34304 / 60000]\n",
            "Loss: 2.2167375087738037 [34560 / 60000]\n",
            "Loss: 2.2257394790649414 [34816 / 60000]\n",
            "Loss: 2.215832471847534 [35072 / 60000]\n",
            "Loss: 2.216583251953125 [35328 / 60000]\n",
            "Loss: 2.2223708629608154 [35584 / 60000]\n",
            "Loss: 2.229968786239624 [35840 / 60000]\n",
            "Loss: 2.2155210971832275 [36096 / 60000]\n",
            "Loss: 2.211581230163574 [36352 / 60000]\n",
            "Loss: 2.2149617671966553 [36608 / 60000]\n",
            "Loss: 2.2187085151672363 [36864 / 60000]\n",
            "Loss: 2.2176308631896973 [37120 / 60000]\n",
            "Loss: 2.226912260055542 [37376 / 60000]\n",
            "Loss: 2.210012435913086 [37632 / 60000]\n",
            "Loss: 2.2172200679779053 [37888 / 60000]\n",
            "Loss: 2.2245945930480957 [38144 / 60000]\n",
            "Loss: 2.212496042251587 [38400 / 60000]\n",
            "Loss: 2.216200351715088 [38656 / 60000]\n",
            "Loss: 2.216217041015625 [38912 / 60000]\n",
            "Loss: 2.2157061100006104 [39168 / 60000]\n",
            "Loss: 2.221790075302124 [39424 / 60000]\n",
            "Loss: 2.2204387187957764 [39680 / 60000]\n",
            "Loss: 2.220412254333496 [39936 / 60000]\n",
            "Loss: 2.2199480533599854 [40192 / 60000]\n",
            "Loss: 2.2146666049957275 [40448 / 60000]\n",
            "Loss: 2.2283852100372314 [40704 / 60000]\n",
            "Loss: 2.2169766426086426 [40960 / 60000]\n",
            "Loss: 2.217100143432617 [41216 / 60000]\n",
            "Loss: 2.2213029861450195 [41472 / 60000]\n",
            "Loss: 2.215972900390625 [41728 / 60000]\n",
            "Loss: 2.206087112426758 [41984 / 60000]\n",
            "Loss: 2.224578857421875 [42240 / 60000]\n",
            "Loss: 2.2072412967681885 [42496 / 60000]\n",
            "Loss: 2.22344708442688 [42752 / 60000]\n",
            "Loss: 2.2162435054779053 [43008 / 60000]\n",
            "Loss: 2.208780527114868 [43264 / 60000]\n",
            "Loss: 2.2125704288482666 [43520 / 60000]\n",
            "Loss: 2.2153091430664062 [43776 / 60000]\n",
            "Loss: 2.211967706680298 [44032 / 60000]\n",
            "Loss: 2.2158968448638916 [44288 / 60000]\n",
            "Loss: 2.208347797393799 [44544 / 60000]\n",
            "Loss: 2.214378833770752 [44800 / 60000]\n",
            "Loss: 2.2090904712677 [45056 / 60000]\n",
            "Loss: 2.2123875617980957 [45312 / 60000]\n",
            "Loss: 2.210716962814331 [45568 / 60000]\n",
            "Loss: 2.2063889503479004 [45824 / 60000]\n",
            "Loss: 2.205339193344116 [46080 / 60000]\n",
            "Loss: 2.2210824489593506 [46336 / 60000]\n",
            "Loss: 2.202864408493042 [46592 / 60000]\n",
            "Loss: 2.207535743713379 [46848 / 60000]\n",
            "Loss: 2.219083309173584 [47104 / 60000]\n",
            "Loss: 2.21126389503479 [47360 / 60000]\n",
            "Loss: 2.2175886631011963 [47616 / 60000]\n",
            "Loss: 2.1962270736694336 [47872 / 60000]\n",
            "Loss: 2.205650568008423 [48128 / 60000]\n",
            "Loss: 2.2091007232666016 [48384 / 60000]\n",
            "Loss: 2.208460807800293 [48640 / 60000]\n",
            "Loss: 2.2107627391815186 [48896 / 60000]\n",
            "Loss: 2.2089738845825195 [49152 / 60000]\n",
            "Loss: 2.2136685848236084 [49408 / 60000]\n",
            "Loss: 2.2121965885162354 [49664 / 60000]\n",
            "Loss: 2.2146213054656982 [49920 / 60000]\n",
            "Loss: 2.2016420364379883 [50176 / 60000]\n",
            "Loss: 2.2068822383880615 [50432 / 60000]\n",
            "Loss: 2.2075984477996826 [50688 / 60000]\n",
            "Loss: 2.196834087371826 [50944 / 60000]\n",
            "Loss: 2.214588165283203 [51200 / 60000]\n",
            "Loss: 2.2002995014190674 [51456 / 60000]\n",
            "Loss: 2.196751594543457 [51712 / 60000]\n",
            "Loss: 2.2128241062164307 [51968 / 60000]\n",
            "Loss: 2.2132277488708496 [52224 / 60000]\n",
            "Loss: 2.208535671234131 [52480 / 60000]\n",
            "Loss: 2.2008233070373535 [52736 / 60000]\n",
            "Loss: 2.207653522491455 [52992 / 60000]\n",
            "Loss: 2.2055299282073975 [53248 / 60000]\n",
            "Loss: 2.2026708126068115 [53504 / 60000]\n",
            "Loss: 2.2010951042175293 [53760 / 60000]\n",
            "Loss: 2.1999757289886475 [54016 / 60000]\n",
            "Loss: 2.207833766937256 [54272 / 60000]\n",
            "Loss: 2.2080368995666504 [54528 / 60000]\n",
            "Loss: 2.1938090324401855 [54784 / 60000]\n",
            "Loss: 2.192110061645508 [55040 / 60000]\n",
            "Loss: 2.1982550621032715 [55296 / 60000]\n",
            "Loss: 2.195927143096924 [55552 / 60000]\n",
            "Loss: 2.2076973915100098 [55808 / 60000]\n",
            "Loss: 2.212113857269287 [56064 / 60000]\n",
            "Loss: 2.2039337158203125 [56320 / 60000]\n",
            "Loss: 2.203397750854492 [56576 / 60000]\n",
            "Loss: 2.191634178161621 [56832 / 60000]\n",
            "Loss: 2.2007575035095215 [57088 / 60000]\n",
            "Loss: 2.1955173015594482 [57344 / 60000]\n",
            "Loss: 2.201026439666748 [57600 / 60000]\n",
            "Loss: 2.208167791366577 [57856 / 60000]\n",
            "Loss: 2.206303119659424 [58112 / 60000]\n",
            "Loss: 2.1969118118286133 [58368 / 60000]\n",
            "Loss: 2.197175979614258 [58624 / 60000]\n",
            "Loss: 2.2147977352142334 [58880 / 60000]\n",
            "Loss: 2.198462724685669 [59136 / 60000]\n",
            "Loss: 2.1977357864379883 [59392 / 60000]\n",
            "Loss: 2.2003724575042725 [59648 / 60000]\n",
            "Loss: 2.2015087604522705 [22464 / 60000]\n",
            "Test Loss: 2.1945539891719816 Accuracy:46.57\n",
            "epoch:3=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 2.2020769119262695 [0 / 60000]\n",
            "Loss: 2.191150665283203 [256 / 60000]\n",
            "Loss: 2.2086498737335205 [512 / 60000]\n",
            "Loss: 2.1967103481292725 [768 / 60000]\n",
            "Loss: 2.196066379547119 [1024 / 60000]\n",
            "Loss: 2.1950440406799316 [1280 / 60000]\n",
            "Loss: 2.1939258575439453 [1536 / 60000]\n",
            "Loss: 2.1946427822113037 [1792 / 60000]\n",
            "Loss: 2.196169853210449 [2048 / 60000]\n",
            "Loss: 2.202082633972168 [2304 / 60000]\n",
            "Loss: 2.1956121921539307 [2560 / 60000]\n",
            "Loss: 2.1989877223968506 [2816 / 60000]\n",
            "Loss: 2.1829116344451904 [3072 / 60000]\n",
            "Loss: 2.204241991043091 [3328 / 60000]\n",
            "Loss: 2.19527530670166 [3584 / 60000]\n",
            "Loss: 2.1922221183776855 [3840 / 60000]\n",
            "Loss: 2.1965739727020264 [4096 / 60000]\n",
            "Loss: 2.197373151779175 [4352 / 60000]\n",
            "Loss: 2.1972718238830566 [4608 / 60000]\n",
            "Loss: 2.1839759349823 [4864 / 60000]\n",
            "Loss: 2.1962175369262695 [5120 / 60000]\n",
            "Loss: 2.190201759338379 [5376 / 60000]\n",
            "Loss: 2.1862735748291016 [5632 / 60000]\n",
            "Loss: 2.1862497329711914 [5888 / 60000]\n",
            "Loss: 2.1974339485168457 [6144 / 60000]\n",
            "Loss: 2.196460008621216 [6400 / 60000]\n",
            "Loss: 2.190363883972168 [6656 / 60000]\n",
            "Loss: 2.1888601779937744 [6912 / 60000]\n",
            "Loss: 2.197944164276123 [7168 / 60000]\n",
            "Loss: 2.208310127258301 [7424 / 60000]\n",
            "Loss: 2.2018916606903076 [7680 / 60000]\n",
            "Loss: 2.191901206970215 [7936 / 60000]\n",
            "Loss: 2.1999387741088867 [8192 / 60000]\n",
            "Loss: 2.1915476322174072 [8448 / 60000]\n",
            "Loss: 2.1925463676452637 [8704 / 60000]\n",
            "Loss: 2.1841542720794678 [8960 / 60000]\n",
            "Loss: 2.191972494125366 [9216 / 60000]\n",
            "Loss: 2.1957309246063232 [9472 / 60000]\n",
            "Loss: 2.196606159210205 [9728 / 60000]\n",
            "Loss: 2.1920509338378906 [9984 / 60000]\n",
            "Loss: 2.178915023803711 [10240 / 60000]\n",
            "Loss: 2.1971921920776367 [10496 / 60000]\n",
            "Loss: 2.1899678707122803 [10752 / 60000]\n",
            "Loss: 2.190066337585449 [11008 / 60000]\n",
            "Loss: 2.189406156539917 [11264 / 60000]\n",
            "Loss: 2.1983275413513184 [11520 / 60000]\n",
            "Loss: 2.186387777328491 [11776 / 60000]\n",
            "Loss: 2.1921963691711426 [12032 / 60000]\n",
            "Loss: 2.1831696033477783 [12288 / 60000]\n",
            "Loss: 2.186394453048706 [12544 / 60000]\n",
            "Loss: 2.1924595832824707 [12800 / 60000]\n",
            "Loss: 2.1899657249450684 [13056 / 60000]\n",
            "Loss: 2.1876513957977295 [13312 / 60000]\n",
            "Loss: 2.1797287464141846 [13568 / 60000]\n",
            "Loss: 2.178988218307495 [13824 / 60000]\n",
            "Loss: 2.190319776535034 [14080 / 60000]\n",
            "Loss: 2.188796281814575 [14336 / 60000]\n",
            "Loss: 2.186166524887085 [14592 / 60000]\n",
            "Loss: 2.177175998687744 [14848 / 60000]\n",
            "Loss: 2.1831579208374023 [15104 / 60000]\n",
            "Loss: 2.1896865367889404 [15360 / 60000]\n",
            "Loss: 2.1791269779205322 [15616 / 60000]\n",
            "Loss: 2.178443670272827 [15872 / 60000]\n",
            "Loss: 2.183913469314575 [16128 / 60000]\n",
            "Loss: 2.1770153045654297 [16384 / 60000]\n",
            "Loss: 2.1730048656463623 [16640 / 60000]\n",
            "Loss: 2.1873769760131836 [16896 / 60000]\n",
            "Loss: 2.181792736053467 [17152 / 60000]\n",
            "Loss: 2.174182176589966 [17408 / 60000]\n",
            "Loss: 2.1795194149017334 [17664 / 60000]\n",
            "Loss: 2.1793949604034424 [17920 / 60000]\n",
            "Loss: 2.1785571575164795 [18176 / 60000]\n",
            "Loss: 2.1824874877929688 [18432 / 60000]\n",
            "Loss: 2.18257212638855 [18688 / 60000]\n",
            "Loss: 2.1766159534454346 [18944 / 60000]\n",
            "Loss: 2.181056022644043 [19200 / 60000]\n",
            "Loss: 2.1818675994873047 [19456 / 60000]\n",
            "Loss: 2.1704952716827393 [19712 / 60000]\n",
            "Loss: 2.1896703243255615 [19968 / 60000]\n",
            "Loss: 2.1861517429351807 [20224 / 60000]\n",
            "Loss: 2.1702098846435547 [20480 / 60000]\n",
            "Loss: 2.1830596923828125 [20736 / 60000]\n",
            "Loss: 2.1758246421813965 [20992 / 60000]\n",
            "Loss: 2.181459665298462 [21248 / 60000]\n",
            "Loss: 2.175678253173828 [21504 / 60000]\n",
            "Loss: 2.1795668601989746 [21760 / 60000]\n",
            "Loss: 2.1829230785369873 [22016 / 60000]\n",
            "Loss: 2.183587074279785 [22272 / 60000]\n",
            "Loss: 2.1777005195617676 [22528 / 60000]\n",
            "Loss: 2.1750710010528564 [22784 / 60000]\n",
            "Loss: 2.1732351779937744 [23040 / 60000]\n",
            "Loss: 2.1672253608703613 [23296 / 60000]\n",
            "Loss: 2.1774280071258545 [23552 / 60000]\n",
            "Loss: 2.184832811355591 [23808 / 60000]\n",
            "Loss: 2.1695363521575928 [24064 / 60000]\n",
            "Loss: 2.1700966358184814 [24320 / 60000]\n",
            "Loss: 2.177398681640625 [24576 / 60000]\n",
            "Loss: 2.1701548099517822 [24832 / 60000]\n",
            "Loss: 2.173858404159546 [25088 / 60000]\n",
            "Loss: 2.17352557182312 [25344 / 60000]\n",
            "Loss: 2.180525064468384 [25600 / 60000]\n",
            "Loss: 2.1788382530212402 [25856 / 60000]\n",
            "Loss: 2.171841621398926 [26112 / 60000]\n",
            "Loss: 2.175142526626587 [26368 / 60000]\n",
            "Loss: 2.1656556129455566 [26624 / 60000]\n",
            "Loss: 2.1792995929718018 [26880 / 60000]\n",
            "Loss: 2.1787383556365967 [27136 / 60000]\n",
            "Loss: 2.159600257873535 [27392 / 60000]\n",
            "Loss: 2.1706488132476807 [27648 / 60000]\n",
            "Loss: 2.175560235977173 [27904 / 60000]\n",
            "Loss: 2.176558256149292 [28160 / 60000]\n",
            "Loss: 2.1633379459381104 [28416 / 60000]\n",
            "Loss: 2.1712892055511475 [28672 / 60000]\n",
            "Loss: 2.170854091644287 [28928 / 60000]\n",
            "Loss: 2.183905601501465 [29184 / 60000]\n",
            "Loss: 2.1785223484039307 [29440 / 60000]\n",
            "Loss: 2.163825750350952 [29696 / 60000]\n",
            "Loss: 2.1621224880218506 [29952 / 60000]\n",
            "Loss: 2.164013385772705 [30208 / 60000]\n",
            "Loss: 2.1754350662231445 [30464 / 60000]\n",
            "Loss: 2.171609878540039 [30720 / 60000]\n",
            "Loss: 2.161916494369507 [30976 / 60000]\n",
            "Loss: 2.171980857849121 [31232 / 60000]\n",
            "Loss: 2.166996955871582 [31488 / 60000]\n",
            "Loss: 2.173938751220703 [31744 / 60000]\n",
            "Loss: 2.1609301567077637 [32000 / 60000]\n",
            "Loss: 2.166618585586548 [32256 / 60000]\n",
            "Loss: 2.1710736751556396 [32512 / 60000]\n",
            "Loss: 2.1718497276306152 [32768 / 60000]\n",
            "Loss: 2.166067361831665 [33024 / 60000]\n",
            "Loss: 2.172274351119995 [33280 / 60000]\n",
            "Loss: 2.173234224319458 [33536 / 60000]\n",
            "Loss: 2.1669349670410156 [33792 / 60000]\n",
            "Loss: 2.1686508655548096 [34048 / 60000]\n",
            "Loss: 2.170366048812866 [34304 / 60000]\n",
            "Loss: 2.170775890350342 [34560 / 60000]\n",
            "Loss: 2.169003486633301 [34816 / 60000]\n",
            "Loss: 2.1612489223480225 [35072 / 60000]\n",
            "Loss: 2.1687068939208984 [35328 / 60000]\n",
            "Loss: 2.1663646697998047 [35584 / 60000]\n",
            "Loss: 2.1602389812469482 [35840 / 60000]\n",
            "Loss: 2.1653172969818115 [36096 / 60000]\n",
            "Loss: 2.1667752265930176 [36352 / 60000]\n",
            "Loss: 2.1730902194976807 [36608 / 60000]\n",
            "Loss: 2.168020725250244 [36864 / 60000]\n",
            "Loss: 2.1686394214630127 [37120 / 60000]\n",
            "Loss: 2.158341407775879 [37376 / 60000]\n",
            "Loss: 2.1600921154022217 [37632 / 60000]\n",
            "Loss: 2.164019823074341 [37888 / 60000]\n",
            "Loss: 2.1629638671875 [38144 / 60000]\n",
            "Loss: 2.1599087715148926 [38400 / 60000]\n",
            "Loss: 2.1634223461151123 [38656 / 60000]\n",
            "Loss: 2.1632916927337646 [38912 / 60000]\n",
            "Loss: 2.1608500480651855 [39168 / 60000]\n",
            "Loss: 2.1676347255706787 [39424 / 60000]\n",
            "Loss: 2.1508705615997314 [39680 / 60000]\n",
            "Loss: 2.1653025150299072 [39936 / 60000]\n",
            "Loss: 2.16373610496521 [40192 / 60000]\n",
            "Loss: 2.15639591217041 [40448 / 60000]\n",
            "Loss: 2.163959264755249 [40704 / 60000]\n",
            "Loss: 2.1657283306121826 [40960 / 60000]\n",
            "Loss: 2.1642649173736572 [41216 / 60000]\n",
            "Loss: 2.164921522140503 [41472 / 60000]\n",
            "Loss: 2.155836343765259 [41728 / 60000]\n",
            "Loss: 2.162065029144287 [41984 / 60000]\n",
            "Loss: 2.1474902629852295 [42240 / 60000]\n",
            "Loss: 2.1631178855895996 [42496 / 60000]\n",
            "Loss: 2.1650478839874268 [42752 / 60000]\n",
            "Loss: 2.166428327560425 [43008 / 60000]\n",
            "Loss: 2.166780710220337 [43264 / 60000]\n",
            "Loss: 2.1676385402679443 [43520 / 60000]\n",
            "Loss: 2.1634039878845215 [43776 / 60000]\n",
            "Loss: 2.152493715286255 [44032 / 60000]\n",
            "Loss: 2.164968252182007 [44288 / 60000]\n",
            "Loss: 2.1521549224853516 [44544 / 60000]\n",
            "Loss: 2.154374122619629 [44800 / 60000]\n",
            "Loss: 2.163315773010254 [45056 / 60000]\n",
            "Loss: 2.155471086502075 [45312 / 60000]\n",
            "Loss: 2.146714925765991 [45568 / 60000]\n",
            "Loss: 2.1517226696014404 [45824 / 60000]\n",
            "Loss: 2.1627492904663086 [46080 / 60000]\n",
            "Loss: 2.149575710296631 [46336 / 60000]\n",
            "Loss: 2.164828300476074 [46592 / 60000]\n",
            "Loss: 2.166433334350586 [46848 / 60000]\n",
            "Loss: 2.1508994102478027 [47104 / 60000]\n",
            "Loss: 2.1499059200286865 [47360 / 60000]\n",
            "Loss: 2.159177303314209 [47616 / 60000]\n",
            "Loss: 2.147127151489258 [47872 / 60000]\n",
            "Loss: 2.1503090858459473 [48128 / 60000]\n",
            "Loss: 2.1633071899414062 [48384 / 60000]\n",
            "Loss: 2.1632463932037354 [48640 / 60000]\n",
            "Loss: 2.14819073677063 [48896 / 60000]\n",
            "Loss: 2.160499334335327 [49152 / 60000]\n",
            "Loss: 2.1458284854888916 [49408 / 60000]\n",
            "Loss: 2.143685817718506 [49664 / 60000]\n",
            "Loss: 2.1611623764038086 [49920 / 60000]\n",
            "Loss: 2.1597442626953125 [50176 / 60000]\n",
            "Loss: 2.152000904083252 [50432 / 60000]\n",
            "Loss: 2.1551785469055176 [50688 / 60000]\n",
            "Loss: 2.1452887058258057 [50944 / 60000]\n",
            "Loss: 2.150829315185547 [51200 / 60000]\n",
            "Loss: 2.1380977630615234 [51456 / 60000]\n",
            "Loss: 2.1423747539520264 [51712 / 60000]\n",
            "Loss: 2.1478517055511475 [51968 / 60000]\n",
            "Loss: 2.1495091915130615 [52224 / 60000]\n",
            "Loss: 2.1562867164611816 [52480 / 60000]\n",
            "Loss: 2.1433801651000977 [52736 / 60000]\n",
            "Loss: 2.1466939449310303 [52992 / 60000]\n",
            "Loss: 2.146329641342163 [53248 / 60000]\n",
            "Loss: 2.1388819217681885 [53504 / 60000]\n",
            "Loss: 2.147085666656494 [53760 / 60000]\n",
            "Loss: 2.1489102840423584 [54016 / 60000]\n",
            "Loss: 2.1454660892486572 [54272 / 60000]\n",
            "Loss: 2.142728567123413 [54528 / 60000]\n",
            "Loss: 2.1392972469329834 [54784 / 60000]\n",
            "Loss: 2.151937961578369 [55040 / 60000]\n",
            "Loss: 2.140331745147705 [55296 / 60000]\n",
            "Loss: 2.148362398147583 [55552 / 60000]\n",
            "Loss: 2.146495819091797 [55808 / 60000]\n",
            "Loss: 2.157045841217041 [56064 / 60000]\n",
            "Loss: 2.1386566162109375 [56320 / 60000]\n",
            "Loss: 2.1449058055877686 [56576 / 60000]\n",
            "Loss: 2.148512601852417 [56832 / 60000]\n",
            "Loss: 2.14259934425354 [57088 / 60000]\n",
            "Loss: 2.1559247970581055 [57344 / 60000]\n",
            "Loss: 2.1305198669433594 [57600 / 60000]\n",
            "Loss: 2.148526668548584 [57856 / 60000]\n",
            "Loss: 2.138155221939087 [58112 / 60000]\n",
            "Loss: 2.1293461322784424 [58368 / 60000]\n",
            "Loss: 2.1415812969207764 [58624 / 60000]\n",
            "Loss: 2.153836965560913 [58880 / 60000]\n",
            "Loss: 2.140846014022827 [59136 / 60000]\n",
            "Loss: 2.1448230743408203 [59392 / 60000]\n",
            "Loss: 2.143364667892456 [59648 / 60000]\n",
            "Loss: 2.1424717903137207 [22464 / 60000]\n",
            "Test Loss: 2.1373401641845704 Accuracy:59.51\n",
            "epoch:4=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 2.1307218074798584 [0 / 60000]\n",
            "Loss: 2.135270833969116 [256 / 60000]\n",
            "Loss: 2.154367208480835 [512 / 60000]\n",
            "Loss: 2.1373722553253174 [768 / 60000]\n",
            "Loss: 2.1417412757873535 [1024 / 60000]\n",
            "Loss: 2.1355254650115967 [1280 / 60000]\n",
            "Loss: 2.1371097564697266 [1536 / 60000]\n",
            "Loss: 2.1297452449798584 [1792 / 60000]\n",
            "Loss: 2.1448280811309814 [2048 / 60000]\n",
            "Loss: 2.142345428466797 [2304 / 60000]\n",
            "Loss: 2.1414802074432373 [2560 / 60000]\n",
            "Loss: 2.132309675216675 [2816 / 60000]\n",
            "Loss: 2.1380927562713623 [3072 / 60000]\n",
            "Loss: 2.133026123046875 [3328 / 60000]\n",
            "Loss: 2.1419661045074463 [3584 / 60000]\n",
            "Loss: 2.140813112258911 [3840 / 60000]\n",
            "Loss: 2.158221483230591 [4096 / 60000]\n",
            "Loss: 2.146996259689331 [4352 / 60000]\n",
            "Loss: 2.1336076259613037 [4608 / 60000]\n",
            "Loss: 2.148643970489502 [4864 / 60000]\n",
            "Loss: 2.133322238922119 [5120 / 60000]\n",
            "Loss: 2.1362714767456055 [5376 / 60000]\n",
            "Loss: 2.149819850921631 [5632 / 60000]\n",
            "Loss: 2.133765935897827 [5888 / 60000]\n",
            "Loss: 2.1438565254211426 [6144 / 60000]\n",
            "Loss: 2.1331076622009277 [6400 / 60000]\n",
            "Loss: 2.152397871017456 [6656 / 60000]\n",
            "Loss: 2.147843599319458 [6912 / 60000]\n",
            "Loss: 2.136174440383911 [7168 / 60000]\n",
            "Loss: 2.1296637058258057 [7424 / 60000]\n",
            "Loss: 2.132458448410034 [7680 / 60000]\n",
            "Loss: 2.1338324546813965 [7936 / 60000]\n",
            "Loss: 2.1369683742523193 [8192 / 60000]\n",
            "Loss: 2.137525796890259 [8448 / 60000]\n",
            "Loss: 2.144665002822876 [8704 / 60000]\n",
            "Loss: 2.1242871284484863 [8960 / 60000]\n",
            "Loss: 2.1372151374816895 [9216 / 60000]\n",
            "Loss: 2.1338417530059814 [9472 / 60000]\n",
            "Loss: 2.131614923477173 [9728 / 60000]\n",
            "Loss: 2.130728006362915 [9984 / 60000]\n",
            "Loss: 2.1319081783294678 [10240 / 60000]\n",
            "Loss: 2.1387178897857666 [10496 / 60000]\n",
            "Loss: 2.129688024520874 [10752 / 60000]\n",
            "Loss: 2.132347345352173 [11008 / 60000]\n",
            "Loss: 2.126396656036377 [11264 / 60000]\n",
            "Loss: 2.1368424892425537 [11520 / 60000]\n",
            "Loss: 2.133864402770996 [11776 / 60000]\n",
            "Loss: 2.1513404846191406 [12032 / 60000]\n",
            "Loss: 2.1448113918304443 [12288 / 60000]\n",
            "Loss: 2.136862277984619 [12544 / 60000]\n",
            "Loss: 2.1265571117401123 [12800 / 60000]\n",
            "Loss: 2.134366273880005 [13056 / 60000]\n",
            "Loss: 2.128241539001465 [13312 / 60000]\n",
            "Loss: 2.1375465393066406 [13568 / 60000]\n",
            "Loss: 2.1227500438690186 [13824 / 60000]\n",
            "Loss: 2.127763271331787 [14080 / 60000]\n",
            "Loss: 2.13252329826355 [14336 / 60000]\n",
            "Loss: 2.1179730892181396 [14592 / 60000]\n",
            "Loss: 2.1295433044433594 [14848 / 60000]\n",
            "Loss: 2.1378426551818848 [15104 / 60000]\n",
            "Loss: 2.1377389430999756 [15360 / 60000]\n",
            "Loss: 2.126310110092163 [15616 / 60000]\n",
            "Loss: 2.1246273517608643 [15872 / 60000]\n",
            "Loss: 2.1355068683624268 [16128 / 60000]\n",
            "Loss: 2.128236770629883 [16384 / 60000]\n",
            "Loss: 2.118795394897461 [16640 / 60000]\n",
            "Loss: 2.1300442218780518 [16896 / 60000]\n",
            "Loss: 2.127495527267456 [17152 / 60000]\n",
            "Loss: 2.135547399520874 [17408 / 60000]\n",
            "Loss: 2.1369972229003906 [17664 / 60000]\n",
            "Loss: 2.1261215209960938 [17920 / 60000]\n",
            "Loss: 2.115536689758301 [18176 / 60000]\n",
            "Loss: 2.132521152496338 [18432 / 60000]\n",
            "Loss: 2.1238672733306885 [18688 / 60000]\n",
            "Loss: 2.1178832054138184 [18944 / 60000]\n",
            "Loss: 2.1170012950897217 [19200 / 60000]\n",
            "Loss: 2.1134979724884033 [19456 / 60000]\n",
            "Loss: 2.1197497844696045 [19712 / 60000]\n",
            "Loss: 2.1409053802490234 [19968 / 60000]\n",
            "Loss: 2.1228549480438232 [20224 / 60000]\n",
            "Loss: 2.1284945011138916 [20480 / 60000]\n",
            "Loss: 2.11207914352417 [20736 / 60000]\n",
            "Loss: 2.1144142150878906 [20992 / 60000]\n",
            "Loss: 2.1220552921295166 [21248 / 60000]\n",
            "Loss: 2.1196179389953613 [21504 / 60000]\n",
            "Loss: 2.123720169067383 [21760 / 60000]\n",
            "Loss: 2.123802423477173 [22016 / 60000]\n",
            "Loss: 2.136418104171753 [22272 / 60000]\n",
            "Loss: 2.1124627590179443 [22528 / 60000]\n",
            "Loss: 2.120054244995117 [22784 / 60000]\n",
            "Loss: 2.110959053039551 [23040 / 60000]\n",
            "Loss: 2.1282033920288086 [23296 / 60000]\n",
            "Loss: 2.1182682514190674 [23552 / 60000]\n",
            "Loss: 2.120962142944336 [23808 / 60000]\n",
            "Loss: 2.1207094192504883 [24064 / 60000]\n",
            "Loss: 2.1167900562286377 [24320 / 60000]\n",
            "Loss: 2.1363372802734375 [24576 / 60000]\n",
            "Loss: 2.1081583499908447 [24832 / 60000]\n",
            "Loss: 2.119028091430664 [25088 / 60000]\n",
            "Loss: 2.10235595703125 [25344 / 60000]\n",
            "Loss: 2.131218671798706 [25600 / 60000]\n",
            "Loss: 2.117642641067505 [25856 / 60000]\n",
            "Loss: 2.121227502822876 [26112 / 60000]\n",
            "Loss: 2.122581720352173 [26368 / 60000]\n",
            "Loss: 2.0998733043670654 [26624 / 60000]\n",
            "Loss: 2.1141369342803955 [26880 / 60000]\n",
            "Loss: 2.1100118160247803 [27136 / 60000]\n",
            "Loss: 2.1003806591033936 [27392 / 60000]\n",
            "Loss: 2.111726760864258 [27648 / 60000]\n",
            "Loss: 2.127556800842285 [27904 / 60000]\n",
            "Loss: 2.1224160194396973 [28160 / 60000]\n",
            "Loss: 2.107088088989258 [28416 / 60000]\n",
            "Loss: 2.114370584487915 [28672 / 60000]\n",
            "Loss: 2.1105713844299316 [28928 / 60000]\n",
            "Loss: 2.1219897270202637 [29184 / 60000]\n",
            "Loss: 2.1059367656707764 [29440 / 60000]\n",
            "Loss: 2.1051783561706543 [29696 / 60000]\n",
            "Loss: 2.11430025100708 [29952 / 60000]\n",
            "Loss: 2.094777822494507 [30208 / 60000]\n",
            "Loss: 2.11606502532959 [30464 / 60000]\n",
            "Loss: 2.1021339893341064 [30720 / 60000]\n",
            "Loss: 2.11712646484375 [30976 / 60000]\n",
            "Loss: 2.110537052154541 [31232 / 60000]\n",
            "Loss: 2.110921859741211 [31488 / 60000]\n",
            "Loss: 2.1060893535614014 [31744 / 60000]\n",
            "Loss: 2.120210647583008 [32000 / 60000]\n",
            "Loss: 2.1030709743499756 [32256 / 60000]\n",
            "Loss: 2.1064422130584717 [32512 / 60000]\n",
            "Loss: 2.106581687927246 [32768 / 60000]\n",
            "Loss: 2.1164586544036865 [33024 / 60000]\n",
            "Loss: 2.112799644470215 [33280 / 60000]\n",
            "Loss: 2.1171884536743164 [33536 / 60000]\n",
            "Loss: 2.101792335510254 [33792 / 60000]\n",
            "Loss: 2.1141271591186523 [34048 / 60000]\n",
            "Loss: 2.1280057430267334 [34304 / 60000]\n",
            "Loss: 2.111506462097168 [34560 / 60000]\n",
            "Loss: 2.1162984371185303 [34816 / 60000]\n",
            "Loss: 2.110241651535034 [35072 / 60000]\n",
            "Loss: 2.1089627742767334 [35328 / 60000]\n",
            "Loss: 2.1179680824279785 [35584 / 60000]\n",
            "Loss: 2.107011079788208 [35840 / 60000]\n",
            "Loss: 2.105151653289795 [36096 / 60000]\n",
            "Loss: 2.1052064895629883 [36352 / 60000]\n",
            "Loss: 2.097306251525879 [36608 / 60000]\n",
            "Loss: 2.105391502380371 [36864 / 60000]\n",
            "Loss: 2.0933949947357178 [37120 / 60000]\n",
            "Loss: 2.094893217086792 [37376 / 60000]\n",
            "Loss: 2.097512722015381 [37632 / 60000]\n",
            "Loss: 2.1049468517303467 [37888 / 60000]\n",
            "Loss: 2.0982534885406494 [38144 / 60000]\n",
            "Loss: 2.102365493774414 [38400 / 60000]\n",
            "Loss: 2.0898239612579346 [38656 / 60000]\n",
            "Loss: 2.0971550941467285 [38912 / 60000]\n",
            "Loss: 2.1060264110565186 [39168 / 60000]\n",
            "Loss: 2.108381986618042 [39424 / 60000]\n",
            "Loss: 2.111748218536377 [39680 / 60000]\n",
            "Loss: 2.0967445373535156 [39936 / 60000]\n",
            "Loss: 2.1148242950439453 [40192 / 60000]\n",
            "Loss: 2.0998337268829346 [40448 / 60000]\n",
            "Loss: 2.101364850997925 [40704 / 60000]\n",
            "Loss: 2.083163022994995 [40960 / 60000]\n",
            "Loss: 2.118215322494507 [41216 / 60000]\n",
            "Loss: 2.0984420776367188 [41472 / 60000]\n",
            "Loss: 2.1077311038970947 [41728 / 60000]\n",
            "Loss: 2.09502911567688 [41984 / 60000]\n",
            "Loss: 2.113116502761841 [42240 / 60000]\n",
            "Loss: 2.08273983001709 [42496 / 60000]\n",
            "Loss: 2.1007893085479736 [42752 / 60000]\n",
            "Loss: 2.104997396469116 [43008 / 60000]\n",
            "Loss: 2.1032023429870605 [43264 / 60000]\n",
            "Loss: 2.1101996898651123 [43520 / 60000]\n",
            "Loss: 2.099180221557617 [43776 / 60000]\n",
            "Loss: 2.112769365310669 [44032 / 60000]\n",
            "Loss: 2.094879150390625 [44288 / 60000]\n",
            "Loss: 2.094618320465088 [44544 / 60000]\n",
            "Loss: 2.1044323444366455 [44800 / 60000]\n",
            "Loss: 2.0848755836486816 [45056 / 60000]\n",
            "Loss: 2.088700294494629 [45312 / 60000]\n",
            "Loss: 2.10326886177063 [45568 / 60000]\n",
            "Loss: 2.0955822467803955 [45824 / 60000]\n",
            "Loss: 2.1106557846069336 [46080 / 60000]\n",
            "Loss: 2.0951435565948486 [46336 / 60000]\n",
            "Loss: 2.097344160079956 [46592 / 60000]\n",
            "Loss: 2.0966620445251465 [46848 / 60000]\n",
            "Loss: 2.1136038303375244 [47104 / 60000]\n",
            "Loss: 2.084960460662842 [47360 / 60000]\n",
            "Loss: 2.0828075408935547 [47616 / 60000]\n",
            "Loss: 2.100963592529297 [47872 / 60000]\n",
            "Loss: 2.0851807594299316 [48128 / 60000]\n",
            "Loss: 2.107377529144287 [48384 / 60000]\n",
            "Loss: 2.1082723140716553 [48640 / 60000]\n",
            "Loss: 2.1153061389923096 [48896 / 60000]\n",
            "Loss: 2.102266311645508 [49152 / 60000]\n",
            "Loss: 2.0804173946380615 [49408 / 60000]\n",
            "Loss: 2.090993881225586 [49664 / 60000]\n",
            "Loss: 2.093696117401123 [49920 / 60000]\n",
            "Loss: 2.0833115577697754 [50176 / 60000]\n",
            "Loss: 2.0868961811065674 [50432 / 60000]\n",
            "Loss: 2.088378429412842 [50688 / 60000]\n",
            "Loss: 2.0840115547180176 [50944 / 60000]\n",
            "Loss: 2.096604585647583 [51200 / 60000]\n",
            "Loss: 2.07222843170166 [51456 / 60000]\n",
            "Loss: 2.0837910175323486 [51712 / 60000]\n",
            "Loss: 2.0737814903259277 [51968 / 60000]\n",
            "Loss: 2.095689535140991 [52224 / 60000]\n",
            "Loss: 2.0860610008239746 [52480 / 60000]\n",
            "Loss: 2.079162359237671 [52736 / 60000]\n",
            "Loss: 2.09201717376709 [52992 / 60000]\n",
            "Loss: 2.088571310043335 [53248 / 60000]\n",
            "Loss: 2.088015556335449 [53504 / 60000]\n",
            "Loss: 2.0817694664001465 [53760 / 60000]\n",
            "Loss: 2.0947458744049072 [54016 / 60000]\n",
            "Loss: 2.0831546783447266 [54272 / 60000]\n",
            "Loss: 2.090613842010498 [54528 / 60000]\n",
            "Loss: 2.0872974395751953 [54784 / 60000]\n",
            "Loss: 2.0884721279144287 [55040 / 60000]\n",
            "Loss: 2.093468427658081 [55296 / 60000]\n",
            "Loss: 2.0851409435272217 [55552 / 60000]\n",
            "Loss: 2.0854763984680176 [55808 / 60000]\n",
            "Loss: 2.0845022201538086 [56064 / 60000]\n",
            "Loss: 2.0949974060058594 [56320 / 60000]\n",
            "Loss: 2.0883171558380127 [56576 / 60000]\n",
            "Loss: 2.0846352577209473 [56832 / 60000]\n",
            "Loss: 2.079678535461426 [57088 / 60000]\n",
            "Loss: 2.09647274017334 [57344 / 60000]\n",
            "Loss: 2.0741631984710693 [57600 / 60000]\n",
            "Loss: 2.0909814834594727 [57856 / 60000]\n",
            "Loss: 2.0934557914733887 [58112 / 60000]\n",
            "Loss: 2.0674142837524414 [58368 / 60000]\n",
            "Loss: 2.0952625274658203 [58624 / 60000]\n",
            "Loss: 2.0707743167877197 [58880 / 60000]\n",
            "Loss: 2.097996711730957 [59136 / 60000]\n",
            "Loss: 2.076899290084839 [59392 / 60000]\n",
            "Loss: 2.0809905529022217 [59648 / 60000]\n",
            "Loss: 2.081944227218628 [22464 / 60000]\n",
            "Test Loss: 2.0760638236999513 Accuracy:65.83\n",
            "epoch:5=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 2.0614821910858154 [0 / 60000]\n",
            "Loss: 2.0923686027526855 [256 / 60000]\n",
            "Loss: 2.0855841636657715 [512 / 60000]\n",
            "Loss: 2.0871527194976807 [768 / 60000]\n",
            "Loss: 2.0805017948150635 [1024 / 60000]\n",
            "Loss: 2.0831425189971924 [1280 / 60000]\n",
            "Loss: 2.082641839981079 [1536 / 60000]\n",
            "Loss: 2.0592434406280518 [1792 / 60000]\n",
            "Loss: 2.0834462642669678 [2048 / 60000]\n",
            "Loss: 2.0873775482177734 [2304 / 60000]\n",
            "Loss: 2.07373046875 [2560 / 60000]\n",
            "Loss: 2.1035847663879395 [2816 / 60000]\n",
            "Loss: 2.0851972103118896 [3072 / 60000]\n",
            "Loss: 2.062504291534424 [3328 / 60000]\n",
            "Loss: 2.1052467823028564 [3584 / 60000]\n",
            "Loss: 2.0779738426208496 [3840 / 60000]\n",
            "Loss: 2.075666666030884 [4096 / 60000]\n",
            "Loss: 2.05945086479187 [4352 / 60000]\n",
            "Loss: 2.0465786457061768 [4608 / 60000]\n",
            "Loss: 2.0819876194000244 [4864 / 60000]\n",
            "Loss: 2.084787368774414 [5120 / 60000]\n",
            "Loss: 2.0621302127838135 [5376 / 60000]\n",
            "Loss: 2.072458267211914 [5632 / 60000]\n",
            "Loss: 2.0739078521728516 [5888 / 60000]\n",
            "Loss: 2.079331874847412 [6144 / 60000]\n",
            "Loss: 2.080972909927368 [6400 / 60000]\n",
            "Loss: 2.099565267562866 [6656 / 60000]\n",
            "Loss: 2.0621228218078613 [6912 / 60000]\n",
            "Loss: 2.0935118198394775 [7168 / 60000]\n",
            "Loss: 2.0584871768951416 [7424 / 60000]\n",
            "Loss: 2.064314842224121 [7680 / 60000]\n",
            "Loss: 2.0793874263763428 [7936 / 60000]\n",
            "Loss: 2.0537052154541016 [8192 / 60000]\n",
            "Loss: 2.081392526626587 [8448 / 60000]\n",
            "Loss: 2.0755531787872314 [8704 / 60000]\n",
            "Loss: 2.0768816471099854 [8960 / 60000]\n",
            "Loss: 2.085890531539917 [9216 / 60000]\n",
            "Loss: 2.0799357891082764 [9472 / 60000]\n",
            "Loss: 2.0660860538482666 [9728 / 60000]\n",
            "Loss: 2.088057518005371 [9984 / 60000]\n",
            "Loss: 2.0499088764190674 [10240 / 60000]\n",
            "Loss: 2.0686240196228027 [10496 / 60000]\n",
            "Loss: 2.068324327468872 [10752 / 60000]\n",
            "Loss: 2.0705924034118652 [11008 / 60000]\n",
            "Loss: 2.0911552906036377 [11264 / 60000]\n",
            "Loss: 2.066516160964966 [11520 / 60000]\n",
            "Loss: 2.0697035789489746 [11776 / 60000]\n",
            "Loss: 2.0572125911712646 [12032 / 60000]\n",
            "Loss: 2.0672242641448975 [12288 / 60000]\n",
            "Loss: 2.062802314758301 [12544 / 60000]\n",
            "Loss: 2.0544865131378174 [12800 / 60000]\n",
            "Loss: 2.0572805404663086 [13056 / 60000]\n",
            "Loss: 2.063451051712036 [13312 / 60000]\n",
            "Loss: 2.0800986289978027 [13568 / 60000]\n",
            "Loss: 2.076037883758545 [13824 / 60000]\n",
            "Loss: 2.071503162384033 [14080 / 60000]\n",
            "Loss: 2.0580925941467285 [14336 / 60000]\n",
            "Loss: 2.0750620365142822 [14592 / 60000]\n",
            "Loss: 2.075298547744751 [14848 / 60000]\n",
            "Loss: 2.070650815963745 [15104 / 60000]\n",
            "Loss: 2.06900691986084 [15360 / 60000]\n",
            "Loss: 2.0835647583007812 [15616 / 60000]\n",
            "Loss: 2.055492877960205 [15872 / 60000]\n",
            "Loss: 2.046093463897705 [16128 / 60000]\n",
            "Loss: 2.0731635093688965 [16384 / 60000]\n",
            "Loss: 2.067751884460449 [16640 / 60000]\n",
            "Loss: 2.0764658451080322 [16896 / 60000]\n",
            "Loss: 2.053539991378784 [17152 / 60000]\n",
            "Loss: 2.060788869857788 [17408 / 60000]\n",
            "Loss: 2.069115400314331 [17664 / 60000]\n",
            "Loss: 2.055934190750122 [17920 / 60000]\n",
            "Loss: 2.0663087368011475 [18176 / 60000]\n",
            "Loss: 2.0683369636535645 [18432 / 60000]\n",
            "Loss: 2.0646286010742188 [18688 / 60000]\n",
            "Loss: 2.0733323097229004 [18944 / 60000]\n",
            "Loss: 2.064146041870117 [19200 / 60000]\n",
            "Loss: 2.045776605606079 [19456 / 60000]\n",
            "Loss: 2.0382065773010254 [19712 / 60000]\n",
            "Loss: 2.072450637817383 [19968 / 60000]\n",
            "Loss: 2.0559418201446533 [20224 / 60000]\n",
            "Loss: 2.059812545776367 [20480 / 60000]\n",
            "Loss: 2.059959888458252 [20736 / 60000]\n",
            "Loss: 2.076653003692627 [20992 / 60000]\n",
            "Loss: 2.061422824859619 [21248 / 60000]\n",
            "Loss: 2.0736818313598633 [21504 / 60000]\n",
            "Loss: 2.036085367202759 [21760 / 60000]\n",
            "Loss: 2.051879405975342 [22016 / 60000]\n",
            "Loss: 2.0648927688598633 [22272 / 60000]\n",
            "Loss: 2.0456175804138184 [22528 / 60000]\n",
            "Loss: 2.067350149154663 [22784 / 60000]\n",
            "Loss: 2.056438684463501 [23040 / 60000]\n",
            "Loss: 2.0646493434906006 [23296 / 60000]\n",
            "Loss: 2.077833890914917 [23552 / 60000]\n",
            "Loss: 2.0589592456817627 [23808 / 60000]\n",
            "Loss: 2.062013626098633 [24064 / 60000]\n",
            "Loss: 2.0545265674591064 [24320 / 60000]\n",
            "Loss: 2.040391206741333 [24576 / 60000]\n",
            "Loss: 2.0686967372894287 [24832 / 60000]\n",
            "Loss: 2.0494730472564697 [25088 / 60000]\n",
            "Loss: 2.054678440093994 [25344 / 60000]\n",
            "Loss: 2.0630385875701904 [25600 / 60000]\n",
            "Loss: 2.066200017929077 [25856 / 60000]\n",
            "Loss: 2.045825242996216 [26112 / 60000]\n",
            "Loss: 2.069957971572876 [26368 / 60000]\n",
            "Loss: 2.0490944385528564 [26624 / 60000]\n",
            "Loss: 2.0557937622070312 [26880 / 60000]\n",
            "Loss: 2.0380945205688477 [27136 / 60000]\n",
            "Loss: 2.06146240234375 [27392 / 60000]\n",
            "Loss: 2.0697691440582275 [27648 / 60000]\n",
            "Loss: 2.0725409984588623 [27904 / 60000]\n",
            "Loss: 2.049990653991699 [28160 / 60000]\n",
            "Loss: 2.0489912033081055 [28416 / 60000]\n",
            "Loss: 2.0837974548339844 [28672 / 60000]\n",
            "Loss: 2.0709080696105957 [28928 / 60000]\n",
            "Loss: 2.0660951137542725 [29184 / 60000]\n",
            "Loss: 2.0452795028686523 [29440 / 60000]\n",
            "Loss: 2.0390114784240723 [29696 / 60000]\n",
            "Loss: 2.050828218460083 [29952 / 60000]\n",
            "Loss: 2.0408663749694824 [30208 / 60000]\n",
            "Loss: 2.041313409805298 [30464 / 60000]\n",
            "Loss: 2.046708822250366 [30720 / 60000]\n",
            "Loss: 2.0478122234344482 [30976 / 60000]\n",
            "Loss: 2.0382139682769775 [31232 / 60000]\n",
            "Loss: 2.0515599250793457 [31488 / 60000]\n",
            "Loss: 2.045450210571289 [31744 / 60000]\n",
            "Loss: 2.0648374557495117 [32000 / 60000]\n",
            "Loss: 2.0357465744018555 [32256 / 60000]\n",
            "Loss: 2.0476267337799072 [32512 / 60000]\n",
            "Loss: 2.0502445697784424 [32768 / 60000]\n",
            "Loss: 2.034358024597168 [33024 / 60000]\n",
            "Loss: 2.0292932987213135 [33280 / 60000]\n",
            "Loss: 2.0351858139038086 [33536 / 60000]\n",
            "Loss: 2.046985387802124 [33792 / 60000]\n",
            "Loss: 2.049164056777954 [34048 / 60000]\n",
            "Loss: 2.050819158554077 [34304 / 60000]\n",
            "Loss: 2.0229148864746094 [34560 / 60000]\n",
            "Loss: 2.0482754707336426 [34816 / 60000]\n",
            "Loss: 2.0410914421081543 [35072 / 60000]\n",
            "Loss: 2.052847146987915 [35328 / 60000]\n",
            "Loss: 2.0605156421661377 [35584 / 60000]\n",
            "Loss: 2.0337908267974854 [35840 / 60000]\n",
            "Loss: 2.013786792755127 [36096 / 60000]\n",
            "Loss: 2.0348432064056396 [36352 / 60000]\n",
            "Loss: 2.0473835468292236 [36608 / 60000]\n",
            "Loss: 2.0437347888946533 [36864 / 60000]\n",
            "Loss: 2.0174765586853027 [37120 / 60000]\n",
            "Loss: 2.0468342304229736 [37376 / 60000]\n",
            "Loss: 2.0428671836853027 [37632 / 60000]\n",
            "Loss: 2.0367848873138428 [37888 / 60000]\n",
            "Loss: 2.0279197692871094 [38144 / 60000]\n",
            "Loss: 2.038623571395874 [38400 / 60000]\n",
            "Loss: 2.028266668319702 [38656 / 60000]\n",
            "Loss: 2.066444158554077 [38912 / 60000]\n",
            "Loss: 2.0481786727905273 [39168 / 60000]\n",
            "Loss: 2.034580945968628 [39424 / 60000]\n",
            "Loss: 2.050865650177002 [39680 / 60000]\n",
            "Loss: 2.044926643371582 [39936 / 60000]\n",
            "Loss: 2.043382406234741 [40192 / 60000]\n",
            "Loss: 2.038576602935791 [40448 / 60000]\n",
            "Loss: 2.03890323638916 [40704 / 60000]\n",
            "Loss: 2.0352401733398438 [40960 / 60000]\n",
            "Loss: 2.0394716262817383 [41216 / 60000]\n",
            "Loss: 2.0431673526763916 [41472 / 60000]\n",
            "Loss: 2.0529298782348633 [41728 / 60000]\n",
            "Loss: 2.0530753135681152 [41984 / 60000]\n",
            "Loss: 2.0460093021392822 [42240 / 60000]\n",
            "Loss: 2.036907196044922 [42496 / 60000]\n",
            "Loss: 2.024871349334717 [42752 / 60000]\n",
            "Loss: 2.039727210998535 [43008 / 60000]\n",
            "Loss: 2.0300450325012207 [43264 / 60000]\n",
            "Loss: 2.0479907989501953 [43520 / 60000]\n",
            "Loss: 2.0447704792022705 [43776 / 60000]\n",
            "Loss: 2.058617353439331 [44032 / 60000]\n",
            "Loss: 2.0277159214019775 [44288 / 60000]\n",
            "Loss: 2.0458736419677734 [44544 / 60000]\n",
            "Loss: 2.0228331089019775 [44800 / 60000]\n",
            "Loss: 2.024465322494507 [45056 / 60000]\n",
            "Loss: 2.024123430252075 [45312 / 60000]\n",
            "Loss: 2.034508466720581 [45568 / 60000]\n",
            "Loss: 2.0475499629974365 [45824 / 60000]\n",
            "Loss: 2.034646987915039 [46080 / 60000]\n",
            "Loss: 2.0261454582214355 [46336 / 60000]\n",
            "Loss: 2.0278422832489014 [46592 / 60000]\n",
            "Loss: 2.043492078781128 [46848 / 60000]\n",
            "Loss: 2.0312912464141846 [47104 / 60000]\n",
            "Loss: 2.0112550258636475 [47360 / 60000]\n",
            "Loss: 2.0510523319244385 [47616 / 60000]\n",
            "Loss: 2.022636651992798 [47872 / 60000]\n",
            "Loss: 2.0441231727600098 [48128 / 60000]\n",
            "Loss: 2.040254831314087 [48384 / 60000]\n",
            "Loss: 2.021029472351074 [48640 / 60000]\n",
            "Loss: 2.0093443393707275 [48896 / 60000]\n",
            "Loss: 2.0393567085266113 [49152 / 60000]\n",
            "Loss: 2.0352954864501953 [49408 / 60000]\n",
            "Loss: 2.0383095741271973 [49664 / 60000]\n",
            "Loss: 2.0396060943603516 [49920 / 60000]\n",
            "Loss: 2.009565591812134 [50176 / 60000]\n",
            "Loss: 2.041536331176758 [50432 / 60000]\n",
            "Loss: 2.043635129928589 [50688 / 60000]\n",
            "Loss: 2.027783155441284 [50944 / 60000]\n",
            "Loss: 2.0368099212646484 [51200 / 60000]\n",
            "Loss: 2.033498764038086 [51456 / 60000]\n",
            "Loss: 2.0121333599090576 [51712 / 60000]\n",
            "Loss: 2.0200743675231934 [51968 / 60000]\n",
            "Loss: 2.0276825428009033 [52224 / 60000]\n",
            "Loss: 2.026667594909668 [52480 / 60000]\n",
            "Loss: 2.0195040702819824 [52736 / 60000]\n",
            "Loss: 2.0424375534057617 [52992 / 60000]\n",
            "Loss: 2.044069290161133 [53248 / 60000]\n",
            "Loss: 2.0231242179870605 [53504 / 60000]\n",
            "Loss: 2.032033920288086 [53760 / 60000]\n",
            "Loss: 2.046921491622925 [54016 / 60000]\n",
            "Loss: 2.033726215362549 [54272 / 60000]\n",
            "Loss: 2.019178867340088 [54528 / 60000]\n",
            "Loss: 2.0420126914978027 [54784 / 60000]\n",
            "Loss: 2.016868829727173 [55040 / 60000]\n",
            "Loss: 2.0111708641052246 [55296 / 60000]\n",
            "Loss: 2.0276002883911133 [55552 / 60000]\n",
            "Loss: 2.0106446743011475 [55808 / 60000]\n",
            "Loss: 2.030104875564575 [56064 / 60000]\n",
            "Loss: 2.0194482803344727 [56320 / 60000]\n",
            "Loss: 2.041971445083618 [56576 / 60000]\n",
            "Loss: 2.031663179397583 [56832 / 60000]\n",
            "Loss: 2.012603998184204 [57088 / 60000]\n",
            "Loss: 2.024043321609497 [57344 / 60000]\n",
            "Loss: 2.0111329555511475 [57600 / 60000]\n",
            "Loss: 2.0238490104675293 [57856 / 60000]\n",
            "Loss: 2.0314745903015137 [58112 / 60000]\n",
            "Loss: 2.011086940765381 [58368 / 60000]\n",
            "Loss: 2.0171661376953125 [58624 / 60000]\n",
            "Loss: 2.027933120727539 [58880 / 60000]\n",
            "Loss: 2.0058789253234863 [59136 / 60000]\n",
            "Loss: 2.040079355239868 [59392 / 60000]\n",
            "Loss: 1.996059775352478 [59648 / 60000]\n",
            "Loss: 2.0027904510498047 [22464 / 60000]\n",
            "Test Loss: 2.0097627609968187 Accuracy:69.06\n",
            "epoch:6=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 2.017350673675537 [0 / 60000]\n",
            "Loss: 2.0118296146392822 [256 / 60000]\n",
            "Loss: 2.0139644145965576 [512 / 60000]\n",
            "Loss: 2.0446627140045166 [768 / 60000]\n",
            "Loss: 2.022300958633423 [1024 / 60000]\n",
            "Loss: 2.018343448638916 [1280 / 60000]\n",
            "Loss: 2.006535530090332 [1536 / 60000]\n",
            "Loss: 2.0211143493652344 [1792 / 60000]\n",
            "Loss: 2.0075151920318604 [2048 / 60000]\n",
            "Loss: 2.0161292552948 [2304 / 60000]\n",
            "Loss: 2.0114378929138184 [2560 / 60000]\n",
            "Loss: 2.0109431743621826 [2816 / 60000]\n",
            "Loss: 2.026379346847534 [3072 / 60000]\n",
            "Loss: 2.0100741386413574 [3328 / 60000]\n",
            "Loss: 2.0086238384246826 [3584 / 60000]\n",
            "Loss: 1.995133399963379 [3840 / 60000]\n",
            "Loss: 2.0195915699005127 [4096 / 60000]\n",
            "Loss: 2.0137948989868164 [4352 / 60000]\n",
            "Loss: 1.990777850151062 [4608 / 60000]\n",
            "Loss: 2.015872001647949 [4864 / 60000]\n",
            "Loss: 2.0029847621917725 [5120 / 60000]\n",
            "Loss: 2.0288946628570557 [5376 / 60000]\n",
            "Loss: 2.016921043395996 [5632 / 60000]\n",
            "Loss: 2.0041000843048096 [5888 / 60000]\n",
            "Loss: 2.0129029750823975 [6144 / 60000]\n",
            "Loss: 2.012584686279297 [6400 / 60000]\n",
            "Loss: 2.0155136585235596 [6656 / 60000]\n",
            "Loss: 2.022674322128296 [6912 / 60000]\n",
            "Loss: 2.012291669845581 [7168 / 60000]\n",
            "Loss: 2.0182697772979736 [7424 / 60000]\n",
            "Loss: 2.004646062850952 [7680 / 60000]\n",
            "Loss: 1.9934108257293701 [7936 / 60000]\n",
            "Loss: 2.0053648948669434 [8192 / 60000]\n",
            "Loss: 2.0026695728302 [8448 / 60000]\n",
            "Loss: 1.9938173294067383 [8704 / 60000]\n",
            "Loss: 1.9994529485702515 [8960 / 60000]\n",
            "Loss: 2.0299041271209717 [9216 / 60000]\n",
            "Loss: 2.012392282485962 [9472 / 60000]\n",
            "Loss: 1.9901710748672485 [9728 / 60000]\n",
            "Loss: 2.0280892848968506 [9984 / 60000]\n",
            "Loss: 1.9953807592391968 [10240 / 60000]\n",
            "Loss: 1.9937384128570557 [10496 / 60000]\n",
            "Loss: 2.009781837463379 [10752 / 60000]\n",
            "Loss: 1.9963737726211548 [11008 / 60000]\n",
            "Loss: 1.991781234741211 [11264 / 60000]\n",
            "Loss: 2.0365006923675537 [11520 / 60000]\n",
            "Loss: 1.9950578212738037 [11776 / 60000]\n",
            "Loss: 2.0177977085113525 [12032 / 60000]\n",
            "Loss: 2.0213828086853027 [12288 / 60000]\n",
            "Loss: 1.995477557182312 [12544 / 60000]\n",
            "Loss: 2.013385772705078 [12800 / 60000]\n",
            "Loss: 1.9919798374176025 [13056 / 60000]\n",
            "Loss: 2.015547513961792 [13312 / 60000]\n",
            "Loss: 2.015270471572876 [13568 / 60000]\n",
            "Loss: 2.016535520553589 [13824 / 60000]\n",
            "Loss: 2.0013327598571777 [14080 / 60000]\n",
            "Loss: 2.013563394546509 [14336 / 60000]\n",
            "Loss: 2.0151705741882324 [14592 / 60000]\n",
            "Loss: 1.9806005954742432 [14848 / 60000]\n",
            "Loss: 2.0221152305603027 [15104 / 60000]\n",
            "Loss: 2.0152950286865234 [15360 / 60000]\n",
            "Loss: 2.0100176334381104 [15616 / 60000]\n",
            "Loss: 1.9953378438949585 [15872 / 60000]\n",
            "Loss: 1.9779325723648071 [16128 / 60000]\n",
            "Loss: 1.9855175018310547 [16384 / 60000]\n",
            "Loss: 2.0040128231048584 [16640 / 60000]\n",
            "Loss: 2.0094215869903564 [16896 / 60000]\n",
            "Loss: 2.0277762413024902 [17152 / 60000]\n",
            "Loss: 2.007188320159912 [17408 / 60000]\n",
            "Loss: 1.978450894355774 [17664 / 60000]\n",
            "Loss: 1.9856748580932617 [17920 / 60000]\n",
            "Loss: 1.9941835403442383 [18176 / 60000]\n",
            "Loss: 2.0063424110412598 [18432 / 60000]\n",
            "Loss: 1.987410068511963 [18688 / 60000]\n",
            "Loss: 2.012939214706421 [18944 / 60000]\n",
            "Loss: 1.9767096042633057 [19200 / 60000]\n",
            "Loss: 2.0109400749206543 [19456 / 60000]\n",
            "Loss: 1.9808474779129028 [19712 / 60000]\n",
            "Loss: 2.0102784633636475 [19968 / 60000]\n",
            "Loss: 1.9931597709655762 [20224 / 60000]\n",
            "Loss: 1.9966000318527222 [20480 / 60000]\n",
            "Loss: 2.012998580932617 [20736 / 60000]\n",
            "Loss: 1.997355341911316 [20992 / 60000]\n",
            "Loss: 1.9957062005996704 [21248 / 60000]\n",
            "Loss: 2.019166946411133 [21504 / 60000]\n",
            "Loss: 1.9978617429733276 [21760 / 60000]\n",
            "Loss: 2.0199191570281982 [22016 / 60000]\n",
            "Loss: 1.9820187091827393 [22272 / 60000]\n",
            "Loss: 1.9981017112731934 [22528 / 60000]\n",
            "Loss: 1.9904946088790894 [22784 / 60000]\n",
            "Loss: 1.9765888452529907 [23040 / 60000]\n",
            "Loss: 2.001201629638672 [23296 / 60000]\n",
            "Loss: 1.9844025373458862 [23552 / 60000]\n",
            "Loss: 1.994507908821106 [23808 / 60000]\n",
            "Loss: 1.992519736289978 [24064 / 60000]\n",
            "Loss: 1.9652595520019531 [24320 / 60000]\n",
            "Loss: 1.9855625629425049 [24576 / 60000]\n",
            "Loss: 1.9815852642059326 [24832 / 60000]\n",
            "Loss: 1.978042483329773 [25088 / 60000]\n",
            "Loss: 1.9830536842346191 [25344 / 60000]\n",
            "Loss: 1.9698095321655273 [25600 / 60000]\n",
            "Loss: 2.0001707077026367 [25856 / 60000]\n",
            "Loss: 1.993038296699524 [26112 / 60000]\n",
            "Loss: 1.9789963960647583 [26368 / 60000]\n",
            "Loss: 1.992277979850769 [26624 / 60000]\n",
            "Loss: 1.9876123666763306 [26880 / 60000]\n",
            "Loss: 2.000221014022827 [27136 / 60000]\n",
            "Loss: 1.9588727951049805 [27392 / 60000]\n",
            "Loss: 1.9824628829956055 [27648 / 60000]\n",
            "Loss: 2.0035035610198975 [27904 / 60000]\n",
            "Loss: 1.9913302659988403 [28160 / 60000]\n",
            "Loss: 1.9546284675598145 [28416 / 60000]\n",
            "Loss: 1.9619320631027222 [28672 / 60000]\n",
            "Loss: 1.9618229866027832 [28928 / 60000]\n",
            "Loss: 2.009124755859375 [29184 / 60000]\n",
            "Loss: 1.9769212007522583 [29440 / 60000]\n",
            "Loss: 1.9738653898239136 [29696 / 60000]\n",
            "Loss: 1.9912378787994385 [29952 / 60000]\n",
            "Loss: 1.9906195402145386 [30208 / 60000]\n",
            "Loss: 1.9713276624679565 [30464 / 60000]\n",
            "Loss: 1.9980337619781494 [30720 / 60000]\n",
            "Loss: 1.956681251525879 [30976 / 60000]\n",
            "Loss: 1.9922080039978027 [31232 / 60000]\n",
            "Loss: 1.983034372329712 [31488 / 60000]\n",
            "Loss: 1.9986332654953003 [31744 / 60000]\n",
            "Loss: 1.9832348823547363 [32000 / 60000]\n",
            "Loss: 1.9751861095428467 [32256 / 60000]\n",
            "Loss: 1.9785994291305542 [32512 / 60000]\n",
            "Loss: 1.9895528554916382 [32768 / 60000]\n",
            "Loss: 1.960374116897583 [33024 / 60000]\n",
            "Loss: 1.9866317510604858 [33280 / 60000]\n",
            "Loss: 1.9715309143066406 [33536 / 60000]\n",
            "Loss: 1.9710801839828491 [33792 / 60000]\n",
            "Loss: 1.985875129699707 [34048 / 60000]\n",
            "Loss: 1.9729204177856445 [34304 / 60000]\n",
            "Loss: 1.9738713502883911 [34560 / 60000]\n",
            "Loss: 2.012862205505371 [34816 / 60000]\n",
            "Loss: 1.9661540985107422 [35072 / 60000]\n",
            "Loss: 1.9863579273223877 [35328 / 60000]\n",
            "Loss: 1.9855751991271973 [35584 / 60000]\n",
            "Loss: 1.9898877143859863 [35840 / 60000]\n",
            "Loss: 1.9690393209457397 [36096 / 60000]\n",
            "Loss: 1.9757599830627441 [36352 / 60000]\n",
            "Loss: 1.9762065410614014 [36608 / 60000]\n",
            "Loss: 1.9545782804489136 [36864 / 60000]\n",
            "Loss: 1.9882985353469849 [37120 / 60000]\n",
            "Loss: 1.9502166509628296 [37376 / 60000]\n",
            "Loss: 1.9649784564971924 [37632 / 60000]\n",
            "Loss: 1.9943645000457764 [37888 / 60000]\n",
            "Loss: 1.9724738597869873 [38144 / 60000]\n",
            "Loss: 1.9561456441879272 [38400 / 60000]\n",
            "Loss: 1.9658708572387695 [38656 / 60000]\n",
            "Loss: 1.9773887395858765 [38912 / 60000]\n",
            "Loss: 1.9790558815002441 [39168 / 60000]\n",
            "Loss: 1.9630980491638184 [39424 / 60000]\n",
            "Loss: 1.959893822669983 [39680 / 60000]\n",
            "Loss: 1.9712677001953125 [39936 / 60000]\n",
            "Loss: 1.9736628532409668 [40192 / 60000]\n",
            "Loss: 1.963823914527893 [40448 / 60000]\n",
            "Loss: 1.9782885313034058 [40704 / 60000]\n",
            "Loss: 1.9732104539871216 [40960 / 60000]\n",
            "Loss: 1.9675743579864502 [41216 / 60000]\n",
            "Loss: 1.985490083694458 [41472 / 60000]\n",
            "Loss: 1.9694815874099731 [41728 / 60000]\n",
            "Loss: 1.9715169668197632 [41984 / 60000]\n",
            "Loss: 1.981835126876831 [42240 / 60000]\n",
            "Loss: 1.9819121360778809 [42496 / 60000]\n",
            "Loss: 1.9827170372009277 [42752 / 60000]\n",
            "Loss: 1.9701255559921265 [43008 / 60000]\n",
            "Loss: 1.9547251462936401 [43264 / 60000]\n",
            "Loss: 1.9675617218017578 [43520 / 60000]\n",
            "Loss: 1.962071180343628 [43776 / 60000]\n",
            "Loss: 1.9675699472427368 [44032 / 60000]\n",
            "Loss: 1.970192790031433 [44288 / 60000]\n",
            "Loss: 1.9770078659057617 [44544 / 60000]\n",
            "Loss: 1.9991400241851807 [44800 / 60000]\n",
            "Loss: 1.9453918933868408 [45056 / 60000]\n",
            "Loss: 1.9599437713623047 [45312 / 60000]\n",
            "Loss: 1.951159119606018 [45568 / 60000]\n",
            "Loss: 1.9763637781143188 [45824 / 60000]\n",
            "Loss: 1.9835227727890015 [46080 / 60000]\n",
            "Loss: 1.9703447818756104 [46336 / 60000]\n",
            "Loss: 1.9387059211730957 [46592 / 60000]\n",
            "Loss: 1.9544615745544434 [46848 / 60000]\n",
            "Loss: 1.9924840927124023 [47104 / 60000]\n",
            "Loss: 1.954461693763733 [47360 / 60000]\n",
            "Loss: 1.9714744091033936 [47616 / 60000]\n",
            "Loss: 1.9745821952819824 [47872 / 60000]\n",
            "Loss: 1.9685066938400269 [48128 / 60000]\n",
            "Loss: 1.9649637937545776 [48384 / 60000]\n",
            "Loss: 1.9578802585601807 [48640 / 60000]\n",
            "Loss: 1.9389153718948364 [48896 / 60000]\n",
            "Loss: 1.9469422101974487 [49152 / 60000]\n",
            "Loss: 1.9566582441329956 [49408 / 60000]\n",
            "Loss: 1.9758341312408447 [49664 / 60000]\n",
            "Loss: 1.962390661239624 [49920 / 60000]\n",
            "Loss: 1.9706672430038452 [50176 / 60000]\n",
            "Loss: 1.9732377529144287 [50432 / 60000]\n",
            "Loss: 1.9599897861480713 [50688 / 60000]\n",
            "Loss: 1.9297723770141602 [50944 / 60000]\n",
            "Loss: 1.9396799802780151 [51200 / 60000]\n",
            "Loss: 1.9385020732879639 [51456 / 60000]\n",
            "Loss: 1.9408526420593262 [51712 / 60000]\n",
            "Loss: 1.9637755155563354 [51968 / 60000]\n",
            "Loss: 1.9926003217697144 [52224 / 60000]\n",
            "Loss: 1.961196780204773 [52480 / 60000]\n",
            "Loss: 1.949182152748108 [52736 / 60000]\n",
            "Loss: 1.946041226387024 [52992 / 60000]\n",
            "Loss: 1.9532155990600586 [53248 / 60000]\n",
            "Loss: 1.974848985671997 [53504 / 60000]\n",
            "Loss: 1.9811108112335205 [53760 / 60000]\n",
            "Loss: 1.9408338069915771 [54016 / 60000]\n",
            "Loss: 1.9640233516693115 [54272 / 60000]\n",
            "Loss: 1.933260440826416 [54528 / 60000]\n",
            "Loss: 1.944766640663147 [54784 / 60000]\n",
            "Loss: 1.9398918151855469 [55040 / 60000]\n",
            "Loss: 1.9423645734786987 [55296 / 60000]\n",
            "Loss: 1.9535839557647705 [55552 / 60000]\n",
            "Loss: 1.9688009023666382 [55808 / 60000]\n",
            "Loss: 1.9424312114715576 [56064 / 60000]\n",
            "Loss: 1.94536292552948 [56320 / 60000]\n",
            "Loss: 1.9415817260742188 [56576 / 60000]\n",
            "Loss: 1.9322729110717773 [56832 / 60000]\n",
            "Loss: 1.9634038209915161 [57088 / 60000]\n",
            "Loss: 1.9504551887512207 [57344 / 60000]\n",
            "Loss: 1.933542013168335 [57600 / 60000]\n",
            "Loss: 1.949432611465454 [57856 / 60000]\n",
            "Loss: 1.9562503099441528 [58112 / 60000]\n",
            "Loss: 1.9791998863220215 [58368 / 60000]\n",
            "Loss: 1.9507547616958618 [58624 / 60000]\n",
            "Loss: 1.948983073234558 [58880 / 60000]\n",
            "Loss: 1.951643943786621 [59136 / 60000]\n",
            "Loss: 1.9550423622131348 [59392 / 60000]\n",
            "Loss: 1.9713584184646606 [59648 / 60000]\n",
            "Loss: 1.9667304754257202 [22464 / 60000]\n",
            "Test Loss: 1.938153001666069 Accuracy:71.63000000000001\n",
            "epoch:7=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.9239321947097778 [0 / 60000]\n",
            "Loss: 1.98652184009552 [256 / 60000]\n",
            "Loss: 1.9493986368179321 [512 / 60000]\n",
            "Loss: 1.9345736503601074 [768 / 60000]\n",
            "Loss: 1.955155849456787 [1024 / 60000]\n",
            "Loss: 1.970680594444275 [1280 / 60000]\n",
            "Loss: 1.962209939956665 [1536 / 60000]\n",
            "Loss: 1.9456058740615845 [1792 / 60000]\n",
            "Loss: 1.956739902496338 [2048 / 60000]\n",
            "Loss: 1.9225095510482788 [2304 / 60000]\n",
            "Loss: 1.9596149921417236 [2560 / 60000]\n",
            "Loss: 1.9145476818084717 [2816 / 60000]\n",
            "Loss: 1.9434205293655396 [3072 / 60000]\n",
            "Loss: 1.9467055797576904 [3328 / 60000]\n",
            "Loss: 1.9283947944641113 [3584 / 60000]\n",
            "Loss: 1.928276538848877 [3840 / 60000]\n",
            "Loss: 1.9610459804534912 [4096 / 60000]\n",
            "Loss: 1.9605284929275513 [4352 / 60000]\n",
            "Loss: 1.9196618795394897 [4608 / 60000]\n",
            "Loss: 1.9270377159118652 [4864 / 60000]\n",
            "Loss: 1.9390509128570557 [5120 / 60000]\n",
            "Loss: 1.9403949975967407 [5376 / 60000]\n",
            "Loss: 1.942191481590271 [5632 / 60000]\n",
            "Loss: 1.9470874071121216 [5888 / 60000]\n",
            "Loss: 1.9407143592834473 [6144 / 60000]\n",
            "Loss: 1.9233330488204956 [6400 / 60000]\n",
            "Loss: 1.9676833152770996 [6656 / 60000]\n",
            "Loss: 1.931607723236084 [6912 / 60000]\n",
            "Loss: 1.9336345195770264 [7168 / 60000]\n",
            "Loss: 1.930297613143921 [7424 / 60000]\n",
            "Loss: 1.963147759437561 [7680 / 60000]\n",
            "Loss: 1.9417359828948975 [7936 / 60000]\n",
            "Loss: 1.9471452236175537 [8192 / 60000]\n",
            "Loss: 1.9418041706085205 [8448 / 60000]\n",
            "Loss: 1.929727554321289 [8704 / 60000]\n",
            "Loss: 1.9393726587295532 [8960 / 60000]\n",
            "Loss: 1.9576408863067627 [9216 / 60000]\n",
            "Loss: 1.919609546661377 [9472 / 60000]\n",
            "Loss: 1.937127709388733 [9728 / 60000]\n",
            "Loss: 1.9270577430725098 [9984 / 60000]\n",
            "Loss: 1.9508824348449707 [10240 / 60000]\n",
            "Loss: 1.9723740816116333 [10496 / 60000]\n",
            "Loss: 1.9250292778015137 [10752 / 60000]\n",
            "Loss: 1.9415233135223389 [11008 / 60000]\n",
            "Loss: 1.920851707458496 [11264 / 60000]\n",
            "Loss: 1.94273042678833 [11520 / 60000]\n",
            "Loss: 1.9437004327774048 [11776 / 60000]\n",
            "Loss: 1.9089335203170776 [12032 / 60000]\n",
            "Loss: 1.9185010194778442 [12288 / 60000]\n",
            "Loss: 1.9341833591461182 [12544 / 60000]\n",
            "Loss: 1.9362210035324097 [12800 / 60000]\n",
            "Loss: 1.9181917905807495 [13056 / 60000]\n",
            "Loss: 1.9427554607391357 [13312 / 60000]\n",
            "Loss: 1.93422532081604 [13568 / 60000]\n",
            "Loss: 1.9393484592437744 [13824 / 60000]\n",
            "Loss: 1.9183478355407715 [14080 / 60000]\n",
            "Loss: 1.9511916637420654 [14336 / 60000]\n",
            "Loss: 1.9199118614196777 [14592 / 60000]\n",
            "Loss: 1.9460374116897583 [14848 / 60000]\n",
            "Loss: 1.934025526046753 [15104 / 60000]\n",
            "Loss: 1.9322717189788818 [15360 / 60000]\n",
            "Loss: 1.9207268953323364 [15616 / 60000]\n",
            "Loss: 1.953465461730957 [15872 / 60000]\n",
            "Loss: 1.955633282661438 [16128 / 60000]\n",
            "Loss: 1.922857642173767 [16384 / 60000]\n",
            "Loss: 1.9432060718536377 [16640 / 60000]\n",
            "Loss: 1.9324456453323364 [16896 / 60000]\n",
            "Loss: 1.9401051998138428 [17152 / 60000]\n",
            "Loss: 1.9191629886627197 [17408 / 60000]\n",
            "Loss: 1.9246747493743896 [17664 / 60000]\n",
            "Loss: 1.8979300260543823 [17920 / 60000]\n",
            "Loss: 1.9203541278839111 [18176 / 60000]\n",
            "Loss: 1.907692551612854 [18432 / 60000]\n",
            "Loss: 1.9186487197875977 [18688 / 60000]\n",
            "Loss: 1.9313753843307495 [18944 / 60000]\n",
            "Loss: 1.934826135635376 [19200 / 60000]\n",
            "Loss: 1.9407137632369995 [19456 / 60000]\n",
            "Loss: 1.947283148765564 [19712 / 60000]\n",
            "Loss: 1.9218798875808716 [19968 / 60000]\n",
            "Loss: 1.9188653230667114 [20224 / 60000]\n",
            "Loss: 1.9050506353378296 [20480 / 60000]\n",
            "Loss: 1.9479138851165771 [20736 / 60000]\n",
            "Loss: 1.943245768547058 [20992 / 60000]\n",
            "Loss: 1.9300740957260132 [21248 / 60000]\n",
            "Loss: 1.9195808172225952 [21504 / 60000]\n",
            "Loss: 1.9060465097427368 [21760 / 60000]\n",
            "Loss: 1.8992220163345337 [22016 / 60000]\n",
            "Loss: 1.9490712881088257 [22272 / 60000]\n",
            "Loss: 1.9103580713272095 [22528 / 60000]\n",
            "Loss: 1.9237643480300903 [22784 / 60000]\n",
            "Loss: 1.936708927154541 [23040 / 60000]\n",
            "Loss: 1.9418412446975708 [23296 / 60000]\n",
            "Loss: 1.9264470338821411 [23552 / 60000]\n",
            "Loss: 1.9235198497772217 [23808 / 60000]\n",
            "Loss: 1.9088022708892822 [24064 / 60000]\n",
            "Loss: 1.915915846824646 [24320 / 60000]\n",
            "Loss: 1.885901689529419 [24576 / 60000]\n",
            "Loss: 1.914777398109436 [24832 / 60000]\n",
            "Loss: 1.9138662815093994 [25088 / 60000]\n",
            "Loss: 1.9073898792266846 [25344 / 60000]\n",
            "Loss: 1.9103273153305054 [25600 / 60000]\n",
            "Loss: 1.9183932542800903 [25856 / 60000]\n",
            "Loss: 1.8912172317504883 [26112 / 60000]\n",
            "Loss: 1.8856428861618042 [26368 / 60000]\n",
            "Loss: 1.9023054838180542 [26624 / 60000]\n",
            "Loss: 1.9342970848083496 [26880 / 60000]\n",
            "Loss: 1.9359385967254639 [27136 / 60000]\n",
            "Loss: 1.8955680131912231 [27392 / 60000]\n",
            "Loss: 1.919327974319458 [27648 / 60000]\n",
            "Loss: 1.928705096244812 [27904 / 60000]\n",
            "Loss: 1.9099383354187012 [28160 / 60000]\n",
            "Loss: 1.9192545413970947 [28416 / 60000]\n",
            "Loss: 1.9103807210922241 [28672 / 60000]\n",
            "Loss: 1.9171277284622192 [28928 / 60000]\n",
            "Loss: 1.944278359413147 [29184 / 60000]\n",
            "Loss: 1.8995777368545532 [29440 / 60000]\n",
            "Loss: 1.9003862142562866 [29696 / 60000]\n",
            "Loss: 1.927783727645874 [29952 / 60000]\n",
            "Loss: 1.8956631422042847 [30208 / 60000]\n",
            "Loss: 1.9481040239334106 [30464 / 60000]\n",
            "Loss: 1.9204277992248535 [30720 / 60000]\n",
            "Loss: 1.9110472202301025 [30976 / 60000]\n",
            "Loss: 1.9299633502960205 [31232 / 60000]\n",
            "Loss: 1.8825052976608276 [31488 / 60000]\n",
            "Loss: 1.9175598621368408 [31744 / 60000]\n",
            "Loss: 1.9197670221328735 [32000 / 60000]\n",
            "Loss: 1.9021615982055664 [32256 / 60000]\n",
            "Loss: 1.9152663946151733 [32512 / 60000]\n",
            "Loss: 1.8645031452178955 [32768 / 60000]\n",
            "Loss: 1.9119832515716553 [33024 / 60000]\n",
            "Loss: 1.896052360534668 [33280 / 60000]\n",
            "Loss: 1.9180999994277954 [33536 / 60000]\n",
            "Loss: 1.9049824476242065 [33792 / 60000]\n",
            "Loss: 1.8958654403686523 [34048 / 60000]\n",
            "Loss: 1.8897062540054321 [34304 / 60000]\n",
            "Loss: 1.886245846748352 [34560 / 60000]\n",
            "Loss: 1.911550760269165 [34816 / 60000]\n",
            "Loss: 1.8893736600875854 [35072 / 60000]\n",
            "Loss: 1.8992034196853638 [35328 / 60000]\n",
            "Loss: 1.915328860282898 [35584 / 60000]\n",
            "Loss: 1.9105602502822876 [35840 / 60000]\n",
            "Loss: 1.8963946104049683 [36096 / 60000]\n",
            "Loss: 1.8621257543563843 [36352 / 60000]\n",
            "Loss: 1.9263243675231934 [36608 / 60000]\n",
            "Loss: 1.9030344486236572 [36864 / 60000]\n",
            "Loss: 1.909740924835205 [37120 / 60000]\n",
            "Loss: 1.918485403060913 [37376 / 60000]\n",
            "Loss: 1.9313217401504517 [37632 / 60000]\n",
            "Loss: 1.9104869365692139 [37888 / 60000]\n",
            "Loss: 1.9259438514709473 [38144 / 60000]\n",
            "Loss: 1.8807967901229858 [38400 / 60000]\n",
            "Loss: 1.9172827005386353 [38656 / 60000]\n",
            "Loss: 1.9020884037017822 [38912 / 60000]\n",
            "Loss: 1.889504313468933 [39168 / 60000]\n",
            "Loss: 1.8964325189590454 [39424 / 60000]\n",
            "Loss: 1.9103347063064575 [39680 / 60000]\n",
            "Loss: 1.8978347778320312 [39936 / 60000]\n",
            "Loss: 1.9079244136810303 [40192 / 60000]\n",
            "Loss: 1.9087599515914917 [40448 / 60000]\n",
            "Loss: 1.884301781654358 [40704 / 60000]\n",
            "Loss: 1.9097541570663452 [40960 / 60000]\n",
            "Loss: 1.8946552276611328 [41216 / 60000]\n",
            "Loss: 1.9129416942596436 [41472 / 60000]\n",
            "Loss: 1.9206942319869995 [41728 / 60000]\n",
            "Loss: 1.9021902084350586 [41984 / 60000]\n",
            "Loss: 1.9123388528823853 [42240 / 60000]\n",
            "Loss: 1.8890434503555298 [42496 / 60000]\n",
            "Loss: 1.8378596305847168 [42752 / 60000]\n",
            "Loss: 1.881034255027771 [43008 / 60000]\n",
            "Loss: 1.8840067386627197 [43264 / 60000]\n",
            "Loss: 1.886481761932373 [43520 / 60000]\n",
            "Loss: 1.8977299928665161 [43776 / 60000]\n",
            "Loss: 1.877196192741394 [44032 / 60000]\n",
            "Loss: 1.8966346979141235 [44288 / 60000]\n",
            "Loss: 1.8854881525039673 [44544 / 60000]\n",
            "Loss: 1.8742296695709229 [44800 / 60000]\n",
            "Loss: 1.8863600492477417 [45056 / 60000]\n",
            "Loss: 1.8717879056930542 [45312 / 60000]\n",
            "Loss: 1.8903521299362183 [45568 / 60000]\n",
            "Loss: 1.9096823930740356 [45824 / 60000]\n",
            "Loss: 1.8826305866241455 [46080 / 60000]\n",
            "Loss: 1.8929415941238403 [46336 / 60000]\n",
            "Loss: 1.9019917249679565 [46592 / 60000]\n",
            "Loss: 1.885581374168396 [46848 / 60000]\n",
            "Loss: 1.912827730178833 [47104 / 60000]\n",
            "Loss: 1.9154863357543945 [47360 / 60000]\n",
            "Loss: 1.9197396039962769 [47616 / 60000]\n",
            "Loss: 1.870522379875183 [47872 / 60000]\n",
            "Loss: 1.8488993644714355 [48128 / 60000]\n",
            "Loss: 1.8955367803573608 [48384 / 60000]\n",
            "Loss: 1.8973195552825928 [48640 / 60000]\n",
            "Loss: 1.8543288707733154 [48896 / 60000]\n",
            "Loss: 1.8736956119537354 [49152 / 60000]\n",
            "Loss: 1.9209986925125122 [49408 / 60000]\n",
            "Loss: 1.8765777349472046 [49664 / 60000]\n",
            "Loss: 1.9177476167678833 [49920 / 60000]\n",
            "Loss: 1.8798810243606567 [50176 / 60000]\n",
            "Loss: 1.8792592287063599 [50432 / 60000]\n",
            "Loss: 1.8897645473480225 [50688 / 60000]\n",
            "Loss: 1.8940657377243042 [50944 / 60000]\n",
            "Loss: 1.9088643789291382 [51200 / 60000]\n",
            "Loss: 1.888475775718689 [51456 / 60000]\n",
            "Loss: 1.8872538805007935 [51712 / 60000]\n",
            "Loss: 1.8515007495880127 [51968 / 60000]\n",
            "Loss: 1.892174243927002 [52224 / 60000]\n",
            "Loss: 1.8866770267486572 [52480 / 60000]\n",
            "Loss: 1.8807697296142578 [52736 / 60000]\n",
            "Loss: 1.8502869606018066 [52992 / 60000]\n",
            "Loss: 1.886541724205017 [53248 / 60000]\n",
            "Loss: 1.8877134323120117 [53504 / 60000]\n",
            "Loss: 1.8934671878814697 [53760 / 60000]\n",
            "Loss: 1.870839238166809 [54016 / 60000]\n",
            "Loss: 1.8885290622711182 [54272 / 60000]\n",
            "Loss: 1.8723759651184082 [54528 / 60000]\n",
            "Loss: 1.8782130479812622 [54784 / 60000]\n",
            "Loss: 1.8826375007629395 [55040 / 60000]\n",
            "Loss: 1.878204107284546 [55296 / 60000]\n",
            "Loss: 1.8739380836486816 [55552 / 60000]\n",
            "Loss: 1.8386571407318115 [55808 / 60000]\n",
            "Loss: 1.8564318418502808 [56064 / 60000]\n",
            "Loss: 1.8994420766830444 [56320 / 60000]\n",
            "Loss: 1.8717401027679443 [56576 / 60000]\n",
            "Loss: 1.872113585472107 [56832 / 60000]\n",
            "Loss: 1.8732842206954956 [57088 / 60000]\n",
            "Loss: 1.885226845741272 [57344 / 60000]\n",
            "Loss: 1.8658910989761353 [57600 / 60000]\n",
            "Loss: 1.8741254806518555 [57856 / 60000]\n",
            "Loss: 1.8821661472320557 [58112 / 60000]\n",
            "Loss: 1.8840762376785278 [58368 / 60000]\n",
            "Loss: 1.8832124471664429 [58624 / 60000]\n",
            "Loss: 1.8735841512680054 [58880 / 60000]\n",
            "Loss: 1.8776541948318481 [59136 / 60000]\n",
            "Loss: 1.8290128707885742 [59392 / 60000]\n",
            "Loss: 1.8564791679382324 [59648 / 60000]\n",
            "Loss: 1.8870652914047241 [22464 / 60000]\n",
            "Test Loss: 1.8614744991064072 Accuracy:73.03\n",
            "epoch:8=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.8852676153182983 [0 / 60000]\n",
            "Loss: 1.8815889358520508 [256 / 60000]\n",
            "Loss: 1.8623292446136475 [512 / 60000]\n",
            "Loss: 1.8724865913391113 [768 / 60000]\n",
            "Loss: 1.9114348888397217 [1024 / 60000]\n",
            "Loss: 1.8657417297363281 [1280 / 60000]\n",
            "Loss: 1.8871731758117676 [1536 / 60000]\n",
            "Loss: 1.8932702541351318 [1792 / 60000]\n",
            "Loss: 1.9021461009979248 [2048 / 60000]\n",
            "Loss: 1.878305435180664 [2304 / 60000]\n",
            "Loss: 1.8702850341796875 [2560 / 60000]\n",
            "Loss: 1.8676937818527222 [2816 / 60000]\n",
            "Loss: 1.8627630472183228 [3072 / 60000]\n",
            "Loss: 1.8638719320297241 [3328 / 60000]\n",
            "Loss: 1.8395799398422241 [3584 / 60000]\n",
            "Loss: 1.8583102226257324 [3840 / 60000]\n",
            "Loss: 1.8458045721054077 [4096 / 60000]\n",
            "Loss: 1.8556618690490723 [4352 / 60000]\n",
            "Loss: 1.9027795791625977 [4608 / 60000]\n",
            "Loss: 1.8563084602355957 [4864 / 60000]\n",
            "Loss: 1.8314799070358276 [5120 / 60000]\n",
            "Loss: 1.8903470039367676 [5376 / 60000]\n",
            "Loss: 1.874708652496338 [5632 / 60000]\n",
            "Loss: 1.8776715993881226 [5888 / 60000]\n",
            "Loss: 1.8600598573684692 [6144 / 60000]\n",
            "Loss: 1.8439483642578125 [6400 / 60000]\n",
            "Loss: 1.8450216054916382 [6656 / 60000]\n",
            "Loss: 1.8403123617172241 [6912 / 60000]\n",
            "Loss: 1.8569613695144653 [7168 / 60000]\n",
            "Loss: 1.8409234285354614 [7424 / 60000]\n",
            "Loss: 1.850457787513733 [7680 / 60000]\n",
            "Loss: 1.849958896636963 [7936 / 60000]\n",
            "Loss: 1.8527814149856567 [8192 / 60000]\n",
            "Loss: 1.8670148849487305 [8448 / 60000]\n",
            "Loss: 1.863991379737854 [8704 / 60000]\n",
            "Loss: 1.8792494535446167 [8960 / 60000]\n",
            "Loss: 1.87444007396698 [9216 / 60000]\n",
            "Loss: 1.8504021167755127 [9472 / 60000]\n",
            "Loss: 1.8612825870513916 [9728 / 60000]\n",
            "Loss: 1.8541048765182495 [9984 / 60000]\n",
            "Loss: 1.8681256771087646 [10240 / 60000]\n",
            "Loss: 1.8429149389266968 [10496 / 60000]\n",
            "Loss: 1.855249285697937 [10752 / 60000]\n",
            "Loss: 1.8619487285614014 [11008 / 60000]\n",
            "Loss: 1.8733844757080078 [11264 / 60000]\n",
            "Loss: 1.8556219339370728 [11520 / 60000]\n",
            "Loss: 1.8517684936523438 [11776 / 60000]\n",
            "Loss: 1.8459562063217163 [12032 / 60000]\n",
            "Loss: 1.8519699573516846 [12288 / 60000]\n",
            "Loss: 1.8700541257858276 [12544 / 60000]\n",
            "Loss: 1.8509197235107422 [12800 / 60000]\n",
            "Loss: 1.8350201845169067 [13056 / 60000]\n",
            "Loss: 1.8785792589187622 [13312 / 60000]\n",
            "Loss: 1.8258533477783203 [13568 / 60000]\n",
            "Loss: 1.8429023027420044 [13824 / 60000]\n",
            "Loss: 1.886746883392334 [14080 / 60000]\n",
            "Loss: 1.8631609678268433 [14336 / 60000]\n",
            "Loss: 1.8559972047805786 [14592 / 60000]\n",
            "Loss: 1.8557316064834595 [14848 / 60000]\n",
            "Loss: 1.8191447257995605 [15104 / 60000]\n",
            "Loss: 1.8341869115829468 [15360 / 60000]\n",
            "Loss: 1.8633562326431274 [15616 / 60000]\n",
            "Loss: 1.8651632070541382 [15872 / 60000]\n",
            "Loss: 1.8420828580856323 [16128 / 60000]\n",
            "Loss: 1.8892409801483154 [16384 / 60000]\n",
            "Loss: 1.8766487836837769 [16640 / 60000]\n",
            "Loss: 1.8555688858032227 [16896 / 60000]\n",
            "Loss: 1.8639329671859741 [17152 / 60000]\n",
            "Loss: 1.8733909130096436 [17408 / 60000]\n",
            "Loss: 1.8308281898498535 [17664 / 60000]\n",
            "Loss: 1.8265762329101562 [17920 / 60000]\n",
            "Loss: 1.820006012916565 [18176 / 60000]\n",
            "Loss: 1.857290506362915 [18432 / 60000]\n",
            "Loss: 1.8211952447891235 [18688 / 60000]\n",
            "Loss: 1.8371673822402954 [18944 / 60000]\n",
            "Loss: 1.8200987577438354 [19200 / 60000]\n",
            "Loss: 1.83493971824646 [19456 / 60000]\n",
            "Loss: 1.8444433212280273 [19712 / 60000]\n",
            "Loss: 1.8124183416366577 [19968 / 60000]\n",
            "Loss: 1.8610858917236328 [20224 / 60000]\n",
            "Loss: 1.8835476636886597 [20480 / 60000]\n",
            "Loss: 1.82561194896698 [20736 / 60000]\n",
            "Loss: 1.8590446710586548 [20992 / 60000]\n",
            "Loss: 1.8482146263122559 [21248 / 60000]\n",
            "Loss: 1.8417657613754272 [21504 / 60000]\n",
            "Loss: 1.8501957654953003 [21760 / 60000]\n",
            "Loss: 1.869248628616333 [22016 / 60000]\n",
            "Loss: 1.8456668853759766 [22272 / 60000]\n",
            "Loss: 1.8404549360275269 [22528 / 60000]\n",
            "Loss: 1.816689372062683 [22784 / 60000]\n",
            "Loss: 1.8252156972885132 [23040 / 60000]\n",
            "Loss: 1.8363858461380005 [23296 / 60000]\n",
            "Loss: 1.8408454656600952 [23552 / 60000]\n",
            "Loss: 1.7984329462051392 [23808 / 60000]\n",
            "Loss: 1.840795636177063 [24064 / 60000]\n",
            "Loss: 1.8418830633163452 [24320 / 60000]\n",
            "Loss: 1.8314777612686157 [24576 / 60000]\n",
            "Loss: 1.8739680051803589 [24832 / 60000]\n",
            "Loss: 1.8338440656661987 [25088 / 60000]\n",
            "Loss: 1.8758935928344727 [25344 / 60000]\n",
            "Loss: 1.873624563217163 [25600 / 60000]\n",
            "Loss: 1.861733078956604 [25856 / 60000]\n",
            "Loss: 1.8765653371810913 [26112 / 60000]\n",
            "Loss: 1.8295862674713135 [26368 / 60000]\n",
            "Loss: 1.8642494678497314 [26624 / 60000]\n",
            "Loss: 1.8706902265548706 [26880 / 60000]\n",
            "Loss: 1.8459471464157104 [27136 / 60000]\n",
            "Loss: 1.8523118495941162 [27392 / 60000]\n",
            "Loss: 1.835168480873108 [27648 / 60000]\n",
            "Loss: 1.8323267698287964 [27904 / 60000]\n",
            "Loss: 1.7994554042816162 [28160 / 60000]\n",
            "Loss: 1.8377379179000854 [28416 / 60000]\n",
            "Loss: 1.8540680408477783 [28672 / 60000]\n",
            "Loss: 1.8624581098556519 [28928 / 60000]\n",
            "Loss: 1.8386057615280151 [29184 / 60000]\n",
            "Loss: 1.8285492658615112 [29440 / 60000]\n",
            "Loss: 1.8311214447021484 [29696 / 60000]\n",
            "Loss: 1.8345668315887451 [29952 / 60000]\n",
            "Loss: 1.8410626649856567 [30208 / 60000]\n",
            "Loss: 1.8241987228393555 [30464 / 60000]\n",
            "Loss: 1.825226902961731 [30720 / 60000]\n",
            "Loss: 1.8277913331985474 [30976 / 60000]\n",
            "Loss: 1.831578254699707 [31232 / 60000]\n",
            "Loss: 1.8510723114013672 [31488 / 60000]\n",
            "Loss: 1.8443964719772339 [31744 / 60000]\n",
            "Loss: 1.8138774633407593 [32000 / 60000]\n",
            "Loss: 1.8061199188232422 [32256 / 60000]\n",
            "Loss: 1.8204342126846313 [32512 / 60000]\n",
            "Loss: 1.7936911582946777 [32768 / 60000]\n",
            "Loss: 1.8457844257354736 [33024 / 60000]\n",
            "Loss: 1.813199758529663 [33280 / 60000]\n",
            "Loss: 1.8292938470840454 [33536 / 60000]\n",
            "Loss: 1.8394713401794434 [33792 / 60000]\n",
            "Loss: 1.8290131092071533 [34048 / 60000]\n",
            "Loss: 1.852756142616272 [34304 / 60000]\n",
            "Loss: 1.8293956518173218 [34560 / 60000]\n",
            "Loss: 1.841117262840271 [34816 / 60000]\n",
            "Loss: 1.8532705307006836 [35072 / 60000]\n",
            "Loss: 1.806950569152832 [35328 / 60000]\n",
            "Loss: 1.8738009929656982 [35584 / 60000]\n",
            "Loss: 1.808060646057129 [35840 / 60000]\n",
            "Loss: 1.8252769708633423 [36096 / 60000]\n",
            "Loss: 1.8149805068969727 [36352 / 60000]\n",
            "Loss: 1.8075932264328003 [36608 / 60000]\n",
            "Loss: 1.7956814765930176 [36864 / 60000]\n",
            "Loss: 1.8233884572982788 [37120 / 60000]\n",
            "Loss: 1.8201184272766113 [37376 / 60000]\n",
            "Loss: 1.8148915767669678 [37632 / 60000]\n",
            "Loss: 1.8211075067520142 [37888 / 60000]\n",
            "Loss: 1.820529580116272 [38144 / 60000]\n",
            "Loss: 1.827558994293213 [38400 / 60000]\n",
            "Loss: 1.829892873764038 [38656 / 60000]\n",
            "Loss: 1.8416168689727783 [38912 / 60000]\n",
            "Loss: 1.862925410270691 [39168 / 60000]\n",
            "Loss: 1.8054763078689575 [39424 / 60000]\n",
            "Loss: 1.8029050827026367 [39680 / 60000]\n",
            "Loss: 1.8234943151474 [39936 / 60000]\n",
            "Loss: 1.7872484922409058 [40192 / 60000]\n",
            "Loss: 1.8231192827224731 [40448 / 60000]\n",
            "Loss: 1.8136810064315796 [40704 / 60000]\n",
            "Loss: 1.7940317392349243 [40960 / 60000]\n",
            "Loss: 1.784140706062317 [41216 / 60000]\n",
            "Loss: 1.8216629028320312 [41472 / 60000]\n",
            "Loss: 1.8268392086029053 [41728 / 60000]\n",
            "Loss: 1.8146127462387085 [41984 / 60000]\n",
            "Loss: 1.8394935131072998 [42240 / 60000]\n",
            "Loss: 1.839328408241272 [42496 / 60000]\n",
            "Loss: 1.8370099067687988 [42752 / 60000]\n",
            "Loss: 1.8255983591079712 [43008 / 60000]\n",
            "Loss: 1.8194756507873535 [43264 / 60000]\n",
            "Loss: 1.8084477186203003 [43520 / 60000]\n",
            "Loss: 1.8124873638153076 [43776 / 60000]\n",
            "Loss: 1.8237156867980957 [44032 / 60000]\n",
            "Loss: 1.773797869682312 [44288 / 60000]\n",
            "Loss: 1.8092377185821533 [44544 / 60000]\n",
            "Loss: 1.7941577434539795 [44800 / 60000]\n",
            "Loss: 1.8361361026763916 [45056 / 60000]\n",
            "Loss: 1.8389971256256104 [45312 / 60000]\n",
            "Loss: 1.8051443099975586 [45568 / 60000]\n",
            "Loss: 1.7906070947647095 [45824 / 60000]\n",
            "Loss: 1.8215417861938477 [46080 / 60000]\n",
            "Loss: 1.8337606191635132 [46336 / 60000]\n",
            "Loss: 1.8176820278167725 [46592 / 60000]\n",
            "Loss: 1.7912970781326294 [46848 / 60000]\n",
            "Loss: 1.8500118255615234 [47104 / 60000]\n",
            "Loss: 1.7838252782821655 [47360 / 60000]\n",
            "Loss: 1.8382117748260498 [47616 / 60000]\n",
            "Loss: 1.8083419799804688 [47872 / 60000]\n",
            "Loss: 1.8267334699630737 [48128 / 60000]\n",
            "Loss: 1.8340812921524048 [48384 / 60000]\n",
            "Loss: 1.8129265308380127 [48640 / 60000]\n",
            "Loss: 1.815864086151123 [48896 / 60000]\n",
            "Loss: 1.8154487609863281 [49152 / 60000]\n",
            "Loss: 1.8105310201644897 [49408 / 60000]\n",
            "Loss: 1.7871500253677368 [49664 / 60000]\n",
            "Loss: 1.8017165660858154 [49920 / 60000]\n",
            "Loss: 1.8336442708969116 [50176 / 60000]\n",
            "Loss: 1.8251748085021973 [50432 / 60000]\n",
            "Loss: 1.8336745500564575 [50688 / 60000]\n",
            "Loss: 1.774921178817749 [50944 / 60000]\n",
            "Loss: 1.7918760776519775 [51200 / 60000]\n",
            "Loss: 1.802870512008667 [51456 / 60000]\n",
            "Loss: 1.7800899744033813 [51712 / 60000]\n",
            "Loss: 1.799560308456421 [51968 / 60000]\n",
            "Loss: 1.7955999374389648 [52224 / 60000]\n",
            "Loss: 1.825141191482544 [52480 / 60000]\n",
            "Loss: 1.8367029428482056 [52736 / 60000]\n",
            "Loss: 1.8311772346496582 [52992 / 60000]\n",
            "Loss: 1.7718493938446045 [53248 / 60000]\n",
            "Loss: 1.8186938762664795 [53504 / 60000]\n",
            "Loss: 1.816658854484558 [53760 / 60000]\n",
            "Loss: 1.7708901166915894 [54016 / 60000]\n",
            "Loss: 1.8220936059951782 [54272 / 60000]\n",
            "Loss: 1.813504934310913 [54528 / 60000]\n",
            "Loss: 1.7842310667037964 [54784 / 60000]\n",
            "Loss: 1.8115031719207764 [55040 / 60000]\n",
            "Loss: 1.7997912168502808 [55296 / 60000]\n",
            "Loss: 1.8100992441177368 [55552 / 60000]\n",
            "Loss: 1.8157498836517334 [55808 / 60000]\n",
            "Loss: 1.7907710075378418 [56064 / 60000]\n",
            "Loss: 1.8437774181365967 [56320 / 60000]\n",
            "Loss: 1.7974613904953003 [56576 / 60000]\n",
            "Loss: 1.7779583930969238 [56832 / 60000]\n",
            "Loss: 1.766506552696228 [57088 / 60000]\n",
            "Loss: 1.804430603981018 [57344 / 60000]\n",
            "Loss: 1.7619013786315918 [57600 / 60000]\n",
            "Loss: 1.8216392993927002 [57856 / 60000]\n",
            "Loss: 1.8101341724395752 [58112 / 60000]\n",
            "Loss: 1.7981932163238525 [58368 / 60000]\n",
            "Loss: 1.7948753833770752 [58624 / 60000]\n",
            "Loss: 1.7690978050231934 [58880 / 60000]\n",
            "Loss: 1.800878643989563 [59136 / 60000]\n",
            "Loss: 1.7613991498947144 [59392 / 60000]\n",
            "Loss: 1.790107250213623 [59648 / 60000]\n",
            "Loss: 1.8291398286819458 [22464 / 60000]\n",
            "Test Loss: 1.7804487645626068 Accuracy:74.08\n",
            "epoch:9=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.7649915218353271 [0 / 60000]\n",
            "Loss: 1.804473638534546 [256 / 60000]\n",
            "Loss: 1.7810004949569702 [512 / 60000]\n",
            "Loss: 1.7689101696014404 [768 / 60000]\n",
            "Loss: 1.8059455156326294 [1024 / 60000]\n",
            "Loss: 1.801690697669983 [1280 / 60000]\n",
            "Loss: 1.7921836376190186 [1536 / 60000]\n",
            "Loss: 1.7940516471862793 [1792 / 60000]\n",
            "Loss: 1.8019442558288574 [2048 / 60000]\n",
            "Loss: 1.7650306224822998 [2304 / 60000]\n",
            "Loss: 1.8385342359542847 [2560 / 60000]\n",
            "Loss: 1.7876101732254028 [2816 / 60000]\n",
            "Loss: 1.7820180654525757 [3072 / 60000]\n",
            "Loss: 1.7813780307769775 [3328 / 60000]\n",
            "Loss: 1.7971690893173218 [3584 / 60000]\n",
            "Loss: 1.8117340803146362 [3840 / 60000]\n",
            "Loss: 1.814717173576355 [4096 / 60000]\n",
            "Loss: 1.7778921127319336 [4352 / 60000]\n",
            "Loss: 1.7391743659973145 [4608 / 60000]\n",
            "Loss: 1.7861586809158325 [4864 / 60000]\n",
            "Loss: 1.8057329654693604 [5120 / 60000]\n",
            "Loss: 1.7575955390930176 [5376 / 60000]\n",
            "Loss: 1.7656487226486206 [5632 / 60000]\n",
            "Loss: 1.7343395948410034 [5888 / 60000]\n",
            "Loss: 1.7821483612060547 [6144 / 60000]\n",
            "Loss: 1.7813324928283691 [6400 / 60000]\n",
            "Loss: 1.7902568578720093 [6656 / 60000]\n",
            "Loss: 1.7901039123535156 [6912 / 60000]\n",
            "Loss: 1.7885727882385254 [7168 / 60000]\n",
            "Loss: 1.8086283206939697 [7424 / 60000]\n",
            "Loss: 1.8112812042236328 [7680 / 60000]\n",
            "Loss: 1.7705585956573486 [7936 / 60000]\n",
            "Loss: 1.7700202465057373 [8192 / 60000]\n",
            "Loss: 1.7838215827941895 [8448 / 60000]\n",
            "Loss: 1.8040332794189453 [8704 / 60000]\n",
            "Loss: 1.7969547510147095 [8960 / 60000]\n",
            "Loss: 1.7664530277252197 [9216 / 60000]\n",
            "Loss: 1.7516396045684814 [9472 / 60000]\n",
            "Loss: 1.781510591506958 [9728 / 60000]\n",
            "Loss: 1.7829475402832031 [9984 / 60000]\n",
            "Loss: 1.7361249923706055 [10240 / 60000]\n",
            "Loss: 1.8022384643554688 [10496 / 60000]\n",
            "Loss: 1.7700778245925903 [10752 / 60000]\n",
            "Loss: 1.7954390048980713 [11008 / 60000]\n",
            "Loss: 1.7969573736190796 [11264 / 60000]\n",
            "Loss: 1.7872084379196167 [11520 / 60000]\n",
            "Loss: 1.7670937776565552 [11776 / 60000]\n",
            "Loss: 1.7737021446228027 [12032 / 60000]\n",
            "Loss: 1.7859539985656738 [12288 / 60000]\n",
            "Loss: 1.7622277736663818 [12544 / 60000]\n",
            "Loss: 1.7996331453323364 [12800 / 60000]\n",
            "Loss: 1.7791805267333984 [13056 / 60000]\n",
            "Loss: 1.7439777851104736 [13312 / 60000]\n",
            "Loss: 1.7291911840438843 [13568 / 60000]\n",
            "Loss: 1.77473783493042 [13824 / 60000]\n",
            "Loss: 1.7380216121673584 [14080 / 60000]\n",
            "Loss: 1.7784123420715332 [14336 / 60000]\n",
            "Loss: 1.7804286479949951 [14592 / 60000]\n",
            "Loss: 1.7918716669082642 [14848 / 60000]\n",
            "Loss: 1.8057951927185059 [15104 / 60000]\n",
            "Loss: 1.7685695886611938 [15360 / 60000]\n",
            "Loss: 1.802176833152771 [15616 / 60000]\n",
            "Loss: 1.7712947130203247 [15872 / 60000]\n",
            "Loss: 1.7717485427856445 [16128 / 60000]\n",
            "Loss: 1.7534124851226807 [16384 / 60000]\n",
            "Loss: 1.7694567441940308 [16640 / 60000]\n",
            "Loss: 1.7626194953918457 [16896 / 60000]\n",
            "Loss: 1.7928858995437622 [17152 / 60000]\n",
            "Loss: 1.738254189491272 [17408 / 60000]\n",
            "Loss: 1.7623239755630493 [17664 / 60000]\n",
            "Loss: 1.7663037776947021 [17920 / 60000]\n",
            "Loss: 1.7797266244888306 [18176 / 60000]\n",
            "Loss: 1.7622525691986084 [18432 / 60000]\n",
            "Loss: 1.794311761856079 [18688 / 60000]\n",
            "Loss: 1.7987737655639648 [18944 / 60000]\n",
            "Loss: 1.7650387287139893 [19200 / 60000]\n",
            "Loss: 1.7938086986541748 [19456 / 60000]\n",
            "Loss: 1.8002803325653076 [19712 / 60000]\n",
            "Loss: 1.7568048238754272 [19968 / 60000]\n",
            "Loss: 1.7339441776275635 [20224 / 60000]\n",
            "Loss: 1.7689800262451172 [20480 / 60000]\n",
            "Loss: 1.7711212635040283 [20736 / 60000]\n",
            "Loss: 1.7703965902328491 [20992 / 60000]\n",
            "Loss: 1.7511069774627686 [21248 / 60000]\n",
            "Loss: 1.7707663774490356 [21504 / 60000]\n",
            "Loss: 1.7372987270355225 [21760 / 60000]\n",
            "Loss: 1.7446377277374268 [22016 / 60000]\n",
            "Loss: 1.7733107805252075 [22272 / 60000]\n",
            "Loss: 1.7592562437057495 [22528 / 60000]\n",
            "Loss: 1.7680317163467407 [22784 / 60000]\n",
            "Loss: 1.7525928020477295 [23040 / 60000]\n",
            "Loss: 1.7599358558654785 [23296 / 60000]\n",
            "Loss: 1.7664899826049805 [23552 / 60000]\n",
            "Loss: 1.7558034658432007 [23808 / 60000]\n",
            "Loss: 1.7568397521972656 [24064 / 60000]\n",
            "Loss: 1.7427493333816528 [24320 / 60000]\n",
            "Loss: 1.7744789123535156 [24576 / 60000]\n",
            "Loss: 1.7529065608978271 [24832 / 60000]\n",
            "Loss: 1.7544174194335938 [25088 / 60000]\n",
            "Loss: 1.7925360202789307 [25344 / 60000]\n",
            "Loss: 1.760491967201233 [25600 / 60000]\n",
            "Loss: 1.7538176774978638 [25856 / 60000]\n",
            "Loss: 1.741995096206665 [26112 / 60000]\n",
            "Loss: 1.7470157146453857 [26368 / 60000]\n",
            "Loss: 1.737183690071106 [26624 / 60000]\n",
            "Loss: 1.7755674123764038 [26880 / 60000]\n",
            "Loss: 1.7455112934112549 [27136 / 60000]\n",
            "Loss: 1.7683225870132446 [27392 / 60000]\n",
            "Loss: 1.7271873950958252 [27648 / 60000]\n",
            "Loss: 1.7563143968582153 [27904 / 60000]\n",
            "Loss: 1.7440557479858398 [28160 / 60000]\n",
            "Loss: 1.7700936794281006 [28416 / 60000]\n",
            "Loss: 1.7239891290664673 [28672 / 60000]\n",
            "Loss: 1.7362794876098633 [28928 / 60000]\n",
            "Loss: 1.759189248085022 [29184 / 60000]\n",
            "Loss: 1.78472900390625 [29440 / 60000]\n",
            "Loss: 1.7490594387054443 [29696 / 60000]\n",
            "Loss: 1.7735612392425537 [29952 / 60000]\n",
            "Loss: 1.768139123916626 [30208 / 60000]\n",
            "Loss: 1.7854437828063965 [30464 / 60000]\n",
            "Loss: 1.756636142730713 [30720 / 60000]\n",
            "Loss: 1.7563844919204712 [30976 / 60000]\n",
            "Loss: 1.7441123723983765 [31232 / 60000]\n",
            "Loss: 1.7576566934585571 [31488 / 60000]\n",
            "Loss: 1.7590713500976562 [31744 / 60000]\n",
            "Loss: 1.7304261922836304 [32000 / 60000]\n",
            "Loss: 1.7077462673187256 [32256 / 60000]\n",
            "Loss: 1.7569822072982788 [32512 / 60000]\n",
            "Loss: 1.7883542776107788 [32768 / 60000]\n",
            "Loss: 1.7551498413085938 [33024 / 60000]\n",
            "Loss: 1.747307300567627 [33280 / 60000]\n",
            "Loss: 1.7575953006744385 [33536 / 60000]\n",
            "Loss: 1.7157810926437378 [33792 / 60000]\n",
            "Loss: 1.7558672428131104 [34048 / 60000]\n",
            "Loss: 1.7777888774871826 [34304 / 60000]\n",
            "Loss: 1.7401175498962402 [34560 / 60000]\n",
            "Loss: 1.7164759635925293 [34816 / 60000]\n",
            "Loss: 1.7258473634719849 [35072 / 60000]\n",
            "Loss: 1.7632008790969849 [35328 / 60000]\n",
            "Loss: 1.7403560876846313 [35584 / 60000]\n",
            "Loss: 1.7628751993179321 [35840 / 60000]\n",
            "Loss: 1.7353845834732056 [36096 / 60000]\n",
            "Loss: 1.7413955926895142 [36352 / 60000]\n",
            "Loss: 1.737791895866394 [36608 / 60000]\n",
            "Loss: 1.73184072971344 [36864 / 60000]\n",
            "Loss: 1.7362133264541626 [37120 / 60000]\n",
            "Loss: 1.6983968019485474 [37376 / 60000]\n",
            "Loss: 1.7359291315078735 [37632 / 60000]\n",
            "Loss: 1.7270827293395996 [37888 / 60000]\n",
            "Loss: 1.7369879484176636 [38144 / 60000]\n",
            "Loss: 1.7491066455841064 [38400 / 60000]\n",
            "Loss: 1.7655808925628662 [38656 / 60000]\n",
            "Loss: 1.7096134424209595 [38912 / 60000]\n",
            "Loss: 1.7913727760314941 [39168 / 60000]\n",
            "Loss: 1.8147848844528198 [39424 / 60000]\n",
            "Loss: 1.7435954809188843 [39680 / 60000]\n",
            "Loss: 1.729192852973938 [39936 / 60000]\n",
            "Loss: 1.7524182796478271 [40192 / 60000]\n",
            "Loss: 1.7728803157806396 [40448 / 60000]\n",
            "Loss: 1.7549465894699097 [40704 / 60000]\n",
            "Loss: 1.7482340335845947 [40960 / 60000]\n",
            "Loss: 1.7249761819839478 [41216 / 60000]\n",
            "Loss: 1.7219884395599365 [41472 / 60000]\n",
            "Loss: 1.7048656940460205 [41728 / 60000]\n",
            "Loss: 1.7208441495895386 [41984 / 60000]\n",
            "Loss: 1.7573318481445312 [42240 / 60000]\n",
            "Loss: 1.727777123451233 [42496 / 60000]\n",
            "Loss: 1.7520294189453125 [42752 / 60000]\n",
            "Loss: 1.7122761011123657 [43008 / 60000]\n",
            "Loss: 1.746935248374939 [43264 / 60000]\n",
            "Loss: 1.7271485328674316 [43520 / 60000]\n",
            "Loss: 1.707074761390686 [43776 / 60000]\n",
            "Loss: 1.7672637701034546 [44032 / 60000]\n",
            "Loss: 1.7132760286331177 [44288 / 60000]\n",
            "Loss: 1.7051548957824707 [44544 / 60000]\n",
            "Loss: 1.7694085836410522 [44800 / 60000]\n",
            "Loss: 1.7382665872573853 [45056 / 60000]\n",
            "Loss: 1.7167158126831055 [45312 / 60000]\n",
            "Loss: 1.7400668859481812 [45568 / 60000]\n",
            "Loss: 1.7417429685592651 [45824 / 60000]\n",
            "Loss: 1.7294610738754272 [46080 / 60000]\n",
            "Loss: 1.7744145393371582 [46336 / 60000]\n",
            "Loss: 1.733614444732666 [46592 / 60000]\n",
            "Loss: 1.7252846956253052 [46848 / 60000]\n",
            "Loss: 1.7487871646881104 [47104 / 60000]\n",
            "Loss: 1.732310175895691 [47360 / 60000]\n",
            "Loss: 1.7323529720306396 [47616 / 60000]\n",
            "Loss: 1.7052819728851318 [47872 / 60000]\n",
            "Loss: 1.756097435951233 [48128 / 60000]\n",
            "Loss: 1.7356469631195068 [48384 / 60000]\n",
            "Loss: 1.764715552330017 [48640 / 60000]\n",
            "Loss: 1.7368345260620117 [48896 / 60000]\n",
            "Loss: 1.7178436517715454 [49152 / 60000]\n",
            "Loss: 1.6852425336837769 [49408 / 60000]\n",
            "Loss: 1.7586137056350708 [49664 / 60000]\n",
            "Loss: 1.721482515335083 [49920 / 60000]\n",
            "Loss: 1.7460578680038452 [50176 / 60000]\n",
            "Loss: 1.7246776819229126 [50432 / 60000]\n",
            "Loss: 1.7366595268249512 [50688 / 60000]\n",
            "Loss: 1.7368230819702148 [50944 / 60000]\n",
            "Loss: 1.7438870668411255 [51200 / 60000]\n",
            "Loss: 1.751381754875183 [51456 / 60000]\n",
            "Loss: 1.7272816896438599 [51712 / 60000]\n",
            "Loss: 1.6934621334075928 [51968 / 60000]\n",
            "Loss: 1.7186987400054932 [52224 / 60000]\n",
            "Loss: 1.73070228099823 [52480 / 60000]\n",
            "Loss: 1.7183518409729004 [52736 / 60000]\n",
            "Loss: 1.7012138366699219 [52992 / 60000]\n",
            "Loss: 1.7129489183425903 [53248 / 60000]\n",
            "Loss: 1.748278260231018 [53504 / 60000]\n",
            "Loss: 1.7076802253723145 [53760 / 60000]\n",
            "Loss: 1.736712098121643 [54016 / 60000]\n",
            "Loss: 1.7193032503128052 [54272 / 60000]\n",
            "Loss: 1.6895596981048584 [54528 / 60000]\n",
            "Loss: 1.7130260467529297 [54784 / 60000]\n",
            "Loss: 1.69808030128479 [55040 / 60000]\n",
            "Loss: 1.7020909786224365 [55296 / 60000]\n",
            "Loss: 1.6812750101089478 [55552 / 60000]\n",
            "Loss: 1.716192364692688 [55808 / 60000]\n",
            "Loss: 1.699022650718689 [56064 / 60000]\n",
            "Loss: 1.689143419265747 [56320 / 60000]\n",
            "Loss: 1.7334767580032349 [56576 / 60000]\n",
            "Loss: 1.7633161544799805 [56832 / 60000]\n",
            "Loss: 1.7075926065444946 [57088 / 60000]\n",
            "Loss: 1.7138429880142212 [57344 / 60000]\n",
            "Loss: 1.73392915725708 [57600 / 60000]\n",
            "Loss: 1.7257252931594849 [57856 / 60000]\n",
            "Loss: 1.7146129608154297 [58112 / 60000]\n",
            "Loss: 1.7115806341171265 [58368 / 60000]\n",
            "Loss: 1.7321572303771973 [58624 / 60000]\n",
            "Loss: 1.7254754304885864 [58880 / 60000]\n",
            "Loss: 1.7082159519195557 [59136 / 60000]\n",
            "Loss: 1.7167894840240479 [59392 / 60000]\n",
            "Loss: 1.7082008123397827 [59648 / 60000]\n",
            "Loss: 1.750876545906067 [22464 / 60000]\n",
            "Test Loss: 1.696400409936905 Accuracy:75.02\n",
            "epoch:10=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.7139157056808472 [0 / 60000]\n",
            "Loss: 1.7271150350570679 [256 / 60000]\n",
            "Loss: 1.7063877582550049 [512 / 60000]\n",
            "Loss: 1.7136954069137573 [768 / 60000]\n",
            "Loss: 1.7218430042266846 [1024 / 60000]\n",
            "Loss: 1.6987029314041138 [1280 / 60000]\n",
            "Loss: 1.7313098907470703 [1536 / 60000]\n",
            "Loss: 1.7155545949935913 [1792 / 60000]\n",
            "Loss: 1.6990855932235718 [2048 / 60000]\n",
            "Loss: 1.6814019680023193 [2304 / 60000]\n",
            "Loss: 1.707356333732605 [2560 / 60000]\n",
            "Loss: 1.7207227945327759 [2816 / 60000]\n",
            "Loss: 1.7284331321716309 [3072 / 60000]\n",
            "Loss: 1.7383488416671753 [3328 / 60000]\n",
            "Loss: 1.686895728111267 [3584 / 60000]\n",
            "Loss: 1.70806884765625 [3840 / 60000]\n",
            "Loss: 1.7019529342651367 [4096 / 60000]\n",
            "Loss: 1.7553534507751465 [4352 / 60000]\n",
            "Loss: 1.680362582206726 [4608 / 60000]\n",
            "Loss: 1.6660305261611938 [4864 / 60000]\n",
            "Loss: 1.712988257408142 [5120 / 60000]\n",
            "Loss: 1.7216851711273193 [5376 / 60000]\n",
            "Loss: 1.6899995803833008 [5632 / 60000]\n",
            "Loss: 1.711413860321045 [5888 / 60000]\n",
            "Loss: 1.6616981029510498 [6144 / 60000]\n",
            "Loss: 1.7212458848953247 [6400 / 60000]\n",
            "Loss: 1.698227882385254 [6656 / 60000]\n",
            "Loss: 1.7142999172210693 [6912 / 60000]\n",
            "Loss: 1.6950169801712036 [7168 / 60000]\n",
            "Loss: 1.7163125276565552 [7424 / 60000]\n",
            "Loss: 1.6962757110595703 [7680 / 60000]\n",
            "Loss: 1.7059574127197266 [7936 / 60000]\n",
            "Loss: 1.6881366968154907 [8192 / 60000]\n",
            "Loss: 1.701375961303711 [8448 / 60000]\n",
            "Loss: 1.737202763557434 [8704 / 60000]\n",
            "Loss: 1.671842336654663 [8960 / 60000]\n",
            "Loss: 1.6841927766799927 [9216 / 60000]\n",
            "Loss: 1.70247220993042 [9472 / 60000]\n",
            "Loss: 1.7151521444320679 [9728 / 60000]\n",
            "Loss: 1.7353038787841797 [9984 / 60000]\n",
            "Loss: 1.7132974863052368 [10240 / 60000]\n",
            "Loss: 1.7321187257766724 [10496 / 60000]\n",
            "Loss: 1.6793164014816284 [10752 / 60000]\n",
            "Loss: 1.6875168085098267 [11008 / 60000]\n",
            "Loss: 1.6561768054962158 [11264 / 60000]\n",
            "Loss: 1.6980079412460327 [11520 / 60000]\n",
            "Loss: 1.655942440032959 [11776 / 60000]\n",
            "Loss: 1.719221830368042 [12032 / 60000]\n",
            "Loss: 1.6705273389816284 [12288 / 60000]\n",
            "Loss: 1.7001867294311523 [12544 / 60000]\n",
            "Loss: 1.7266314029693604 [12800 / 60000]\n",
            "Loss: 1.6800192594528198 [13056 / 60000]\n",
            "Loss: 1.697826862335205 [13312 / 60000]\n",
            "Loss: 1.685925841331482 [13568 / 60000]\n",
            "Loss: 1.6921281814575195 [13824 / 60000]\n",
            "Loss: 1.6766999959945679 [14080 / 60000]\n",
            "Loss: 1.6820236444473267 [14336 / 60000]\n",
            "Loss: 1.6858506202697754 [14592 / 60000]\n",
            "Loss: 1.6550530195236206 [14848 / 60000]\n",
            "Loss: 1.6658471822738647 [15104 / 60000]\n",
            "Loss: 1.687286376953125 [15360 / 60000]\n",
            "Loss: 1.6656008958816528 [15616 / 60000]\n",
            "Loss: 1.702913522720337 [15872 / 60000]\n",
            "Loss: 1.6937944889068604 [16128 / 60000]\n",
            "Loss: 1.660224199295044 [16384 / 60000]\n",
            "Loss: 1.6564418077468872 [16640 / 60000]\n",
            "Loss: 1.64845609664917 [16896 / 60000]\n",
            "Loss: 1.707344651222229 [17152 / 60000]\n",
            "Loss: 1.6929142475128174 [17408 / 60000]\n",
            "Loss: 1.6873385906219482 [17664 / 60000]\n",
            "Loss: 1.6613136529922485 [17920 / 60000]\n",
            "Loss: 1.6650639772415161 [18176 / 60000]\n",
            "Loss: 1.664872646331787 [18432 / 60000]\n",
            "Loss: 1.6921554803848267 [18688 / 60000]\n",
            "Loss: 1.6829259395599365 [18944 / 60000]\n",
            "Loss: 1.6856545209884644 [19200 / 60000]\n",
            "Loss: 1.6815135478973389 [19456 / 60000]\n",
            "Loss: 1.7100036144256592 [19712 / 60000]\n",
            "Loss: 1.668160319328308 [19968 / 60000]\n",
            "Loss: 1.6972424983978271 [20224 / 60000]\n",
            "Loss: 1.7158750295639038 [20480 / 60000]\n",
            "Loss: 1.6935625076293945 [20736 / 60000]\n",
            "Loss: 1.6857521533966064 [20992 / 60000]\n",
            "Loss: 1.6874923706054688 [21248 / 60000]\n",
            "Loss: 1.7059589624404907 [21504 / 60000]\n",
            "Loss: 1.6817169189453125 [21760 / 60000]\n",
            "Loss: 1.69374680519104 [22016 / 60000]\n",
            "Loss: 1.6711318492889404 [22272 / 60000]\n",
            "Loss: 1.6604278087615967 [22528 / 60000]\n",
            "Loss: 1.6642342805862427 [22784 / 60000]\n",
            "Loss: 1.715218186378479 [23040 / 60000]\n",
            "Loss: 1.6921688318252563 [23296 / 60000]\n",
            "Loss: 1.686477780342102 [23552 / 60000]\n",
            "Loss: 1.6873621940612793 [23808 / 60000]\n",
            "Loss: 1.6549299955368042 [24064 / 60000]\n",
            "Loss: 1.694604516029358 [24320 / 60000]\n",
            "Loss: 1.6295225620269775 [24576 / 60000]\n",
            "Loss: 1.6886298656463623 [24832 / 60000]\n",
            "Loss: 1.6814253330230713 [25088 / 60000]\n",
            "Loss: 1.707801103591919 [25344 / 60000]\n",
            "Loss: 1.713107705116272 [25600 / 60000]\n",
            "Loss: 1.7004644870758057 [25856 / 60000]\n",
            "Loss: 1.6429901123046875 [26112 / 60000]\n",
            "Loss: 1.659849762916565 [26368 / 60000]\n",
            "Loss: 1.7137240171432495 [26624 / 60000]\n",
            "Loss: 1.643670916557312 [26880 / 60000]\n",
            "Loss: 1.6816210746765137 [27136 / 60000]\n",
            "Loss: 1.6762105226516724 [27392 / 60000]\n",
            "Loss: 1.678725004196167 [27648 / 60000]\n",
            "Loss: 1.6730539798736572 [27904 / 60000]\n",
            "Loss: 1.6832547187805176 [28160 / 60000]\n",
            "Loss: 1.669232964515686 [28416 / 60000]\n",
            "Loss: 1.6735867261886597 [28672 / 60000]\n",
            "Loss: 1.6647768020629883 [28928 / 60000]\n",
            "Loss: 1.658509373664856 [29184 / 60000]\n",
            "Loss: 1.684903621673584 [29440 / 60000]\n",
            "Loss: 1.685492753982544 [29696 / 60000]\n",
            "Loss: 1.7199252843856812 [29952 / 60000]\n",
            "Loss: 1.7110927104949951 [30208 / 60000]\n",
            "Loss: 1.646702766418457 [30464 / 60000]\n",
            "Loss: 1.6511858701705933 [30720 / 60000]\n",
            "Loss: 1.673909068107605 [30976 / 60000]\n",
            "Loss: 1.6792298555374146 [31232 / 60000]\n",
            "Loss: 1.6569353342056274 [31488 / 60000]\n",
            "Loss: 1.66825532913208 [31744 / 60000]\n",
            "Loss: 1.716774821281433 [32000 / 60000]\n",
            "Loss: 1.6504926681518555 [32256 / 60000]\n",
            "Loss: 1.648952603340149 [32512 / 60000]\n",
            "Loss: 1.7065174579620361 [32768 / 60000]\n",
            "Loss: 1.6818411350250244 [33024 / 60000]\n",
            "Loss: 1.6622034311294556 [33280 / 60000]\n",
            "Loss: 1.670413851737976 [33536 / 60000]\n",
            "Loss: 1.6209988594055176 [33792 / 60000]\n",
            "Loss: 1.6883184909820557 [34048 / 60000]\n",
            "Loss: 1.6668846607208252 [34304 / 60000]\n",
            "Loss: 1.6734724044799805 [34560 / 60000]\n",
            "Loss: 1.649436354637146 [34816 / 60000]\n",
            "Loss: 1.671183466911316 [35072 / 60000]\n",
            "Loss: 1.6369670629501343 [35328 / 60000]\n",
            "Loss: 1.6658035516738892 [35584 / 60000]\n",
            "Loss: 1.6275826692581177 [35840 / 60000]\n",
            "Loss: 1.707141637802124 [36096 / 60000]\n",
            "Loss: 1.669960856437683 [36352 / 60000]\n",
            "Loss: 1.6562457084655762 [36608 / 60000]\n",
            "Loss: 1.6327290534973145 [36864 / 60000]\n",
            "Loss: 1.6004527807235718 [37120 / 60000]\n",
            "Loss: 1.6357548236846924 [37376 / 60000]\n",
            "Loss: 1.695629358291626 [37632 / 60000]\n",
            "Loss: 1.6375380754470825 [37888 / 60000]\n",
            "Loss: 1.6719938516616821 [38144 / 60000]\n",
            "Loss: 1.6898773908615112 [38400 / 60000]\n",
            "Loss: 1.6702351570129395 [38656 / 60000]\n",
            "Loss: 1.6254825592041016 [38912 / 60000]\n",
            "Loss: 1.642455816268921 [39168 / 60000]\n",
            "Loss: 1.6289511919021606 [39424 / 60000]\n",
            "Loss: 1.650342583656311 [39680 / 60000]\n",
            "Loss: 1.6452206373214722 [39936 / 60000]\n",
            "Loss: 1.6131906509399414 [40192 / 60000]\n",
            "Loss: 1.6345906257629395 [40448 / 60000]\n",
            "Loss: 1.6433669328689575 [40704 / 60000]\n",
            "Loss: 1.6533360481262207 [40960 / 60000]\n",
            "Loss: 1.6502019166946411 [41216 / 60000]\n",
            "Loss: 1.638337254524231 [41472 / 60000]\n",
            "Loss: 1.6603459119796753 [41728 / 60000]\n",
            "Loss: 1.6499050855636597 [41984 / 60000]\n",
            "Loss: 1.6452988386154175 [42240 / 60000]\n",
            "Loss: 1.6485589742660522 [42496 / 60000]\n",
            "Loss: 1.6238561868667603 [42752 / 60000]\n",
            "Loss: 1.6589531898498535 [43008 / 60000]\n",
            "Loss: 1.658241868019104 [43264 / 60000]\n",
            "Loss: 1.6512423753738403 [43520 / 60000]\n",
            "Loss: 1.6371567249298096 [43776 / 60000]\n",
            "Loss: 1.6565697193145752 [44032 / 60000]\n",
            "Loss: 1.6361644268035889 [44288 / 60000]\n",
            "Loss: 1.678563117980957 [44544 / 60000]\n",
            "Loss: 1.6467218399047852 [44800 / 60000]\n",
            "Loss: 1.6674071550369263 [45056 / 60000]\n",
            "Loss: 1.6577396392822266 [45312 / 60000]\n",
            "Loss: 1.5886256694793701 [45568 / 60000]\n",
            "Loss: 1.6487730741500854 [45824 / 60000]\n",
            "Loss: 1.592942714691162 [46080 / 60000]\n",
            "Loss: 1.6653724908828735 [46336 / 60000]\n",
            "Loss: 1.639460563659668 [46592 / 60000]\n",
            "Loss: 1.628074049949646 [46848 / 60000]\n",
            "Loss: 1.671985387802124 [47104 / 60000]\n",
            "Loss: 1.643940806388855 [47360 / 60000]\n",
            "Loss: 1.6644352674484253 [47616 / 60000]\n",
            "Loss: 1.6610511541366577 [47872 / 60000]\n",
            "Loss: 1.6281332969665527 [48128 / 60000]\n",
            "Loss: 1.584889531135559 [48384 / 60000]\n",
            "Loss: 1.6876987218856812 [48640 / 60000]\n",
            "Loss: 1.6265183687210083 [48896 / 60000]\n",
            "Loss: 1.6760118007659912 [49152 / 60000]\n",
            "Loss: 1.6746366024017334 [49408 / 60000]\n",
            "Loss: 1.6441783905029297 [49664 / 60000]\n",
            "Loss: 1.6258224248886108 [49920 / 60000]\n",
            "Loss: 1.6618191003799438 [50176 / 60000]\n",
            "Loss: 1.6304879188537598 [50432 / 60000]\n",
            "Loss: 1.669339656829834 [50688 / 60000]\n",
            "Loss: 1.646011233329773 [50944 / 60000]\n",
            "Loss: 1.6855456829071045 [51200 / 60000]\n",
            "Loss: 1.687161922454834 [51456 / 60000]\n",
            "Loss: 1.6047873497009277 [51712 / 60000]\n",
            "Loss: 1.6489028930664062 [51968 / 60000]\n",
            "Loss: 1.669148564338684 [52224 / 60000]\n",
            "Loss: 1.615212321281433 [52480 / 60000]\n",
            "Loss: 1.5939198732376099 [52736 / 60000]\n",
            "Loss: 1.634258508682251 [52992 / 60000]\n",
            "Loss: 1.6184892654418945 [53248 / 60000]\n",
            "Loss: 1.6763629913330078 [53504 / 60000]\n",
            "Loss: 1.6064741611480713 [53760 / 60000]\n",
            "Loss: 1.632637858390808 [54016 / 60000]\n",
            "Loss: 1.6495510339736938 [54272 / 60000]\n",
            "Loss: 1.6202143430709839 [54528 / 60000]\n",
            "Loss: 1.6447292566299438 [54784 / 60000]\n",
            "Loss: 1.6392967700958252 [55040 / 60000]\n",
            "Loss: 1.630550503730774 [55296 / 60000]\n",
            "Loss: 1.6422622203826904 [55552 / 60000]\n",
            "Loss: 1.6381336450576782 [55808 / 60000]\n",
            "Loss: 1.6714074611663818 [56064 / 60000]\n",
            "Loss: 1.6690483093261719 [56320 / 60000]\n",
            "Loss: 1.6326584815979004 [56576 / 60000]\n",
            "Loss: 1.6295109987258911 [56832 / 60000]\n",
            "Loss: 1.6370108127593994 [57088 / 60000]\n",
            "Loss: 1.603785753250122 [57344 / 60000]\n",
            "Loss: 1.6538732051849365 [57600 / 60000]\n",
            "Loss: 1.6446644067764282 [57856 / 60000]\n",
            "Loss: 1.6336013078689575 [58112 / 60000]\n",
            "Loss: 1.644180178642273 [58368 / 60000]\n",
            "Loss: 1.6107542514801025 [58624 / 60000]\n",
            "Loss: 1.6483068466186523 [58880 / 60000]\n",
            "Loss: 1.6229878664016724 [59136 / 60000]\n",
            "Loss: 1.6825603246688843 [59392 / 60000]\n",
            "Loss: 1.6252869367599487 [59648 / 60000]\n",
            "Loss: 1.6639246940612793 [22464 / 60000]\n",
            "Test Loss: 1.6109276801347732 Accuracy:76.13\n",
            "epoch:11=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.6187242269515991 [0 / 60000]\n",
            "Loss: 1.6301535367965698 [256 / 60000]\n",
            "Loss: 1.6206649541854858 [512 / 60000]\n",
            "Loss: 1.5914660692214966 [768 / 60000]\n",
            "Loss: 1.6552585363388062 [1024 / 60000]\n",
            "Loss: 1.6397528648376465 [1280 / 60000]\n",
            "Loss: 1.6258248090744019 [1536 / 60000]\n",
            "Loss: 1.6342300176620483 [1792 / 60000]\n",
            "Loss: 1.6317733526229858 [2048 / 60000]\n",
            "Loss: 1.6426303386688232 [2304 / 60000]\n",
            "Loss: 1.6138070821762085 [2560 / 60000]\n",
            "Loss: 1.5920064449310303 [2816 / 60000]\n",
            "Loss: 1.6495906114578247 [3072 / 60000]\n",
            "Loss: 1.6374319791793823 [3328 / 60000]\n",
            "Loss: 1.632483959197998 [3584 / 60000]\n",
            "Loss: 1.6158891916275024 [3840 / 60000]\n",
            "Loss: 1.6357910633087158 [4096 / 60000]\n",
            "Loss: 1.6355258226394653 [4352 / 60000]\n",
            "Loss: 1.6353294849395752 [4608 / 60000]\n",
            "Loss: 1.6095685958862305 [4864 / 60000]\n",
            "Loss: 1.607767939567566 [5120 / 60000]\n",
            "Loss: 1.5623031854629517 [5376 / 60000]\n",
            "Loss: 1.619299292564392 [5632 / 60000]\n",
            "Loss: 1.6432703733444214 [5888 / 60000]\n",
            "Loss: 1.6395454406738281 [6144 / 60000]\n",
            "Loss: 1.6216316223144531 [6400 / 60000]\n",
            "Loss: 1.5849360227584839 [6656 / 60000]\n",
            "Loss: 1.6077654361724854 [6912 / 60000]\n",
            "Loss: 1.6709575653076172 [7168 / 60000]\n",
            "Loss: 1.6202754974365234 [7424 / 60000]\n",
            "Loss: 1.5919318199157715 [7680 / 60000]\n",
            "Loss: 1.6406995058059692 [7936 / 60000]\n",
            "Loss: 1.6064409017562866 [8192 / 60000]\n",
            "Loss: 1.6126909255981445 [8448 / 60000]\n",
            "Loss: 1.644150733947754 [8704 / 60000]\n",
            "Loss: 1.6211358308792114 [8960 / 60000]\n",
            "Loss: 1.6014083623886108 [9216 / 60000]\n",
            "Loss: 1.554994821548462 [9472 / 60000]\n",
            "Loss: 1.5917675495147705 [9728 / 60000]\n",
            "Loss: 1.5561193227767944 [9984 / 60000]\n",
            "Loss: 1.621547818183899 [10240 / 60000]\n",
            "Loss: 1.5907198190689087 [10496 / 60000]\n",
            "Loss: 1.633717656135559 [10752 / 60000]\n",
            "Loss: 1.593300700187683 [11008 / 60000]\n",
            "Loss: 1.6059664487838745 [11264 / 60000]\n",
            "Loss: 1.5964435338974 [11520 / 60000]\n",
            "Loss: 1.608170747756958 [11776 / 60000]\n",
            "Loss: 1.590104103088379 [12032 / 60000]\n",
            "Loss: 1.6169687509536743 [12288 / 60000]\n",
            "Loss: 1.5892524719238281 [12544 / 60000]\n",
            "Loss: 1.5758532285690308 [12800 / 60000]\n",
            "Loss: 1.6036344766616821 [13056 / 60000]\n",
            "Loss: 1.657821774482727 [13312 / 60000]\n",
            "Loss: 1.641513705253601 [13568 / 60000]\n",
            "Loss: 1.617815375328064 [13824 / 60000]\n",
            "Loss: 1.6050386428833008 [14080 / 60000]\n",
            "Loss: 1.5995560884475708 [14336 / 60000]\n",
            "Loss: 1.6011766195297241 [14592 / 60000]\n",
            "Loss: 1.61203932762146 [14848 / 60000]\n",
            "Loss: 1.6205469369888306 [15104 / 60000]\n",
            "Loss: 1.6331861019134521 [15360 / 60000]\n",
            "Loss: 1.5821468830108643 [15616 / 60000]\n",
            "Loss: 1.6619772911071777 [15872 / 60000]\n",
            "Loss: 1.5902612209320068 [16128 / 60000]\n",
            "Loss: 1.5581358671188354 [16384 / 60000]\n",
            "Loss: 1.6074339151382446 [16640 / 60000]\n",
            "Loss: 1.610546350479126 [16896 / 60000]\n",
            "Loss: 1.5932395458221436 [17152 / 60000]\n",
            "Loss: 1.6390466690063477 [17408 / 60000]\n",
            "Loss: 1.60020112991333 [17664 / 60000]\n",
            "Loss: 1.6134560108184814 [17920 / 60000]\n",
            "Loss: 1.55276358127594 [18176 / 60000]\n",
            "Loss: 1.5981078147888184 [18432 / 60000]\n",
            "Loss: 1.671101450920105 [18688 / 60000]\n",
            "Loss: 1.6313793659210205 [18944 / 60000]\n",
            "Loss: 1.598262071609497 [19200 / 60000]\n",
            "Loss: 1.6042698621749878 [19456 / 60000]\n",
            "Loss: 1.613149642944336 [19712 / 60000]\n",
            "Loss: 1.584858775138855 [19968 / 60000]\n",
            "Loss: 1.6141635179519653 [20224 / 60000]\n",
            "Loss: 1.5901412963867188 [20480 / 60000]\n",
            "Loss: 1.6064453125 [20736 / 60000]\n",
            "Loss: 1.6048953533172607 [20992 / 60000]\n",
            "Loss: 1.6530380249023438 [21248 / 60000]\n",
            "Loss: 1.5850613117218018 [21504 / 60000]\n",
            "Loss: 1.6171061992645264 [21760 / 60000]\n",
            "Loss: 1.5649971961975098 [22016 / 60000]\n",
            "Loss: 1.5923551321029663 [22272 / 60000]\n",
            "Loss: 1.5824636220932007 [22528 / 60000]\n",
            "Loss: 1.5854471921920776 [22784 / 60000]\n",
            "Loss: 1.577029824256897 [23040 / 60000]\n",
            "Loss: 1.600433349609375 [23296 / 60000]\n",
            "Loss: 1.6287823915481567 [23552 / 60000]\n",
            "Loss: 1.588221788406372 [23808 / 60000]\n",
            "Loss: 1.5973666906356812 [24064 / 60000]\n",
            "Loss: 1.5726284980773926 [24320 / 60000]\n",
            "Loss: 1.6027907133102417 [24576 / 60000]\n",
            "Loss: 1.5837303400039673 [24832 / 60000]\n",
            "Loss: 1.556842565536499 [25088 / 60000]\n",
            "Loss: 1.6260627508163452 [25344 / 60000]\n",
            "Loss: 1.6160084009170532 [25600 / 60000]\n",
            "Loss: 1.6023198366165161 [25856 / 60000]\n",
            "Loss: 1.5592050552368164 [26112 / 60000]\n",
            "Loss: 1.6142501831054688 [26368 / 60000]\n",
            "Loss: 1.6217482089996338 [26624 / 60000]\n",
            "Loss: 1.598632574081421 [26880 / 60000]\n",
            "Loss: 1.5989255905151367 [27136 / 60000]\n",
            "Loss: 1.5636796951293945 [27392 / 60000]\n",
            "Loss: 1.6086002588272095 [27648 / 60000]\n",
            "Loss: 1.5484846830368042 [27904 / 60000]\n",
            "Loss: 1.566004753112793 [28160 / 60000]\n",
            "Loss: 1.5754112005233765 [28416 / 60000]\n",
            "Loss: 1.604968786239624 [28672 / 60000]\n",
            "Loss: 1.541780710220337 [28928 / 60000]\n",
            "Loss: 1.5771962404251099 [29184 / 60000]\n",
            "Loss: 1.6381258964538574 [29440 / 60000]\n",
            "Loss: 1.6046336889266968 [29696 / 60000]\n",
            "Loss: 1.6000139713287354 [29952 / 60000]\n",
            "Loss: 1.5736044645309448 [30208 / 60000]\n",
            "Loss: 1.549455165863037 [30464 / 60000]\n",
            "Loss: 1.5834014415740967 [30720 / 60000]\n",
            "Loss: 1.5536836385726929 [30976 / 60000]\n",
            "Loss: 1.5601872205734253 [31232 / 60000]\n",
            "Loss: 1.573533535003662 [31488 / 60000]\n",
            "Loss: 1.5409249067306519 [31744 / 60000]\n",
            "Loss: 1.5946741104125977 [32000 / 60000]\n",
            "Loss: 1.5811597108840942 [32256 / 60000]\n",
            "Loss: 1.6050136089324951 [32512 / 60000]\n",
            "Loss: 1.552596926689148 [32768 / 60000]\n",
            "Loss: 1.575387716293335 [33024 / 60000]\n",
            "Loss: 1.546978235244751 [33280 / 60000]\n",
            "Loss: 1.620215654373169 [33536 / 60000]\n",
            "Loss: 1.546385407447815 [33792 / 60000]\n",
            "Loss: 1.612067461013794 [34048 / 60000]\n",
            "Loss: 1.5621659755706787 [34304 / 60000]\n",
            "Loss: 1.5915075540542603 [34560 / 60000]\n",
            "Loss: 1.5797330141067505 [34816 / 60000]\n",
            "Loss: 1.593395709991455 [35072 / 60000]\n",
            "Loss: 1.5716246366500854 [35328 / 60000]\n",
            "Loss: 1.6089037656784058 [35584 / 60000]\n",
            "Loss: 1.5864272117614746 [35840 / 60000]\n",
            "Loss: 1.608839988708496 [36096 / 60000]\n",
            "Loss: 1.5820950269699097 [36352 / 60000]\n",
            "Loss: 1.5409905910491943 [36608 / 60000]\n",
            "Loss: 1.6013036966323853 [36864 / 60000]\n",
            "Loss: 1.5523481369018555 [37120 / 60000]\n",
            "Loss: 1.5587416887283325 [37376 / 60000]\n",
            "Loss: 1.5650689601898193 [37632 / 60000]\n",
            "Loss: 1.5846872329711914 [37888 / 60000]\n",
            "Loss: 1.6011532545089722 [38144 / 60000]\n",
            "Loss: 1.6114245653152466 [38400 / 60000]\n",
            "Loss: 1.5435669422149658 [38656 / 60000]\n",
            "Loss: 1.551925778388977 [38912 / 60000]\n",
            "Loss: 1.5735883712768555 [39168 / 60000]\n",
            "Loss: 1.5718923807144165 [39424 / 60000]\n",
            "Loss: 1.6101394891738892 [39680 / 60000]\n",
            "Loss: 1.5707998275756836 [39936 / 60000]\n",
            "Loss: 1.5779424905776978 [40192 / 60000]\n",
            "Loss: 1.558436393737793 [40448 / 60000]\n",
            "Loss: 1.6231459379196167 [40704 / 60000]\n",
            "Loss: 1.537553071975708 [40960 / 60000]\n",
            "Loss: 1.555006742477417 [41216 / 60000]\n",
            "Loss: 1.5683003664016724 [41472 / 60000]\n",
            "Loss: 1.5828489065170288 [41728 / 60000]\n",
            "Loss: 1.6043281555175781 [41984 / 60000]\n",
            "Loss: 1.563827395439148 [42240 / 60000]\n",
            "Loss: 1.6134397983551025 [42496 / 60000]\n",
            "Loss: 1.530237078666687 [42752 / 60000]\n",
            "Loss: 1.604788064956665 [43008 / 60000]\n",
            "Loss: 1.5978543758392334 [43264 / 60000]\n",
            "Loss: 1.57794189453125 [43520 / 60000]\n",
            "Loss: 1.591137170791626 [43776 / 60000]\n",
            "Loss: 1.5482094287872314 [44032 / 60000]\n",
            "Loss: 1.5597305297851562 [44288 / 60000]\n",
            "Loss: 1.5213619470596313 [44544 / 60000]\n",
            "Loss: 1.578712821006775 [44800 / 60000]\n",
            "Loss: 1.5623747110366821 [45056 / 60000]\n",
            "Loss: 1.5678761005401611 [45312 / 60000]\n",
            "Loss: 1.593526840209961 [45568 / 60000]\n",
            "Loss: 1.52743399143219 [45824 / 60000]\n",
            "Loss: 1.5709255933761597 [46080 / 60000]\n",
            "Loss: 1.6177009344100952 [46336 / 60000]\n",
            "Loss: 1.5683887004852295 [46592 / 60000]\n",
            "Loss: 1.5855755805969238 [46848 / 60000]\n",
            "Loss: 1.5309292078018188 [47104 / 60000]\n",
            "Loss: 1.5655523538589478 [47360 / 60000]\n",
            "Loss: 1.6176238059997559 [47616 / 60000]\n",
            "Loss: 1.5803003311157227 [47872 / 60000]\n",
            "Loss: 1.6026031970977783 [48128 / 60000]\n",
            "Loss: 1.5888694524765015 [48384 / 60000]\n",
            "Loss: 1.5151498317718506 [48640 / 60000]\n",
            "Loss: 1.600162148475647 [48896 / 60000]\n",
            "Loss: 1.5554789304733276 [49152 / 60000]\n",
            "Loss: 1.5442852973937988 [49408 / 60000]\n",
            "Loss: 1.552687406539917 [49664 / 60000]\n",
            "Loss: 1.5641132593154907 [49920 / 60000]\n",
            "Loss: 1.544039011001587 [50176 / 60000]\n",
            "Loss: 1.562825083732605 [50432 / 60000]\n",
            "Loss: 1.5660940408706665 [50688 / 60000]\n",
            "Loss: 1.5520174503326416 [50944 / 60000]\n",
            "Loss: 1.5645968914031982 [51200 / 60000]\n",
            "Loss: 1.5351412296295166 [51456 / 60000]\n",
            "Loss: 1.5394529104232788 [51712 / 60000]\n",
            "Loss: 1.5750494003295898 [51968 / 60000]\n",
            "Loss: 1.5003076791763306 [52224 / 60000]\n",
            "Loss: 1.5745820999145508 [52480 / 60000]\n",
            "Loss: 1.5199564695358276 [52736 / 60000]\n",
            "Loss: 1.5488200187683105 [52992 / 60000]\n",
            "Loss: 1.5031218528747559 [53248 / 60000]\n",
            "Loss: 1.5453476905822754 [53504 / 60000]\n",
            "Loss: 1.5807634592056274 [53760 / 60000]\n",
            "Loss: 1.5762979984283447 [54016 / 60000]\n",
            "Loss: 1.507908821105957 [54272 / 60000]\n",
            "Loss: 1.557291865348816 [54528 / 60000]\n",
            "Loss: 1.528372049331665 [54784 / 60000]\n",
            "Loss: 1.5395312309265137 [55040 / 60000]\n",
            "Loss: 1.5190504789352417 [55296 / 60000]\n",
            "Loss: 1.536532998085022 [55552 / 60000]\n",
            "Loss: 1.5638586282730103 [55808 / 60000]\n",
            "Loss: 1.560315728187561 [56064 / 60000]\n",
            "Loss: 1.509627103805542 [56320 / 60000]\n",
            "Loss: 1.5368963479995728 [56576 / 60000]\n",
            "Loss: 1.5620540380477905 [56832 / 60000]\n",
            "Loss: 1.5241531133651733 [57088 / 60000]\n",
            "Loss: 1.536063551902771 [57344 / 60000]\n",
            "Loss: 1.5773930549621582 [57600 / 60000]\n",
            "Loss: 1.5861499309539795 [57856 / 60000]\n",
            "Loss: 1.5592827796936035 [58112 / 60000]\n",
            "Loss: 1.5679157972335815 [58368 / 60000]\n",
            "Loss: 1.5361045598983765 [58624 / 60000]\n",
            "Loss: 1.569597601890564 [58880 / 60000]\n",
            "Loss: 1.5939278602600098 [59136 / 60000]\n",
            "Loss: 1.5265443325042725 [59392 / 60000]\n",
            "Loss: 1.5762193202972412 [59648 / 60000]\n",
            "Loss: 1.5313920974731445 [22464 / 60000]\n",
            "Test Loss: 1.525722575187683 Accuracy:77.25\n",
            "epoch:12=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.521314263343811 [0 / 60000]\n",
            "Loss: 1.5488708019256592 [256 / 60000]\n",
            "Loss: 1.510607361793518 [512 / 60000]\n",
            "Loss: 1.5531996488571167 [768 / 60000]\n",
            "Loss: 1.4982025623321533 [1024 / 60000]\n",
            "Loss: 1.5557385683059692 [1280 / 60000]\n",
            "Loss: 1.5706170797348022 [1536 / 60000]\n",
            "Loss: 1.5208449363708496 [1792 / 60000]\n",
            "Loss: 1.535444974899292 [2048 / 60000]\n",
            "Loss: 1.5294924974441528 [2304 / 60000]\n",
            "Loss: 1.571153163909912 [2560 / 60000]\n",
            "Loss: 1.5008244514465332 [2816 / 60000]\n",
            "Loss: 1.557338833808899 [3072 / 60000]\n",
            "Loss: 1.5754673480987549 [3328 / 60000]\n",
            "Loss: 1.534720540046692 [3584 / 60000]\n",
            "Loss: 1.5432895421981812 [3840 / 60000]\n",
            "Loss: 1.5510982275009155 [4096 / 60000]\n",
            "Loss: 1.5524793863296509 [4352 / 60000]\n",
            "Loss: 1.5429638624191284 [4608 / 60000]\n",
            "Loss: 1.5467666387557983 [4864 / 60000]\n",
            "Loss: 1.50447416305542 [5120 / 60000]\n",
            "Loss: 1.5443943738937378 [5376 / 60000]\n",
            "Loss: 1.5327900648117065 [5632 / 60000]\n",
            "Loss: 1.4926366806030273 [5888 / 60000]\n",
            "Loss: 1.4844920635223389 [6144 / 60000]\n",
            "Loss: 1.5385243892669678 [6400 / 60000]\n",
            "Loss: 1.5288861989974976 [6656 / 60000]\n",
            "Loss: 1.56619131565094 [6912 / 60000]\n",
            "Loss: 1.5257211923599243 [7168 / 60000]\n",
            "Loss: 1.4922211170196533 [7424 / 60000]\n",
            "Loss: 1.5243268013000488 [7680 / 60000]\n",
            "Loss: 1.5338683128356934 [7936 / 60000]\n",
            "Loss: 1.5375643968582153 [8192 / 60000]\n",
            "Loss: 1.525765299797058 [8448 / 60000]\n",
            "Loss: 1.494109034538269 [8704 / 60000]\n",
            "Loss: 1.4967559576034546 [8960 / 60000]\n",
            "Loss: 1.4935375452041626 [9216 / 60000]\n",
            "Loss: 1.5507272481918335 [9472 / 60000]\n",
            "Loss: 1.4836188554763794 [9728 / 60000]\n",
            "Loss: 1.5646843910217285 [9984 / 60000]\n",
            "Loss: 1.5473848581314087 [10240 / 60000]\n",
            "Loss: 1.5405032634735107 [10496 / 60000]\n",
            "Loss: 1.5440089702606201 [10752 / 60000]\n",
            "Loss: 1.5349370241165161 [11008 / 60000]\n",
            "Loss: 1.5680128335952759 [11264 / 60000]\n",
            "Loss: 1.5480678081512451 [11520 / 60000]\n",
            "Loss: 1.5253369808197021 [11776 / 60000]\n",
            "Loss: 1.5534813404083252 [12032 / 60000]\n",
            "Loss: 1.4537431001663208 [12288 / 60000]\n",
            "Loss: 1.58359956741333 [12544 / 60000]\n",
            "Loss: 1.5388141870498657 [12800 / 60000]\n",
            "Loss: 1.5187201499938965 [13056 / 60000]\n",
            "Loss: 1.5330561399459839 [13312 / 60000]\n",
            "Loss: 1.5626850128173828 [13568 / 60000]\n",
            "Loss: 1.5271244049072266 [13824 / 60000]\n",
            "Loss: 1.5310916900634766 [14080 / 60000]\n",
            "Loss: 1.5195167064666748 [14336 / 60000]\n",
            "Loss: 1.5369131565093994 [14592 / 60000]\n",
            "Loss: 1.5672367811203003 [14848 / 60000]\n",
            "Loss: 1.5014904737472534 [15104 / 60000]\n",
            "Loss: 1.5054447650909424 [15360 / 60000]\n",
            "Loss: 1.5098553895950317 [15616 / 60000]\n",
            "Loss: 1.4987061023712158 [15872 / 60000]\n",
            "Loss: 1.5150434970855713 [16128 / 60000]\n",
            "Loss: 1.5115121603012085 [16384 / 60000]\n",
            "Loss: 1.5207982063293457 [16640 / 60000]\n",
            "Loss: 1.5139998197555542 [16896 / 60000]\n",
            "Loss: 1.518350601196289 [17152 / 60000]\n",
            "Loss: 1.5260870456695557 [17408 / 60000]\n",
            "Loss: 1.5415040254592896 [17664 / 60000]\n",
            "Loss: 1.5284502506256104 [17920 / 60000]\n",
            "Loss: 1.5159364938735962 [18176 / 60000]\n",
            "Loss: 1.5482763051986694 [18432 / 60000]\n",
            "Loss: 1.5264687538146973 [18688 / 60000]\n",
            "Loss: 1.495146632194519 [18944 / 60000]\n",
            "Loss: 1.4805034399032593 [19200 / 60000]\n",
            "Loss: 1.5154117345809937 [19456 / 60000]\n",
            "Loss: 1.428798794746399 [19712 / 60000]\n",
            "Loss: 1.5054118633270264 [19968 / 60000]\n",
            "Loss: 1.5027461051940918 [20224 / 60000]\n",
            "Loss: 1.466951847076416 [20480 / 60000]\n",
            "Loss: 1.5131876468658447 [20736 / 60000]\n",
            "Loss: 1.5511764287948608 [20992 / 60000]\n",
            "Loss: 1.521444320678711 [21248 / 60000]\n",
            "Loss: 1.510704755783081 [21504 / 60000]\n",
            "Loss: 1.502844214439392 [21760 / 60000]\n",
            "Loss: 1.5086991786956787 [22016 / 60000]\n",
            "Loss: 1.5390722751617432 [22272 / 60000]\n",
            "Loss: 1.5156898498535156 [22528 / 60000]\n",
            "Loss: 1.5187067985534668 [22784 / 60000]\n",
            "Loss: 1.5171605348587036 [23040 / 60000]\n",
            "Loss: 1.4930596351623535 [23296 / 60000]\n",
            "Loss: 1.4764705896377563 [23552 / 60000]\n",
            "Loss: 1.4838050603866577 [23808 / 60000]\n",
            "Loss: 1.4887681007385254 [24064 / 60000]\n",
            "Loss: 1.5014238357543945 [24320 / 60000]\n",
            "Loss: 1.5037510395050049 [24576 / 60000]\n",
            "Loss: 1.5275131464004517 [24832 / 60000]\n",
            "Loss: 1.5048913955688477 [25088 / 60000]\n",
            "Loss: 1.4922689199447632 [25344 / 60000]\n",
            "Loss: 1.5010844469070435 [25600 / 60000]\n",
            "Loss: 1.5422073602676392 [25856 / 60000]\n",
            "Loss: 1.5207270383834839 [26112 / 60000]\n",
            "Loss: 1.5132379531860352 [26368 / 60000]\n",
            "Loss: 1.5229811668395996 [26624 / 60000]\n",
            "Loss: 1.5595537424087524 [26880 / 60000]\n",
            "Loss: 1.4947683811187744 [27136 / 60000]\n",
            "Loss: 1.4879136085510254 [27392 / 60000]\n",
            "Loss: 1.5017398595809937 [27648 / 60000]\n",
            "Loss: 1.4787298440933228 [27904 / 60000]\n",
            "Loss: 1.568677544593811 [28160 / 60000]\n",
            "Loss: 1.5083122253417969 [28416 / 60000]\n",
            "Loss: 1.4625552892684937 [28672 / 60000]\n",
            "Loss: 1.5490721464157104 [28928 / 60000]\n",
            "Loss: 1.5412492752075195 [29184 / 60000]\n",
            "Loss: 1.5062693357467651 [29440 / 60000]\n",
            "Loss: 1.5073726177215576 [29696 / 60000]\n",
            "Loss: 1.5208526849746704 [29952 / 60000]\n",
            "Loss: 1.5096522569656372 [30208 / 60000]\n",
            "Loss: 1.5303858518600464 [30464 / 60000]\n",
            "Loss: 1.4898786544799805 [30720 / 60000]\n",
            "Loss: 1.4405664205551147 [30976 / 60000]\n",
            "Loss: 1.5382425785064697 [31232 / 60000]\n",
            "Loss: 1.4868382215499878 [31488 / 60000]\n",
            "Loss: 1.494856595993042 [31744 / 60000]\n",
            "Loss: 1.5263702869415283 [32000 / 60000]\n",
            "Loss: 1.4871835708618164 [32256 / 60000]\n",
            "Loss: 1.538309931755066 [32512 / 60000]\n",
            "Loss: 1.5095314979553223 [32768 / 60000]\n",
            "Loss: 1.4947041273117065 [33024 / 60000]\n",
            "Loss: 1.49037766456604 [33280 / 60000]\n",
            "Loss: 1.580603003501892 [33536 / 60000]\n",
            "Loss: 1.5320994853973389 [33792 / 60000]\n",
            "Loss: 1.5101425647735596 [34048 / 60000]\n",
            "Loss: 1.5010753870010376 [34304 / 60000]\n",
            "Loss: 1.452868103981018 [34560 / 60000]\n",
            "Loss: 1.537379264831543 [34816 / 60000]\n",
            "Loss: 1.5442094802856445 [35072 / 60000]\n",
            "Loss: 1.4969217777252197 [35328 / 60000]\n",
            "Loss: 1.5076427459716797 [35584 / 60000]\n",
            "Loss: 1.469820499420166 [35840 / 60000]\n",
            "Loss: 1.5218998193740845 [36096 / 60000]\n",
            "Loss: 1.4831628799438477 [36352 / 60000]\n",
            "Loss: 1.4834898710250854 [36608 / 60000]\n",
            "Loss: 1.4613115787506104 [36864 / 60000]\n",
            "Loss: 1.5005786418914795 [37120 / 60000]\n",
            "Loss: 1.5023385286331177 [37376 / 60000]\n",
            "Loss: 1.5296390056610107 [37632 / 60000]\n",
            "Loss: 1.451619029045105 [37888 / 60000]\n",
            "Loss: 1.5508038997650146 [38144 / 60000]\n",
            "Loss: 1.5008103847503662 [38400 / 60000]\n",
            "Loss: 1.4744588136672974 [38656 / 60000]\n",
            "Loss: 1.5063539743423462 [38912 / 60000]\n",
            "Loss: 1.4820125102996826 [39168 / 60000]\n",
            "Loss: 1.506335735321045 [39424 / 60000]\n",
            "Loss: 1.4724464416503906 [39680 / 60000]\n",
            "Loss: 1.4487930536270142 [39936 / 60000]\n",
            "Loss: 1.510514497756958 [40192 / 60000]\n",
            "Loss: 1.5222710371017456 [40448 / 60000]\n",
            "Loss: 1.4831517934799194 [40704 / 60000]\n",
            "Loss: 1.503167748451233 [40960 / 60000]\n",
            "Loss: 1.5037084817886353 [41216 / 60000]\n",
            "Loss: 1.5157421827316284 [41472 / 60000]\n",
            "Loss: 1.474226474761963 [41728 / 60000]\n",
            "Loss: 1.5124882459640503 [41984 / 60000]\n",
            "Loss: 1.5101711750030518 [42240 / 60000]\n",
            "Loss: 1.519675612449646 [42496 / 60000]\n",
            "Loss: 1.462549090385437 [42752 / 60000]\n",
            "Loss: 1.5175561904907227 [43008 / 60000]\n",
            "Loss: 1.4330445528030396 [43264 / 60000]\n",
            "Loss: 1.5082452297210693 [43520 / 60000]\n",
            "Loss: 1.4599095582962036 [43776 / 60000]\n",
            "Loss: 1.476119875907898 [44032 / 60000]\n",
            "Loss: 1.4837844371795654 [44288 / 60000]\n",
            "Loss: 1.521888256072998 [44544 / 60000]\n",
            "Loss: 1.4437655210494995 [44800 / 60000]\n",
            "Loss: 1.4443572759628296 [45056 / 60000]\n",
            "Loss: 1.4875701665878296 [45312 / 60000]\n",
            "Loss: 1.4363898038864136 [45568 / 60000]\n",
            "Loss: 1.473323941230774 [45824 / 60000]\n",
            "Loss: 1.4872972965240479 [46080 / 60000]\n",
            "Loss: 1.462227463722229 [46336 / 60000]\n",
            "Loss: 1.5223701000213623 [46592 / 60000]\n",
            "Loss: 1.4553322792053223 [46848 / 60000]\n",
            "Loss: 1.4736748933792114 [47104 / 60000]\n",
            "Loss: 1.4220223426818848 [47360 / 60000]\n",
            "Loss: 1.4836159944534302 [47616 / 60000]\n",
            "Loss: 1.4778302907943726 [47872 / 60000]\n",
            "Loss: 1.4604849815368652 [48128 / 60000]\n",
            "Loss: 1.5087496042251587 [48384 / 60000]\n",
            "Loss: 1.4365419149398804 [48640 / 60000]\n",
            "Loss: 1.5259466171264648 [48896 / 60000]\n",
            "Loss: 1.4712234735488892 [49152 / 60000]\n",
            "Loss: 1.4959924221038818 [49408 / 60000]\n",
            "Loss: 1.5134892463684082 [49664 / 60000]\n",
            "Loss: 1.4604837894439697 [49920 / 60000]\n",
            "Loss: 1.4739744663238525 [50176 / 60000]\n",
            "Loss: 1.453261375427246 [50432 / 60000]\n",
            "Loss: 1.4748055934906006 [50688 / 60000]\n",
            "Loss: 1.4898086786270142 [50944 / 60000]\n",
            "Loss: 1.465592861175537 [51200 / 60000]\n",
            "Loss: 1.4336591958999634 [51456 / 60000]\n",
            "Loss: 1.4960130453109741 [51712 / 60000]\n",
            "Loss: 1.508514642715454 [51968 / 60000]\n",
            "Loss: 1.4621546268463135 [52224 / 60000]\n",
            "Loss: 1.4629024267196655 [52480 / 60000]\n",
            "Loss: 1.483256459236145 [52736 / 60000]\n",
            "Loss: 1.4588978290557861 [52992 / 60000]\n",
            "Loss: 1.4849472045898438 [53248 / 60000]\n",
            "Loss: 1.448567509651184 [53504 / 60000]\n",
            "Loss: 1.504470705986023 [53760 / 60000]\n",
            "Loss: 1.493893027305603 [54016 / 60000]\n",
            "Loss: 1.49193274974823 [54272 / 60000]\n",
            "Loss: 1.5163352489471436 [54528 / 60000]\n",
            "Loss: 1.471041202545166 [54784 / 60000]\n",
            "Loss: 1.487181544303894 [55040 / 60000]\n",
            "Loss: 1.5249640941619873 [55296 / 60000]\n",
            "Loss: 1.4533653259277344 [55552 / 60000]\n",
            "Loss: 1.4181663990020752 [55808 / 60000]\n",
            "Loss: 1.4739493131637573 [56064 / 60000]\n",
            "Loss: 1.4573317766189575 [56320 / 60000]\n",
            "Loss: 1.4942699670791626 [56576 / 60000]\n",
            "Loss: 1.4311614036560059 [56832 / 60000]\n",
            "Loss: 1.4780471324920654 [57088 / 60000]\n",
            "Loss: 1.5150470733642578 [57344 / 60000]\n",
            "Loss: 1.4780486822128296 [57600 / 60000]\n",
            "Loss: 1.446232557296753 [57856 / 60000]\n",
            "Loss: 1.4842408895492554 [58112 / 60000]\n",
            "Loss: 1.4341964721679688 [58368 / 60000]\n",
            "Loss: 1.4707292318344116 [58624 / 60000]\n",
            "Loss: 1.3783700466156006 [58880 / 60000]\n",
            "Loss: 1.4766987562179565 [59136 / 60000]\n",
            "Loss: 1.5114068984985352 [59392 / 60000]\n",
            "Loss: 1.452170729637146 [59648 / 60000]\n",
            "Loss: 1.4426307678222656 [22464 / 60000]\n",
            "Test Loss: 1.4425654113292694 Accuracy:78.02\n",
            "epoch:13=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.4023643732070923 [0 / 60000]\n",
            "Loss: 1.4616820812225342 [256 / 60000]\n",
            "Loss: 1.4762578010559082 [512 / 60000]\n",
            "Loss: 1.4531382322311401 [768 / 60000]\n",
            "Loss: 1.4586840867996216 [1024 / 60000]\n",
            "Loss: 1.4851950407028198 [1280 / 60000]\n",
            "Loss: 1.4293662309646606 [1536 / 60000]\n",
            "Loss: 1.3976318836212158 [1792 / 60000]\n",
            "Loss: 1.4411507844924927 [2048 / 60000]\n",
            "Loss: 1.4308087825775146 [2304 / 60000]\n",
            "Loss: 1.4369115829467773 [2560 / 60000]\n",
            "Loss: 1.4709854125976562 [2816 / 60000]\n",
            "Loss: 1.4171069860458374 [3072 / 60000]\n",
            "Loss: 1.449971318244934 [3328 / 60000]\n",
            "Loss: 1.4581953287124634 [3584 / 60000]\n",
            "Loss: 1.441811442375183 [3840 / 60000]\n",
            "Loss: 1.4667950868606567 [4096 / 60000]\n",
            "Loss: 1.5045804977416992 [4352 / 60000]\n",
            "Loss: 1.452584147453308 [4608 / 60000]\n",
            "Loss: 1.4551321268081665 [4864 / 60000]\n",
            "Loss: 1.4197371006011963 [5120 / 60000]\n",
            "Loss: 1.4704654216766357 [5376 / 60000]\n",
            "Loss: 1.4671528339385986 [5632 / 60000]\n",
            "Loss: 1.4378478527069092 [5888 / 60000]\n",
            "Loss: 1.4862959384918213 [6144 / 60000]\n",
            "Loss: 1.4048770666122437 [6400 / 60000]\n",
            "Loss: 1.4366981983184814 [6656 / 60000]\n",
            "Loss: 1.4279212951660156 [6912 / 60000]\n",
            "Loss: 1.484986424446106 [7168 / 60000]\n",
            "Loss: 1.4377435445785522 [7424 / 60000]\n",
            "Loss: 1.4586008787155151 [7680 / 60000]\n",
            "Loss: 1.4710766077041626 [7936 / 60000]\n",
            "Loss: 1.4709314107894897 [8192 / 60000]\n",
            "Loss: 1.426886796951294 [8448 / 60000]\n",
            "Loss: 1.4407893419265747 [8704 / 60000]\n",
            "Loss: 1.4411296844482422 [8960 / 60000]\n",
            "Loss: 1.458160161972046 [9216 / 60000]\n",
            "Loss: 1.457061529159546 [9472 / 60000]\n",
            "Loss: 1.4039820432662964 [9728 / 60000]\n",
            "Loss: 1.4662134647369385 [9984 / 60000]\n",
            "Loss: 1.4852242469787598 [10240 / 60000]\n",
            "Loss: 1.4708170890808105 [10496 / 60000]\n",
            "Loss: 1.4617550373077393 [10752 / 60000]\n",
            "Loss: 1.507161259651184 [11008 / 60000]\n",
            "Loss: 1.4346232414245605 [11264 / 60000]\n",
            "Loss: 1.459228515625 [11520 / 60000]\n",
            "Loss: 1.4294878244400024 [11776 / 60000]\n",
            "Loss: 1.4378420114517212 [12032 / 60000]\n",
            "Loss: 1.5113292932510376 [12288 / 60000]\n",
            "Loss: 1.4639605283737183 [12544 / 60000]\n",
            "Loss: 1.4285324811935425 [12800 / 60000]\n",
            "Loss: 1.4476735591888428 [13056 / 60000]\n",
            "Loss: 1.423624873161316 [13312 / 60000]\n",
            "Loss: 1.4505201578140259 [13568 / 60000]\n",
            "Loss: 1.4614834785461426 [13824 / 60000]\n",
            "Loss: 1.4405595064163208 [14080 / 60000]\n",
            "Loss: 1.4178649187088013 [14336 / 60000]\n",
            "Loss: 1.4373337030410767 [14592 / 60000]\n",
            "Loss: 1.4745244979858398 [14848 / 60000]\n",
            "Loss: 1.4591350555419922 [15104 / 60000]\n",
            "Loss: 1.4701600074768066 [15360 / 60000]\n",
            "Loss: 1.4635539054870605 [15616 / 60000]\n",
            "Loss: 1.4623769521713257 [15872 / 60000]\n",
            "Loss: 1.4898712635040283 [16128 / 60000]\n",
            "Loss: 1.4731898307800293 [16384 / 60000]\n",
            "Loss: 1.4602560997009277 [16640 / 60000]\n",
            "Loss: 1.4159862995147705 [16896 / 60000]\n",
            "Loss: 1.4457414150238037 [17152 / 60000]\n",
            "Loss: 1.4360297918319702 [17408 / 60000]\n",
            "Loss: 1.4479289054870605 [17664 / 60000]\n",
            "Loss: 1.424431324005127 [17920 / 60000]\n",
            "Loss: 1.4127106666564941 [18176 / 60000]\n",
            "Loss: 1.3697527647018433 [18432 / 60000]\n",
            "Loss: 1.443109393119812 [18688 / 60000]\n",
            "Loss: 1.4156490564346313 [18944 / 60000]\n",
            "Loss: 1.427085280418396 [19200 / 60000]\n",
            "Loss: 1.446610450744629 [19456 / 60000]\n",
            "Loss: 1.425392985343933 [19712 / 60000]\n",
            "Loss: 1.409626841545105 [19968 / 60000]\n",
            "Loss: 1.4116065502166748 [20224 / 60000]\n",
            "Loss: 1.4055275917053223 [20480 / 60000]\n",
            "Loss: 1.4911102056503296 [20736 / 60000]\n",
            "Loss: 1.3848896026611328 [20992 / 60000]\n",
            "Loss: 1.4112536907196045 [21248 / 60000]\n",
            "Loss: 1.4456024169921875 [21504 / 60000]\n",
            "Loss: 1.4540797472000122 [21760 / 60000]\n",
            "Loss: 1.4624608755111694 [22016 / 60000]\n",
            "Loss: 1.4443413019180298 [22272 / 60000]\n",
            "Loss: 1.4048889875411987 [22528 / 60000]\n",
            "Loss: 1.4413350820541382 [22784 / 60000]\n",
            "Loss: 1.41762113571167 [23040 / 60000]\n",
            "Loss: 1.4473987817764282 [23296 / 60000]\n",
            "Loss: 1.4098464250564575 [23552 / 60000]\n",
            "Loss: 1.4290908575057983 [23808 / 60000]\n",
            "Loss: 1.4067052602767944 [24064 / 60000]\n",
            "Loss: 1.406604528427124 [24320 / 60000]\n",
            "Loss: 1.372597098350525 [24576 / 60000]\n",
            "Loss: 1.422013759613037 [24832 / 60000]\n",
            "Loss: 1.4386683702468872 [25088 / 60000]\n",
            "Loss: 1.4560645818710327 [25344 / 60000]\n",
            "Loss: 1.435617208480835 [25600 / 60000]\n",
            "Loss: 1.4647455215454102 [25856 / 60000]\n",
            "Loss: 1.4727102518081665 [26112 / 60000]\n",
            "Loss: 1.413118600845337 [26368 / 60000]\n",
            "Loss: 1.4002482891082764 [26624 / 60000]\n",
            "Loss: 1.4194315671920776 [26880 / 60000]\n",
            "Loss: 1.4229302406311035 [27136 / 60000]\n",
            "Loss: 1.4440362453460693 [27392 / 60000]\n",
            "Loss: 1.440535545349121 [27648 / 60000]\n",
            "Loss: 1.4471170902252197 [27904 / 60000]\n",
            "Loss: 1.3910964727401733 [28160 / 60000]\n",
            "Loss: 1.4290434122085571 [28416 / 60000]\n",
            "Loss: 1.4271806478500366 [28672 / 60000]\n",
            "Loss: 1.3742709159851074 [28928 / 60000]\n",
            "Loss: 1.3907549381256104 [29184 / 60000]\n",
            "Loss: 1.4269503355026245 [29440 / 60000]\n",
            "Loss: 1.4018840789794922 [29696 / 60000]\n",
            "Loss: 1.4594770669937134 [29952 / 60000]\n",
            "Loss: 1.4165503978729248 [30208 / 60000]\n",
            "Loss: 1.4472829103469849 [30464 / 60000]\n",
            "Loss: 1.4247095584869385 [30720 / 60000]\n",
            "Loss: 1.4196785688400269 [30976 / 60000]\n",
            "Loss: 1.3926783800125122 [31232 / 60000]\n",
            "Loss: 1.427573800086975 [31488 / 60000]\n",
            "Loss: 1.3998146057128906 [31744 / 60000]\n",
            "Loss: 1.3793200254440308 [32000 / 60000]\n",
            "Loss: 1.418688178062439 [32256 / 60000]\n",
            "Loss: 1.4301226139068604 [32512 / 60000]\n",
            "Loss: 1.3998688459396362 [32768 / 60000]\n",
            "Loss: 1.3821349143981934 [33024 / 60000]\n",
            "Loss: 1.3731460571289062 [33280 / 60000]\n",
            "Loss: 1.400809645652771 [33536 / 60000]\n",
            "Loss: 1.4408271312713623 [33792 / 60000]\n",
            "Loss: 1.3945845365524292 [34048 / 60000]\n",
            "Loss: 1.4030585289001465 [34304 / 60000]\n",
            "Loss: 1.4183293581008911 [34560 / 60000]\n",
            "Loss: 1.4628846645355225 [34816 / 60000]\n",
            "Loss: 1.4676505327224731 [35072 / 60000]\n",
            "Loss: 1.387899398803711 [35328 / 60000]\n",
            "Loss: 1.4067119359970093 [35584 / 60000]\n",
            "Loss: 1.4423702955245972 [35840 / 60000]\n",
            "Loss: 1.4339663982391357 [36096 / 60000]\n",
            "Loss: 1.415995478630066 [36352 / 60000]\n",
            "Loss: 1.3798359632492065 [36608 / 60000]\n",
            "Loss: 1.428900122642517 [36864 / 60000]\n",
            "Loss: 1.4245012998580933 [37120 / 60000]\n",
            "Loss: 1.4353528022766113 [37376 / 60000]\n",
            "Loss: 1.3727844953536987 [37632 / 60000]\n",
            "Loss: 1.4614704847335815 [37888 / 60000]\n",
            "Loss: 1.4322396516799927 [38144 / 60000]\n",
            "Loss: 1.424673318862915 [38400 / 60000]\n",
            "Loss: 1.420717716217041 [38656 / 60000]\n",
            "Loss: 1.3677138090133667 [38912 / 60000]\n",
            "Loss: 1.4228495359420776 [39168 / 60000]\n",
            "Loss: 1.436357855796814 [39424 / 60000]\n",
            "Loss: 1.3878746032714844 [39680 / 60000]\n",
            "Loss: 1.4010915756225586 [39936 / 60000]\n",
            "Loss: 1.3933353424072266 [40192 / 60000]\n",
            "Loss: 1.4335734844207764 [40448 / 60000]\n",
            "Loss: 1.3699856996536255 [40704 / 60000]\n",
            "Loss: 1.4344125986099243 [40960 / 60000]\n",
            "Loss: 1.367529034614563 [41216 / 60000]\n",
            "Loss: 1.433078646659851 [41472 / 60000]\n",
            "Loss: 1.446285605430603 [41728 / 60000]\n",
            "Loss: 1.4039050340652466 [41984 / 60000]\n",
            "Loss: 1.435670018196106 [42240 / 60000]\n",
            "Loss: 1.4630321264266968 [42496 / 60000]\n",
            "Loss: 1.3716115951538086 [42752 / 60000]\n",
            "Loss: 1.3905742168426514 [43008 / 60000]\n",
            "Loss: 1.399791955947876 [43264 / 60000]\n",
            "Loss: 1.4026747941970825 [43520 / 60000]\n",
            "Loss: 1.3479726314544678 [43776 / 60000]\n",
            "Loss: 1.37895667552948 [44032 / 60000]\n",
            "Loss: 1.3980568647384644 [44288 / 60000]\n",
            "Loss: 1.4130703210830688 [44544 / 60000]\n",
            "Loss: 1.389926791191101 [44800 / 60000]\n",
            "Loss: 1.4211452007293701 [45056 / 60000]\n",
            "Loss: 1.4812510013580322 [45312 / 60000]\n",
            "Loss: 1.368882179260254 [45568 / 60000]\n",
            "Loss: 1.398125171661377 [45824 / 60000]\n",
            "Loss: 1.4029301404953003 [46080 / 60000]\n",
            "Loss: 1.383954405784607 [46336 / 60000]\n",
            "Loss: 1.395785927772522 [46592 / 60000]\n",
            "Loss: 1.4296629428863525 [46848 / 60000]\n",
            "Loss: 1.385790467262268 [47104 / 60000]\n",
            "Loss: 1.4294030666351318 [47360 / 60000]\n",
            "Loss: 1.3787492513656616 [47616 / 60000]\n",
            "Loss: 1.4227575063705444 [47872 / 60000]\n",
            "Loss: 1.402363896369934 [48128 / 60000]\n",
            "Loss: 1.364410638809204 [48384 / 60000]\n",
            "Loss: 1.3723320960998535 [48640 / 60000]\n",
            "Loss: 1.3592052459716797 [48896 / 60000]\n",
            "Loss: 1.4419951438903809 [49152 / 60000]\n",
            "Loss: 1.4114854335784912 [49408 / 60000]\n",
            "Loss: 1.4073243141174316 [49664 / 60000]\n",
            "Loss: 1.3994331359863281 [49920 / 60000]\n",
            "Loss: 1.4066646099090576 [50176 / 60000]\n",
            "Loss: 1.4213745594024658 [50432 / 60000]\n",
            "Loss: 1.4879122972488403 [50688 / 60000]\n",
            "Loss: 1.4006470441818237 [50944 / 60000]\n",
            "Loss: 1.409513235092163 [51200 / 60000]\n",
            "Loss: 1.4149984121322632 [51456 / 60000]\n",
            "Loss: 1.361791729927063 [51712 / 60000]\n",
            "Loss: 1.3866239786148071 [51968 / 60000]\n",
            "Loss: 1.3961327075958252 [52224 / 60000]\n",
            "Loss: 1.353420376777649 [52480 / 60000]\n",
            "Loss: 1.3675779104232788 [52736 / 60000]\n",
            "Loss: 1.390608787536621 [52992 / 60000]\n",
            "Loss: 1.3855724334716797 [53248 / 60000]\n",
            "Loss: 1.3952299356460571 [53504 / 60000]\n",
            "Loss: 1.3738194704055786 [53760 / 60000]\n",
            "Loss: 1.4222264289855957 [54016 / 60000]\n",
            "Loss: 1.4412705898284912 [54272 / 60000]\n",
            "Loss: 1.4113105535507202 [54528 / 60000]\n",
            "Loss: 1.4583773612976074 [54784 / 60000]\n",
            "Loss: 1.37429940700531 [55040 / 60000]\n",
            "Loss: 1.3993200063705444 [55296 / 60000]\n",
            "Loss: 1.381331205368042 [55552 / 60000]\n",
            "Loss: 1.37361741065979 [55808 / 60000]\n",
            "Loss: 1.4256724119186401 [56064 / 60000]\n",
            "Loss: 1.3989832401275635 [56320 / 60000]\n",
            "Loss: 1.4036610126495361 [56576 / 60000]\n",
            "Loss: 1.4336764812469482 [56832 / 60000]\n",
            "Loss: 1.3938542604446411 [57088 / 60000]\n",
            "Loss: 1.398162603378296 [57344 / 60000]\n",
            "Loss: 1.4212688207626343 [57600 / 60000]\n",
            "Loss: 1.3859400749206543 [57856 / 60000]\n",
            "Loss: 1.4220123291015625 [58112 / 60000]\n",
            "Loss: 1.365253210067749 [58368 / 60000]\n",
            "Loss: 1.381428837776184 [58624 / 60000]\n",
            "Loss: 1.396767258644104 [58880 / 60000]\n",
            "Loss: 1.3519755601882935 [59136 / 60000]\n",
            "Loss: 1.4341775178909302 [59392 / 60000]\n",
            "Loss: 1.4114978313446045 [59648 / 60000]\n",
            "Loss: 1.4344953298568726 [22464 / 60000]\n",
            "Test Loss: 1.3628691405057907 Accuracy:78.66\n",
            "epoch:14=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.4042503833770752 [0 / 60000]\n",
            "Loss: 1.3868061304092407 [256 / 60000]\n",
            "Loss: 1.3924379348754883 [512 / 60000]\n",
            "Loss: 1.397302269935608 [768 / 60000]\n",
            "Loss: 1.3460958003997803 [1024 / 60000]\n",
            "Loss: 1.353576898574829 [1280 / 60000]\n",
            "Loss: 1.3723130226135254 [1536 / 60000]\n",
            "Loss: 1.3636186122894287 [1792 / 60000]\n",
            "Loss: 1.388269305229187 [2048 / 60000]\n",
            "Loss: 1.3565913438796997 [2304 / 60000]\n",
            "Loss: 1.4258729219436646 [2560 / 60000]\n",
            "Loss: 1.4166141748428345 [2816 / 60000]\n",
            "Loss: 1.3427574634552002 [3072 / 60000]\n",
            "Loss: 1.3754757642745972 [3328 / 60000]\n",
            "Loss: 1.4045003652572632 [3584 / 60000]\n",
            "Loss: 1.381333827972412 [3840 / 60000]\n",
            "Loss: 1.4230217933654785 [4096 / 60000]\n",
            "Loss: 1.369124174118042 [4352 / 60000]\n",
            "Loss: 1.4105491638183594 [4608 / 60000]\n",
            "Loss: 1.3890756368637085 [4864 / 60000]\n",
            "Loss: 1.3815786838531494 [5120 / 60000]\n",
            "Loss: 1.3825997114181519 [5376 / 60000]\n",
            "Loss: 1.3619499206542969 [5632 / 60000]\n",
            "Loss: 1.404176950454712 [5888 / 60000]\n",
            "Loss: 1.345261812210083 [6144 / 60000]\n",
            "Loss: 1.3999544382095337 [6400 / 60000]\n",
            "Loss: 1.3891348838806152 [6656 / 60000]\n",
            "Loss: 1.3415052890777588 [6912 / 60000]\n",
            "Loss: 1.4080144166946411 [7168 / 60000]\n",
            "Loss: 1.3678480386734009 [7424 / 60000]\n",
            "Loss: 1.3945943117141724 [7680 / 60000]\n",
            "Loss: 1.3598116636276245 [7936 / 60000]\n",
            "Loss: 1.3693398237228394 [8192 / 60000]\n",
            "Loss: 1.392364501953125 [8448 / 60000]\n",
            "Loss: 1.3475404977798462 [8704 / 60000]\n",
            "Loss: 1.3764506578445435 [8960 / 60000]\n",
            "Loss: 1.376526951789856 [9216 / 60000]\n",
            "Loss: 1.439373254776001 [9472 / 60000]\n",
            "Loss: 1.3473849296569824 [9728 / 60000]\n",
            "Loss: 1.3978029489517212 [9984 / 60000]\n",
            "Loss: 1.3049412965774536 [10240 / 60000]\n",
            "Loss: 1.348074197769165 [10496 / 60000]\n",
            "Loss: 1.368809461593628 [10752 / 60000]\n",
            "Loss: 1.3744841814041138 [11008 / 60000]\n",
            "Loss: 1.3523263931274414 [11264 / 60000]\n",
            "Loss: 1.3097476959228516 [11520 / 60000]\n",
            "Loss: 1.3777670860290527 [11776 / 60000]\n",
            "Loss: 1.3266515731811523 [12032 / 60000]\n",
            "Loss: 1.3969208002090454 [12288 / 60000]\n",
            "Loss: 1.3570555448532104 [12544 / 60000]\n",
            "Loss: 1.3947176933288574 [12800 / 60000]\n",
            "Loss: 1.3889968395233154 [13056 / 60000]\n",
            "Loss: 1.3751367330551147 [13312 / 60000]\n",
            "Loss: 1.3417575359344482 [13568 / 60000]\n",
            "Loss: 1.3855341672897339 [13824 / 60000]\n",
            "Loss: 1.3317866325378418 [14080 / 60000]\n",
            "Loss: 1.3850404024124146 [14336 / 60000]\n",
            "Loss: 1.357728123664856 [14592 / 60000]\n",
            "Loss: 1.3690725564956665 [14848 / 60000]\n",
            "Loss: 1.3491421937942505 [15104 / 60000]\n",
            "Loss: 1.3787882328033447 [15360 / 60000]\n",
            "Loss: 1.3470585346221924 [15616 / 60000]\n",
            "Loss: 1.3845255374908447 [15872 / 60000]\n",
            "Loss: 1.313534140586853 [16128 / 60000]\n",
            "Loss: 1.340181827545166 [16384 / 60000]\n",
            "Loss: 1.380643606185913 [16640 / 60000]\n",
            "Loss: 1.3304179906845093 [16896 / 60000]\n",
            "Loss: 1.4173940420150757 [17152 / 60000]\n",
            "Loss: 1.3667731285095215 [17408 / 60000]\n",
            "Loss: 1.3627281188964844 [17664 / 60000]\n",
            "Loss: 1.4043781757354736 [17920 / 60000]\n",
            "Loss: 1.3763244152069092 [18176 / 60000]\n",
            "Loss: 1.3120126724243164 [18432 / 60000]\n",
            "Loss: 1.4107224941253662 [18688 / 60000]\n",
            "Loss: 1.3473503589630127 [18944 / 60000]\n",
            "Loss: 1.4024220705032349 [19200 / 60000]\n",
            "Loss: 1.31711745262146 [19456 / 60000]\n",
            "Loss: 1.3601680994033813 [19712 / 60000]\n",
            "Loss: 1.344063639640808 [19968 / 60000]\n",
            "Loss: 1.389866590499878 [20224 / 60000]\n",
            "Loss: 1.4014136791229248 [20480 / 60000]\n",
            "Loss: 1.3303266763687134 [20736 / 60000]\n",
            "Loss: 1.3013468980789185 [20992 / 60000]\n",
            "Loss: 1.378364086151123 [21248 / 60000]\n",
            "Loss: 1.3421766757965088 [21504 / 60000]\n",
            "Loss: 1.3647739887237549 [21760 / 60000]\n",
            "Loss: 1.3305310010910034 [22016 / 60000]\n",
            "Loss: 1.3710393905639648 [22272 / 60000]\n",
            "Loss: 1.3943144083023071 [22528 / 60000]\n",
            "Loss: 1.404496192932129 [22784 / 60000]\n",
            "Loss: 1.3928418159484863 [23040 / 60000]\n",
            "Loss: 1.3830207586288452 [23296 / 60000]\n",
            "Loss: 1.3524543046951294 [23552 / 60000]\n",
            "Loss: 1.330129623413086 [23808 / 60000]\n",
            "Loss: 1.394916296005249 [24064 / 60000]\n",
            "Loss: 1.3061579465866089 [24320 / 60000]\n",
            "Loss: 1.285640835762024 [24576 / 60000]\n",
            "Loss: 1.2989001274108887 [24832 / 60000]\n",
            "Loss: 1.3907184600830078 [25088 / 60000]\n",
            "Loss: 1.3463507890701294 [25344 / 60000]\n",
            "Loss: 1.3484541177749634 [25600 / 60000]\n",
            "Loss: 1.333356499671936 [25856 / 60000]\n",
            "Loss: 1.3735733032226562 [26112 / 60000]\n",
            "Loss: 1.3606778383255005 [26368 / 60000]\n",
            "Loss: 1.3514479398727417 [26624 / 60000]\n",
            "Loss: 1.3570961952209473 [26880 / 60000]\n",
            "Loss: 1.3650068044662476 [27136 / 60000]\n",
            "Loss: 1.3299249410629272 [27392 / 60000]\n",
            "Loss: 1.4338160753250122 [27648 / 60000]\n",
            "Loss: 1.3598192930221558 [27904 / 60000]\n",
            "Loss: 1.3278837203979492 [28160 / 60000]\n",
            "Loss: 1.3290021419525146 [28416 / 60000]\n",
            "Loss: 1.35423743724823 [28672 / 60000]\n",
            "Loss: 1.3540856838226318 [28928 / 60000]\n",
            "Loss: 1.3579638004302979 [29184 / 60000]\n",
            "Loss: 1.3095238208770752 [29440 / 60000]\n",
            "Loss: 1.3031810522079468 [29696 / 60000]\n",
            "Loss: 1.2844256162643433 [29952 / 60000]\n",
            "Loss: 1.3158962726593018 [30208 / 60000]\n",
            "Loss: 1.3980512619018555 [30464 / 60000]\n",
            "Loss: 1.3549811840057373 [30720 / 60000]\n",
            "Loss: 1.3006603717803955 [30976 / 60000]\n",
            "Loss: 1.3611222505569458 [31232 / 60000]\n",
            "Loss: 1.3353734016418457 [31488 / 60000]\n",
            "Loss: 1.3106913566589355 [31744 / 60000]\n",
            "Loss: 1.328983187675476 [32000 / 60000]\n",
            "Loss: 1.3826069831848145 [32256 / 60000]\n",
            "Loss: 1.3704931735992432 [32512 / 60000]\n",
            "Loss: 1.3320720195770264 [32768 / 60000]\n",
            "Loss: 1.3114137649536133 [33024 / 60000]\n",
            "Loss: 1.3235961198806763 [33280 / 60000]\n",
            "Loss: 1.3748067617416382 [33536 / 60000]\n",
            "Loss: 1.4066694974899292 [33792 / 60000]\n",
            "Loss: 1.3969324827194214 [34048 / 60000]\n",
            "Loss: 1.3017284870147705 [34304 / 60000]\n",
            "Loss: 1.297035813331604 [34560 / 60000]\n",
            "Loss: 1.335767388343811 [34816 / 60000]\n",
            "Loss: 1.3492584228515625 [35072 / 60000]\n",
            "Loss: 1.3406320810317993 [35328 / 60000]\n",
            "Loss: 1.386267066001892 [35584 / 60000]\n",
            "Loss: 1.3208674192428589 [35840 / 60000]\n",
            "Loss: 1.365952491760254 [36096 / 60000]\n",
            "Loss: 1.3487801551818848 [36352 / 60000]\n",
            "Loss: 1.3430637121200562 [36608 / 60000]\n",
            "Loss: 1.3864113092422485 [36864 / 60000]\n",
            "Loss: 1.325909972190857 [37120 / 60000]\n",
            "Loss: 1.3812140226364136 [37376 / 60000]\n",
            "Loss: 1.3062410354614258 [37632 / 60000]\n",
            "Loss: 1.3612005710601807 [37888 / 60000]\n",
            "Loss: 1.3329942226409912 [38144 / 60000]\n",
            "Loss: 1.3591749668121338 [38400 / 60000]\n",
            "Loss: 1.3393683433532715 [38656 / 60000]\n",
            "Loss: 1.318573236465454 [38912 / 60000]\n",
            "Loss: 1.3612343072891235 [39168 / 60000]\n",
            "Loss: 1.3268487453460693 [39424 / 60000]\n",
            "Loss: 1.345529556274414 [39680 / 60000]\n",
            "Loss: 1.3480242490768433 [39936 / 60000]\n",
            "Loss: 1.3574050664901733 [40192 / 60000]\n",
            "Loss: 1.3574249744415283 [40448 / 60000]\n",
            "Loss: 1.3097878694534302 [40704 / 60000]\n",
            "Loss: 1.305602788925171 [40960 / 60000]\n",
            "Loss: 1.302668809890747 [41216 / 60000]\n",
            "Loss: 1.317060947418213 [41472 / 60000]\n",
            "Loss: 1.351338267326355 [41728 / 60000]\n",
            "Loss: 1.3241641521453857 [41984 / 60000]\n",
            "Loss: 1.339408278465271 [42240 / 60000]\n",
            "Loss: 1.3347333669662476 [42496 / 60000]\n",
            "Loss: 1.339859127998352 [42752 / 60000]\n",
            "Loss: 1.311749815940857 [43008 / 60000]\n",
            "Loss: 1.3687559366226196 [43264 / 60000]\n",
            "Loss: 1.3316216468811035 [43520 / 60000]\n",
            "Loss: 1.3423274755477905 [43776 / 60000]\n",
            "Loss: 1.3266205787658691 [44032 / 60000]\n",
            "Loss: 1.3212196826934814 [44288 / 60000]\n",
            "Loss: 1.383506417274475 [44544 / 60000]\n",
            "Loss: 1.3436315059661865 [44800 / 60000]\n",
            "Loss: 1.3519389629364014 [45056 / 60000]\n",
            "Loss: 1.3696365356445312 [45312 / 60000]\n",
            "Loss: 1.3573920726776123 [45568 / 60000]\n",
            "Loss: 1.3227492570877075 [45824 / 60000]\n",
            "Loss: 1.3738110065460205 [46080 / 60000]\n",
            "Loss: 1.3415958881378174 [46336 / 60000]\n",
            "Loss: 1.364567756652832 [46592 / 60000]\n",
            "Loss: 1.3304976224899292 [46848 / 60000]\n",
            "Loss: 1.3553104400634766 [47104 / 60000]\n",
            "Loss: 1.2776368856430054 [47360 / 60000]\n",
            "Loss: 1.3089410066604614 [47616 / 60000]\n",
            "Loss: 1.287917971611023 [47872 / 60000]\n",
            "Loss: 1.3176555633544922 [48128 / 60000]\n",
            "Loss: 1.264695644378662 [48384 / 60000]\n",
            "Loss: 1.2485533952713013 [48640 / 60000]\n",
            "Loss: 1.3041412830352783 [48896 / 60000]\n",
            "Loss: 1.3445560932159424 [49152 / 60000]\n",
            "Loss: 1.343526005744934 [49408 / 60000]\n",
            "Loss: 1.3461405038833618 [49664 / 60000]\n",
            "Loss: 1.3542200326919556 [49920 / 60000]\n",
            "Loss: 1.29900324344635 [50176 / 60000]\n",
            "Loss: 1.3337020874023438 [50432 / 60000]\n",
            "Loss: 1.2650364637374878 [50688 / 60000]\n",
            "Loss: 1.3150098323822021 [50944 / 60000]\n",
            "Loss: 1.310868740081787 [51200 / 60000]\n",
            "Loss: 1.311110258102417 [51456 / 60000]\n",
            "Loss: 1.2950607538223267 [51712 / 60000]\n",
            "Loss: 1.3138567209243774 [51968 / 60000]\n",
            "Loss: 1.3289861679077148 [52224 / 60000]\n",
            "Loss: 1.3230849504470825 [52480 / 60000]\n",
            "Loss: 1.320715069770813 [52736 / 60000]\n",
            "Loss: 1.3266468048095703 [52992 / 60000]\n",
            "Loss: 1.3257302045822144 [53248 / 60000]\n",
            "Loss: 1.3244035243988037 [53504 / 60000]\n",
            "Loss: 1.2832086086273193 [53760 / 60000]\n",
            "Loss: 1.3095483779907227 [54016 / 60000]\n",
            "Loss: 1.2869784832000732 [54272 / 60000]\n",
            "Loss: 1.3486088514328003 [54528 / 60000]\n",
            "Loss: 1.3298367261886597 [54784 / 60000]\n",
            "Loss: 1.3661773204803467 [55040 / 60000]\n",
            "Loss: 1.3695780038833618 [55296 / 60000]\n",
            "Loss: 1.350913405418396 [55552 / 60000]\n",
            "Loss: 1.3130625486373901 [55808 / 60000]\n",
            "Loss: 1.2759277820587158 [56064 / 60000]\n",
            "Loss: 1.3211443424224854 [56320 / 60000]\n",
            "Loss: 1.2971562147140503 [56576 / 60000]\n",
            "Loss: 1.3383424282073975 [56832 / 60000]\n",
            "Loss: 1.2983272075653076 [57088 / 60000]\n",
            "Loss: 1.2777671813964844 [57344 / 60000]\n",
            "Loss: 1.2933366298675537 [57600 / 60000]\n",
            "Loss: 1.3087595701217651 [57856 / 60000]\n",
            "Loss: 1.2946110963821411 [58112 / 60000]\n",
            "Loss: 1.3169878721237183 [58368 / 60000]\n",
            "Loss: 1.3219114542007446 [58624 / 60000]\n",
            "Loss: 1.267240285873413 [58880 / 60000]\n",
            "Loss: 1.2690305709838867 [59136 / 60000]\n",
            "Loss: 1.299529790878296 [59392 / 60000]\n",
            "Loss: 1.292328953742981 [59648 / 60000]\n",
            "Loss: 1.3338714838027954 [22464 / 60000]\n",
            "Test Loss: 1.2877533912658692 Accuracy:79.38\n",
            "epoch:15=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.3133078813552856 [0 / 60000]\n",
            "Loss: 1.2818429470062256 [256 / 60000]\n",
            "Loss: 1.2947492599487305 [512 / 60000]\n",
            "Loss: 1.3160443305969238 [768 / 60000]\n",
            "Loss: 1.308239221572876 [1024 / 60000]\n",
            "Loss: 1.2816311120986938 [1280 / 60000]\n",
            "Loss: 1.287208914756775 [1536 / 60000]\n",
            "Loss: 1.2819104194641113 [1792 / 60000]\n",
            "Loss: 1.2954720258712769 [2048 / 60000]\n",
            "Loss: 1.3327691555023193 [2304 / 60000]\n",
            "Loss: 1.2857892513275146 [2560 / 60000]\n",
            "Loss: 1.3406012058258057 [2816 / 60000]\n",
            "Loss: 1.2837939262390137 [3072 / 60000]\n",
            "Loss: 1.3342173099517822 [3328 / 60000]\n",
            "Loss: 1.2996571063995361 [3584 / 60000]\n",
            "Loss: 1.2416460514068604 [3840 / 60000]\n",
            "Loss: 1.2657383680343628 [4096 / 60000]\n",
            "Loss: 1.3095624446868896 [4352 / 60000]\n",
            "Loss: 1.308036208152771 [4608 / 60000]\n",
            "Loss: 1.3388010263442993 [4864 / 60000]\n",
            "Loss: 1.3255608081817627 [5120 / 60000]\n",
            "Loss: 1.3028287887573242 [5376 / 60000]\n",
            "Loss: 1.3295398950576782 [5632 / 60000]\n",
            "Loss: 1.3216975927352905 [5888 / 60000]\n",
            "Loss: 1.3064769506454468 [6144 / 60000]\n",
            "Loss: 1.3618992567062378 [6400 / 60000]\n",
            "Loss: 1.312302827835083 [6656 / 60000]\n",
            "Loss: 1.3601466417312622 [6912 / 60000]\n",
            "Loss: 1.3433033227920532 [7168 / 60000]\n",
            "Loss: 1.3173891305923462 [7424 / 60000]\n",
            "Loss: 1.3118062019348145 [7680 / 60000]\n",
            "Loss: 1.2759604454040527 [7936 / 60000]\n",
            "Loss: 1.3429070711135864 [8192 / 60000]\n",
            "Loss: 1.2635306119918823 [8448 / 60000]\n",
            "Loss: 1.2990870475769043 [8704 / 60000]\n",
            "Loss: 1.3349339962005615 [8960 / 60000]\n",
            "Loss: 1.3513069152832031 [9216 / 60000]\n",
            "Loss: 1.3392459154129028 [9472 / 60000]\n",
            "Loss: 1.291136622428894 [9728 / 60000]\n",
            "Loss: 1.2619960308074951 [9984 / 60000]\n",
            "Loss: 1.3117332458496094 [10240 / 60000]\n",
            "Loss: 1.2900656461715698 [10496 / 60000]\n",
            "Loss: 1.2822102308273315 [10752 / 60000]\n",
            "Loss: 1.3241004943847656 [11008 / 60000]\n",
            "Loss: 1.2882720232009888 [11264 / 60000]\n",
            "Loss: 1.2837690114974976 [11520 / 60000]\n",
            "Loss: 1.2997456789016724 [11776 / 60000]\n",
            "Loss: 1.275813102722168 [12032 / 60000]\n",
            "Loss: 1.3138601779937744 [12288 / 60000]\n",
            "Loss: 1.2540147304534912 [12544 / 60000]\n",
            "Loss: 1.3229602575302124 [12800 / 60000]\n",
            "Loss: 1.3132576942443848 [13056 / 60000]\n",
            "Loss: 1.2415835857391357 [13312 / 60000]\n",
            "Loss: 1.2945500612258911 [13568 / 60000]\n",
            "Loss: 1.343937873840332 [13824 / 60000]\n",
            "Loss: 1.2827513217926025 [14080 / 60000]\n",
            "Loss: 1.3615293502807617 [14336 / 60000]\n",
            "Loss: 1.2131808996200562 [14592 / 60000]\n",
            "Loss: 1.2742263078689575 [14848 / 60000]\n",
            "Loss: 1.301982045173645 [15104 / 60000]\n",
            "Loss: 1.3096837997436523 [15360 / 60000]\n",
            "Loss: 1.2231838703155518 [15616 / 60000]\n",
            "Loss: 1.3261055946350098 [15872 / 60000]\n",
            "Loss: 1.3286875486373901 [16128 / 60000]\n",
            "Loss: 1.2732409238815308 [16384 / 60000]\n",
            "Loss: 1.2999728918075562 [16640 / 60000]\n",
            "Loss: 1.311120867729187 [16896 / 60000]\n",
            "Loss: 1.3194106817245483 [17152 / 60000]\n",
            "Loss: 1.2949254512786865 [17408 / 60000]\n",
            "Loss: 1.3048073053359985 [17664 / 60000]\n",
            "Loss: 1.304813265800476 [17920 / 60000]\n",
            "Loss: 1.323076844215393 [18176 / 60000]\n",
            "Loss: 1.320759892463684 [18432 / 60000]\n",
            "Loss: 1.2848014831542969 [18688 / 60000]\n",
            "Loss: 1.2637454271316528 [18944 / 60000]\n",
            "Loss: 1.2785706520080566 [19200 / 60000]\n",
            "Loss: 1.2988725900650024 [19456 / 60000]\n",
            "Loss: 1.3714810609817505 [19712 / 60000]\n",
            "Loss: 1.2993762493133545 [19968 / 60000]\n",
            "Loss: 1.3424344062805176 [20224 / 60000]\n",
            "Loss: 1.326950192451477 [20480 / 60000]\n",
            "Loss: 1.2699344158172607 [20736 / 60000]\n",
            "Loss: 1.212257742881775 [20992 / 60000]\n",
            "Loss: 1.284978985786438 [21248 / 60000]\n",
            "Loss: 1.3443870544433594 [21504 / 60000]\n",
            "Loss: 1.2652312517166138 [21760 / 60000]\n",
            "Loss: 1.3191006183624268 [22016 / 60000]\n",
            "Loss: 1.3152399063110352 [22272 / 60000]\n",
            "Loss: 1.2813409566879272 [22528 / 60000]\n",
            "Loss: 1.2901169061660767 [22784 / 60000]\n",
            "Loss: 1.294314980506897 [23040 / 60000]\n",
            "Loss: 1.3292638063430786 [23296 / 60000]\n",
            "Loss: 1.2354329824447632 [23552 / 60000]\n",
            "Loss: 1.335339903831482 [23808 / 60000]\n",
            "Loss: 1.2863904237747192 [24064 / 60000]\n",
            "Loss: 1.2963811159133911 [24320 / 60000]\n",
            "Loss: 1.2724168300628662 [24576 / 60000]\n",
            "Loss: 1.2761622667312622 [24832 / 60000]\n",
            "Loss: 1.2692713737487793 [25088 / 60000]\n",
            "Loss: 1.3124138116836548 [25344 / 60000]\n",
            "Loss: 1.230419635772705 [25600 / 60000]\n",
            "Loss: 1.255068063735962 [25856 / 60000]\n",
            "Loss: 1.2847099304199219 [26112 / 60000]\n",
            "Loss: 1.2883011102676392 [26368 / 60000]\n",
            "Loss: 1.292253017425537 [26624 / 60000]\n",
            "Loss: 1.2879618406295776 [26880 / 60000]\n",
            "Loss: 1.2638542652130127 [27136 / 60000]\n",
            "Loss: 1.2866848707199097 [27392 / 60000]\n",
            "Loss: 1.2548284530639648 [27648 / 60000]\n",
            "Loss: 1.2538783550262451 [27904 / 60000]\n",
            "Loss: 1.286635160446167 [28160 / 60000]\n",
            "Loss: 1.3116704225540161 [28416 / 60000]\n",
            "Loss: 1.3050206899642944 [28672 / 60000]\n",
            "Loss: 1.2731525897979736 [28928 / 60000]\n",
            "Loss: 1.3311139345169067 [29184 / 60000]\n",
            "Loss: 1.2411346435546875 [29440 / 60000]\n",
            "Loss: 1.2017033100128174 [29696 / 60000]\n",
            "Loss: 1.2243512868881226 [29952 / 60000]\n",
            "Loss: 1.2827397584915161 [30208 / 60000]\n",
            "Loss: 1.3171367645263672 [30464 / 60000]\n",
            "Loss: 1.249037742614746 [30720 / 60000]\n",
            "Loss: 1.2485988140106201 [30976 / 60000]\n",
            "Loss: 1.33391535282135 [31232 / 60000]\n",
            "Loss: 1.2187113761901855 [31488 / 60000]\n",
            "Loss: 1.2997764348983765 [31744 / 60000]\n",
            "Loss: 1.2857263088226318 [32000 / 60000]\n",
            "Loss: 1.215620994567871 [32256 / 60000]\n",
            "Loss: 1.2907662391662598 [32512 / 60000]\n",
            "Loss: 1.245662808418274 [32768 / 60000]\n",
            "Loss: 1.2469449043273926 [33024 / 60000]\n",
            "Loss: 1.2548483610153198 [33280 / 60000]\n",
            "Loss: 1.238024353981018 [33536 / 60000]\n",
            "Loss: 1.2654447555541992 [33792 / 60000]\n",
            "Loss: 1.2427905797958374 [34048 / 60000]\n",
            "Loss: 1.2533822059631348 [34304 / 60000]\n",
            "Loss: 1.2423624992370605 [34560 / 60000]\n",
            "Loss: 1.2527269124984741 [34816 / 60000]\n",
            "Loss: 1.2183064222335815 [35072 / 60000]\n",
            "Loss: 1.22995924949646 [35328 / 60000]\n",
            "Loss: 1.215590238571167 [35584 / 60000]\n",
            "Loss: 1.262516975402832 [35840 / 60000]\n",
            "Loss: 1.2609593868255615 [36096 / 60000]\n",
            "Loss: 1.2726428508758545 [36352 / 60000]\n",
            "Loss: 1.2413915395736694 [36608 / 60000]\n",
            "Loss: 1.2524914741516113 [36864 / 60000]\n",
            "Loss: 1.2858742475509644 [37120 / 60000]\n",
            "Loss: 1.2626476287841797 [37376 / 60000]\n",
            "Loss: 1.2552361488342285 [37632 / 60000]\n",
            "Loss: 1.2789232730865479 [37888 / 60000]\n",
            "Loss: 1.288697600364685 [38144 / 60000]\n",
            "Loss: 1.226665735244751 [38400 / 60000]\n",
            "Loss: 1.2479296922683716 [38656 / 60000]\n",
            "Loss: 1.2487369775772095 [38912 / 60000]\n",
            "Loss: 1.283170461654663 [39168 / 60000]\n",
            "Loss: 1.2362265586853027 [39424 / 60000]\n",
            "Loss: 1.3063435554504395 [39680 / 60000]\n",
            "Loss: 1.2334253787994385 [39936 / 60000]\n",
            "Loss: 1.2683079242706299 [40192 / 60000]\n",
            "Loss: 1.3172701597213745 [40448 / 60000]\n",
            "Loss: 1.2321635484695435 [40704 / 60000]\n",
            "Loss: 1.3148988485336304 [40960 / 60000]\n",
            "Loss: 1.1959611177444458 [41216 / 60000]\n",
            "Loss: 1.1993335485458374 [41472 / 60000]\n",
            "Loss: 1.2742191553115845 [41728 / 60000]\n",
            "Loss: 1.2324612140655518 [41984 / 60000]\n",
            "Loss: 1.2766995429992676 [42240 / 60000]\n",
            "Loss: 1.2056398391723633 [42496 / 60000]\n",
            "Loss: 1.2446051836013794 [42752 / 60000]\n",
            "Loss: 1.2598748207092285 [43008 / 60000]\n",
            "Loss: 1.2878215312957764 [43264 / 60000]\n",
            "Loss: 1.2803345918655396 [43520 / 60000]\n",
            "Loss: 1.2293002605438232 [43776 / 60000]\n",
            "Loss: 1.2196084260940552 [44032 / 60000]\n",
            "Loss: 1.274830937385559 [44288 / 60000]\n",
            "Loss: 1.218613624572754 [44544 / 60000]\n",
            "Loss: 1.289556860923767 [44800 / 60000]\n",
            "Loss: 1.2688218355178833 [45056 / 60000]\n",
            "Loss: 1.2383177280426025 [45312 / 60000]\n",
            "Loss: 1.2148244380950928 [45568 / 60000]\n",
            "Loss: 1.237868070602417 [45824 / 60000]\n",
            "Loss: 1.2726958990097046 [46080 / 60000]\n",
            "Loss: 1.2122654914855957 [46336 / 60000]\n",
            "Loss: 1.1994942426681519 [46592 / 60000]\n",
            "Loss: 1.3353098630905151 [46848 / 60000]\n",
            "Loss: 1.295088768005371 [47104 / 60000]\n",
            "Loss: 1.235282063484192 [47360 / 60000]\n",
            "Loss: 1.2779500484466553 [47616 / 60000]\n",
            "Loss: 1.2441585063934326 [47872 / 60000]\n",
            "Loss: 1.2541711330413818 [48128 / 60000]\n",
            "Loss: 1.246232271194458 [48384 / 60000]\n",
            "Loss: 1.2547451257705688 [48640 / 60000]\n",
            "Loss: 1.2472094297409058 [48896 / 60000]\n",
            "Loss: 1.2200711965560913 [49152 / 60000]\n",
            "Loss: 1.2466665506362915 [49408 / 60000]\n",
            "Loss: 1.2683591842651367 [49664 / 60000]\n",
            "Loss: 1.2555593252182007 [49920 / 60000]\n",
            "Loss: 1.2403717041015625 [50176 / 60000]\n",
            "Loss: 1.2562909126281738 [50432 / 60000]\n",
            "Loss: 1.2551862001419067 [50688 / 60000]\n",
            "Loss: 1.2962008714675903 [50944 / 60000]\n",
            "Loss: 1.2364323139190674 [51200 / 60000]\n",
            "Loss: 1.2057195901870728 [51456 / 60000]\n",
            "Loss: 1.2107571363449097 [51712 / 60000]\n",
            "Loss: 1.2387001514434814 [51968 / 60000]\n",
            "Loss: 1.2847404479980469 [52224 / 60000]\n",
            "Loss: 1.2351597547531128 [52480 / 60000]\n",
            "Loss: 1.2168058156967163 [52736 / 60000]\n",
            "Loss: 1.3075575828552246 [52992 / 60000]\n",
            "Loss: 1.24828040599823 [53248 / 60000]\n",
            "Loss: 1.2356733083724976 [53504 / 60000]\n",
            "Loss: 1.3181428909301758 [53760 / 60000]\n",
            "Loss: 1.2546842098236084 [54016 / 60000]\n",
            "Loss: 1.214664101600647 [54272 / 60000]\n",
            "Loss: 1.292183756828308 [54528 / 60000]\n",
            "Loss: 1.2609628438949585 [54784 / 60000]\n",
            "Loss: 1.235495924949646 [55040 / 60000]\n",
            "Loss: 1.24705171585083 [55296 / 60000]\n",
            "Loss: 1.2603373527526855 [55552 / 60000]\n",
            "Loss: 1.2955358028411865 [55808 / 60000]\n",
            "Loss: 1.2706689834594727 [56064 / 60000]\n",
            "Loss: 1.2408056259155273 [56320 / 60000]\n",
            "Loss: 1.2796046733856201 [56576 / 60000]\n",
            "Loss: 1.2710577249526978 [56832 / 60000]\n",
            "Loss: 1.2527154684066772 [57088 / 60000]\n",
            "Loss: 1.241164207458496 [57344 / 60000]\n",
            "Loss: 1.242143988609314 [57600 / 60000]\n",
            "Loss: 1.2545626163482666 [57856 / 60000]\n",
            "Loss: 1.2430310249328613 [58112 / 60000]\n",
            "Loss: 1.258081316947937 [58368 / 60000]\n",
            "Loss: 1.294548511505127 [58624 / 60000]\n",
            "Loss: 1.239828109741211 [58880 / 60000]\n",
            "Loss: 1.2524032592773438 [59136 / 60000]\n",
            "Loss: 1.2097580432891846 [59392 / 60000]\n",
            "Loss: 1.2854598760604858 [59648 / 60000]\n",
            "Loss: 1.2223832607269287 [22464 / 60000]\n",
            "Test Loss: 1.2178829208016395 Accuracy:80.06\n",
            "epoch:16=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.1735954284667969 [0 / 60000]\n",
            "Loss: 1.2570130825042725 [256 / 60000]\n",
            "Loss: 1.1793339252471924 [512 / 60000]\n",
            "Loss: 1.260784387588501 [768 / 60000]\n",
            "Loss: 1.206689476966858 [1024 / 60000]\n",
            "Loss: 1.2595820426940918 [1280 / 60000]\n",
            "Loss: 1.2707123756408691 [1536 / 60000]\n",
            "Loss: 1.2078756093978882 [1792 / 60000]\n",
            "Loss: 1.2634961605072021 [2048 / 60000]\n",
            "Loss: 1.2790213823318481 [2304 / 60000]\n",
            "Loss: 1.2464311122894287 [2560 / 60000]\n",
            "Loss: 1.199692964553833 [2816 / 60000]\n",
            "Loss: 1.2424397468566895 [3072 / 60000]\n",
            "Loss: 1.249259352684021 [3328 / 60000]\n",
            "Loss: 1.2276071310043335 [3584 / 60000]\n",
            "Loss: 1.224537968635559 [3840 / 60000]\n",
            "Loss: 1.2361736297607422 [4096 / 60000]\n",
            "Loss: 1.241915225982666 [4352 / 60000]\n",
            "Loss: 1.2427858114242554 [4608 / 60000]\n",
            "Loss: 1.2148576974868774 [4864 / 60000]\n",
            "Loss: 1.2196142673492432 [5120 / 60000]\n",
            "Loss: 1.2829134464263916 [5376 / 60000]\n",
            "Loss: 1.2219982147216797 [5632 / 60000]\n",
            "Loss: 1.2333208322525024 [5888 / 60000]\n",
            "Loss: 1.267202377319336 [6144 / 60000]\n",
            "Loss: 1.2362990379333496 [6400 / 60000]\n",
            "Loss: 1.1936336755752563 [6656 / 60000]\n",
            "Loss: 1.193756341934204 [6912 / 60000]\n",
            "Loss: 1.2004705667495728 [7168 / 60000]\n",
            "Loss: 1.259912371635437 [7424 / 60000]\n",
            "Loss: 1.2296818494796753 [7680 / 60000]\n",
            "Loss: 1.2593179941177368 [7936 / 60000]\n",
            "Loss: 1.207471251487732 [8192 / 60000]\n",
            "Loss: 1.278548002243042 [8448 / 60000]\n",
            "Loss: 1.2472987174987793 [8704 / 60000]\n",
            "Loss: 1.2418675422668457 [8960 / 60000]\n",
            "Loss: 1.2394218444824219 [9216 / 60000]\n",
            "Loss: 1.223899006843567 [9472 / 60000]\n",
            "Loss: 1.220531940460205 [9728 / 60000]\n",
            "Loss: 1.1985784769058228 [9984 / 60000]\n",
            "Loss: 1.1833070516586304 [10240 / 60000]\n",
            "Loss: 1.2544376850128174 [10496 / 60000]\n",
            "Loss: 1.2776882648468018 [10752 / 60000]\n",
            "Loss: 1.1643043756484985 [11008 / 60000]\n",
            "Loss: 1.251806378364563 [11264 / 60000]\n",
            "Loss: 1.1790691614151 [11520 / 60000]\n",
            "Loss: 1.281722068786621 [11776 / 60000]\n",
            "Loss: 1.2145754098892212 [12032 / 60000]\n",
            "Loss: 1.2280871868133545 [12288 / 60000]\n",
            "Loss: 1.2293184995651245 [12544 / 60000]\n",
            "Loss: 1.1841868162155151 [12800 / 60000]\n",
            "Loss: 1.1802337169647217 [13056 / 60000]\n",
            "Loss: 1.2246370315551758 [13312 / 60000]\n",
            "Loss: 1.2068253755569458 [13568 / 60000]\n",
            "Loss: 1.1724308729171753 [13824 / 60000]\n",
            "Loss: 1.2120758295059204 [14080 / 60000]\n",
            "Loss: 1.205990195274353 [14336 / 60000]\n",
            "Loss: 1.2045832872390747 [14592 / 60000]\n",
            "Loss: 1.1771255731582642 [14848 / 60000]\n",
            "Loss: 1.2716091871261597 [15104 / 60000]\n",
            "Loss: 1.2008190155029297 [15360 / 60000]\n",
            "Loss: 1.205134630203247 [15616 / 60000]\n",
            "Loss: 1.219792366027832 [15872 / 60000]\n",
            "Loss: 1.2479342222213745 [16128 / 60000]\n",
            "Loss: 1.2163845300674438 [16384 / 60000]\n",
            "Loss: 1.2551438808441162 [16640 / 60000]\n",
            "Loss: 1.1897166967391968 [16896 / 60000]\n",
            "Loss: 1.226639986038208 [17152 / 60000]\n",
            "Loss: 1.2455573081970215 [17408 / 60000]\n",
            "Loss: 1.256602168083191 [17664 / 60000]\n",
            "Loss: 1.1774145364761353 [17920 / 60000]\n",
            "Loss: 1.1854361295700073 [18176 / 60000]\n",
            "Loss: 1.1860257387161255 [18432 / 60000]\n",
            "Loss: 1.2080894708633423 [18688 / 60000]\n",
            "Loss: 1.1923167705535889 [18944 / 60000]\n",
            "Loss: 1.1573454141616821 [19200 / 60000]\n",
            "Loss: 1.200077772140503 [19456 / 60000]\n",
            "Loss: 1.2164493799209595 [19712 / 60000]\n",
            "Loss: 1.2203336954116821 [19968 / 60000]\n",
            "Loss: 1.2462220191955566 [20224 / 60000]\n",
            "Loss: 1.2249054908752441 [20480 / 60000]\n",
            "Loss: 1.1893773078918457 [20736 / 60000]\n",
            "Loss: 1.2229632139205933 [20992 / 60000]\n",
            "Loss: 1.1366223096847534 [21248 / 60000]\n",
            "Loss: 1.2479060888290405 [21504 / 60000]\n",
            "Loss: 1.2880343198776245 [21760 / 60000]\n",
            "Loss: 1.2508825063705444 [22016 / 60000]\n",
            "Loss: 1.2581408023834229 [22272 / 60000]\n",
            "Loss: 1.2205346822738647 [22528 / 60000]\n",
            "Loss: 1.204349398612976 [22784 / 60000]\n",
            "Loss: 1.2071990966796875 [23040 / 60000]\n",
            "Loss: 1.2621393203735352 [23296 / 60000]\n",
            "Loss: 1.2271496057510376 [23552 / 60000]\n",
            "Loss: 1.1619242429733276 [23808 / 60000]\n",
            "Loss: 1.1694085597991943 [24064 / 60000]\n",
            "Loss: 1.235159158706665 [24320 / 60000]\n",
            "Loss: 1.2705364227294922 [24576 / 60000]\n",
            "Loss: 1.2047513723373413 [24832 / 60000]\n",
            "Loss: 1.3081862926483154 [25088 / 60000]\n",
            "Loss: 1.235450029373169 [25344 / 60000]\n",
            "Loss: 1.1900311708450317 [25600 / 60000]\n",
            "Loss: 1.1953554153442383 [25856 / 60000]\n",
            "Loss: 1.1881814002990723 [26112 / 60000]\n",
            "Loss: 1.30457603931427 [26368 / 60000]\n",
            "Loss: 1.2151738405227661 [26624 / 60000]\n",
            "Loss: 1.2276344299316406 [26880 / 60000]\n",
            "Loss: 1.212683916091919 [27136 / 60000]\n",
            "Loss: 1.2045111656188965 [27392 / 60000]\n",
            "Loss: 1.2043176889419556 [27648 / 60000]\n",
            "Loss: 1.1754460334777832 [27904 / 60000]\n",
            "Loss: 1.1994765996932983 [28160 / 60000]\n",
            "Loss: 1.2660536766052246 [28416 / 60000]\n",
            "Loss: 1.2004337310791016 [28672 / 60000]\n",
            "Loss: 1.1939432621002197 [28928 / 60000]\n",
            "Loss: 1.2191776037216187 [29184 / 60000]\n",
            "Loss: 1.2184491157531738 [29440 / 60000]\n",
            "Loss: 1.1842588186264038 [29696 / 60000]\n",
            "Loss: 1.2345689535140991 [29952 / 60000]\n",
            "Loss: 1.2095335721969604 [30208 / 60000]\n",
            "Loss: 1.1658849716186523 [30464 / 60000]\n",
            "Loss: 1.214234709739685 [30720 / 60000]\n",
            "Loss: 1.2311944961547852 [30976 / 60000]\n",
            "Loss: 1.2385731935501099 [31232 / 60000]\n",
            "Loss: 1.1698787212371826 [31488 / 60000]\n",
            "Loss: 1.1977800130844116 [31744 / 60000]\n",
            "Loss: 1.1530083417892456 [32000 / 60000]\n",
            "Loss: 1.2291440963745117 [32256 / 60000]\n",
            "Loss: 1.1698670387268066 [32512 / 60000]\n",
            "Loss: 1.2194854021072388 [32768 / 60000]\n",
            "Loss: 1.1891517639160156 [33024 / 60000]\n",
            "Loss: 1.1837084293365479 [33280 / 60000]\n",
            "Loss: 1.2058420181274414 [33536 / 60000]\n",
            "Loss: 1.1560693979263306 [33792 / 60000]\n",
            "Loss: 1.200406551361084 [34048 / 60000]\n",
            "Loss: 1.1050739288330078 [34304 / 60000]\n",
            "Loss: 1.1791094541549683 [34560 / 60000]\n",
            "Loss: 1.2078609466552734 [34816 / 60000]\n",
            "Loss: 1.1891363859176636 [35072 / 60000]\n",
            "Loss: 1.1633976697921753 [35328 / 60000]\n",
            "Loss: 1.2101328372955322 [35584 / 60000]\n",
            "Loss: 1.2439427375793457 [35840 / 60000]\n",
            "Loss: 1.1768200397491455 [36096 / 60000]\n",
            "Loss: 1.1708263158798218 [36352 / 60000]\n",
            "Loss: 1.1779423952102661 [36608 / 60000]\n",
            "Loss: 1.1861450672149658 [36864 / 60000]\n",
            "Loss: 1.2385858297348022 [37120 / 60000]\n",
            "Loss: 1.2149624824523926 [37376 / 60000]\n",
            "Loss: 1.2062938213348389 [37632 / 60000]\n",
            "Loss: 1.2442734241485596 [37888 / 60000]\n",
            "Loss: 1.207056999206543 [38144 / 60000]\n",
            "Loss: 1.224775791168213 [38400 / 60000]\n",
            "Loss: 1.1748541593551636 [38656 / 60000]\n",
            "Loss: 1.2102137804031372 [38912 / 60000]\n",
            "Loss: 1.1330592632293701 [39168 / 60000]\n",
            "Loss: 1.1761072874069214 [39424 / 60000]\n",
            "Loss: 1.192742943763733 [39680 / 60000]\n",
            "Loss: 1.2497655153274536 [39936 / 60000]\n",
            "Loss: 1.1244138479232788 [40192 / 60000]\n",
            "Loss: 1.171263337135315 [40448 / 60000]\n",
            "Loss: 1.2380517721176147 [40704 / 60000]\n",
            "Loss: 1.1535043716430664 [40960 / 60000]\n",
            "Loss: 1.192382574081421 [41216 / 60000]\n",
            "Loss: 1.2123888731002808 [41472 / 60000]\n",
            "Loss: 1.1340333223342896 [41728 / 60000]\n",
            "Loss: 1.2308547496795654 [41984 / 60000]\n",
            "Loss: 1.1929986476898193 [42240 / 60000]\n",
            "Loss: 1.2004122734069824 [42496 / 60000]\n",
            "Loss: 1.2285631895065308 [42752 / 60000]\n",
            "Loss: 1.2238270044326782 [43008 / 60000]\n",
            "Loss: 1.2035638093948364 [43264 / 60000]\n",
            "Loss: 1.2028610706329346 [43520 / 60000]\n",
            "Loss: 1.183422565460205 [43776 / 60000]\n",
            "Loss: 1.2264903783798218 [44032 / 60000]\n",
            "Loss: 1.227354645729065 [44288 / 60000]\n",
            "Loss: 1.1671839952468872 [44544 / 60000]\n",
            "Loss: 1.2369725704193115 [44800 / 60000]\n",
            "Loss: 1.1292706727981567 [45056 / 60000]\n",
            "Loss: 1.1915533542633057 [45312 / 60000]\n",
            "Loss: 1.1429792642593384 [45568 / 60000]\n",
            "Loss: 1.203950047492981 [45824 / 60000]\n",
            "Loss: 1.2011018991470337 [46080 / 60000]\n",
            "Loss: 1.1942487955093384 [46336 / 60000]\n",
            "Loss: 1.171256184577942 [46592 / 60000]\n",
            "Loss: 1.2370036840438843 [46848 / 60000]\n",
            "Loss: 1.2229831218719482 [47104 / 60000]\n",
            "Loss: 1.2022849321365356 [47360 / 60000]\n",
            "Loss: 1.2195069789886475 [47616 / 60000]\n",
            "Loss: 1.1975198984146118 [47872 / 60000]\n",
            "Loss: 1.2259365320205688 [48128 / 60000]\n",
            "Loss: 1.2164506912231445 [48384 / 60000]\n",
            "Loss: 1.2602113485336304 [48640 / 60000]\n",
            "Loss: 1.2090877294540405 [48896 / 60000]\n",
            "Loss: 1.149308443069458 [49152 / 60000]\n",
            "Loss: 1.1900269985198975 [49408 / 60000]\n",
            "Loss: 1.1984059810638428 [49664 / 60000]\n",
            "Loss: 1.19951331615448 [49920 / 60000]\n",
            "Loss: 1.1665420532226562 [50176 / 60000]\n",
            "Loss: 1.2084987163543701 [50432 / 60000]\n",
            "Loss: 1.181774616241455 [50688 / 60000]\n",
            "Loss: 1.1949129104614258 [50944 / 60000]\n",
            "Loss: 1.2528940439224243 [51200 / 60000]\n",
            "Loss: 1.1876739263534546 [51456 / 60000]\n",
            "Loss: 1.1801377534866333 [51712 / 60000]\n",
            "Loss: 1.2271451950073242 [51968 / 60000]\n",
            "Loss: 1.1876721382141113 [52224 / 60000]\n",
            "Loss: 1.2530287504196167 [52480 / 60000]\n",
            "Loss: 1.1864820718765259 [52736 / 60000]\n",
            "Loss: 1.2004132270812988 [52992 / 60000]\n",
            "Loss: 1.153780460357666 [53248 / 60000]\n",
            "Loss: 1.1698951721191406 [53504 / 60000]\n",
            "Loss: 1.1983712911605835 [53760 / 60000]\n",
            "Loss: 1.191054105758667 [54016 / 60000]\n",
            "Loss: 1.2014660835266113 [54272 / 60000]\n",
            "Loss: 1.2177093029022217 [54528 / 60000]\n",
            "Loss: 1.1892478466033936 [54784 / 60000]\n",
            "Loss: 1.1788175106048584 [55040 / 60000]\n",
            "Loss: 1.1825573444366455 [55296 / 60000]\n",
            "Loss: 1.234327793121338 [55552 / 60000]\n",
            "Loss: 1.1886860132217407 [55808 / 60000]\n",
            "Loss: 1.195503830909729 [56064 / 60000]\n",
            "Loss: 1.158615231513977 [56320 / 60000]\n",
            "Loss: 1.2388890981674194 [56576 / 60000]\n",
            "Loss: 1.1928359270095825 [56832 / 60000]\n",
            "Loss: 1.1707509756088257 [57088 / 60000]\n",
            "Loss: 1.1642794609069824 [57344 / 60000]\n",
            "Loss: 1.197877049446106 [57600 / 60000]\n",
            "Loss: 1.1945103406906128 [57856 / 60000]\n",
            "Loss: 1.1481366157531738 [58112 / 60000]\n",
            "Loss: 1.1771018505096436 [58368 / 60000]\n",
            "Loss: 1.2872354984283447 [58624 / 60000]\n",
            "Loss: 1.197752594947815 [58880 / 60000]\n",
            "Loss: 1.2279446125030518 [59136 / 60000]\n",
            "Loss: 1.27200186252594 [59392 / 60000]\n",
            "Loss: 1.2049713134765625 [59648 / 60000]\n",
            "Loss: 1.0188279151916504 [22464 / 60000]\n",
            "Test Loss: 1.1535347655415535 Accuracy:80.62\n",
            "epoch:17=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.1640316247940063 [0 / 60000]\n",
            "Loss: 1.2085671424865723 [256 / 60000]\n",
            "Loss: 1.1793292760849 [512 / 60000]\n",
            "Loss: 1.1659624576568604 [768 / 60000]\n",
            "Loss: 1.1583333015441895 [1024 / 60000]\n",
            "Loss: 1.165820837020874 [1280 / 60000]\n",
            "Loss: 1.1197172403335571 [1536 / 60000]\n",
            "Loss: 1.2147983312606812 [1792 / 60000]\n",
            "Loss: 1.2262614965438843 [2048 / 60000]\n",
            "Loss: 1.1469005346298218 [2304 / 60000]\n",
            "Loss: 1.1795192956924438 [2560 / 60000]\n",
            "Loss: 1.1774108409881592 [2816 / 60000]\n",
            "Loss: 1.1986517906188965 [3072 / 60000]\n",
            "Loss: 1.1914478540420532 [3328 / 60000]\n",
            "Loss: 1.192122459411621 [3584 / 60000]\n",
            "Loss: 1.143839716911316 [3840 / 60000]\n",
            "Loss: 1.151574730873108 [4096 / 60000]\n",
            "Loss: 1.1879693269729614 [4352 / 60000]\n",
            "Loss: 1.1633394956588745 [4608 / 60000]\n",
            "Loss: 1.170064091682434 [4864 / 60000]\n",
            "Loss: 1.1740443706512451 [5120 / 60000]\n",
            "Loss: 1.1926031112670898 [5376 / 60000]\n",
            "Loss: 1.160469651222229 [5632 / 60000]\n",
            "Loss: 1.0998786687850952 [5888 / 60000]\n",
            "Loss: 1.1580114364624023 [6144 / 60000]\n",
            "Loss: 1.1950570344924927 [6400 / 60000]\n",
            "Loss: 1.2292546033859253 [6656 / 60000]\n",
            "Loss: 1.2117060422897339 [6912 / 60000]\n",
            "Loss: 1.1679015159606934 [7168 / 60000]\n",
            "Loss: 1.1579803228378296 [7424 / 60000]\n",
            "Loss: 1.2161657810211182 [7680 / 60000]\n",
            "Loss: 1.2026584148406982 [7936 / 60000]\n",
            "Loss: 1.1039679050445557 [8192 / 60000]\n",
            "Loss: 1.116254448890686 [8448 / 60000]\n",
            "Loss: 1.1219656467437744 [8704 / 60000]\n",
            "Loss: 1.1620614528656006 [8960 / 60000]\n",
            "Loss: 1.1898902654647827 [9216 / 60000]\n",
            "Loss: 1.2207679748535156 [9472 / 60000]\n",
            "Loss: 1.16433846950531 [9728 / 60000]\n",
            "Loss: 1.1300209760665894 [9984 / 60000]\n",
            "Loss: 1.2101835012435913 [10240 / 60000]\n",
            "Loss: 1.1762380599975586 [10496 / 60000]\n",
            "Loss: 1.121476411819458 [10752 / 60000]\n",
            "Loss: 1.247644066810608 [11008 / 60000]\n",
            "Loss: 1.147720456123352 [11264 / 60000]\n",
            "Loss: 1.170154094696045 [11520 / 60000]\n",
            "Loss: 1.1714731454849243 [11776 / 60000]\n",
            "Loss: 1.151213526725769 [12032 / 60000]\n",
            "Loss: 1.1372991800308228 [12288 / 60000]\n",
            "Loss: 1.1836280822753906 [12544 / 60000]\n",
            "Loss: 1.1398898363113403 [12800 / 60000]\n",
            "Loss: 1.205802083015442 [13056 / 60000]\n",
            "Loss: 1.1401951313018799 [13312 / 60000]\n",
            "Loss: 1.1882870197296143 [13568 / 60000]\n",
            "Loss: 1.1699765920639038 [13824 / 60000]\n",
            "Loss: 1.1180750131607056 [14080 / 60000]\n",
            "Loss: 1.199412226676941 [14336 / 60000]\n",
            "Loss: 1.183060646057129 [14592 / 60000]\n",
            "Loss: 1.1612156629562378 [14848 / 60000]\n",
            "Loss: 1.137675166130066 [15104 / 60000]\n",
            "Loss: 1.240300178527832 [15360 / 60000]\n",
            "Loss: 1.1637321710586548 [15616 / 60000]\n",
            "Loss: 1.1227246522903442 [15872 / 60000]\n",
            "Loss: 1.1422150135040283 [16128 / 60000]\n",
            "Loss: 1.229270577430725 [16384 / 60000]\n",
            "Loss: 1.1850733757019043 [16640 / 60000]\n",
            "Loss: 1.1532669067382812 [16896 / 60000]\n",
            "Loss: 1.158265471458435 [17152 / 60000]\n",
            "Loss: 1.150944709777832 [17408 / 60000]\n",
            "Loss: 1.2226499319076538 [17664 / 60000]\n",
            "Loss: 1.1447819471359253 [17920 / 60000]\n",
            "Loss: 1.1872508525848389 [18176 / 60000]\n",
            "Loss: 1.1414967775344849 [18432 / 60000]\n",
            "Loss: 1.1386536359786987 [18688 / 60000]\n",
            "Loss: 1.1490459442138672 [18944 / 60000]\n",
            "Loss: 1.126552700996399 [19200 / 60000]\n",
            "Loss: 1.1899983882904053 [19456 / 60000]\n",
            "Loss: 1.171468734741211 [19712 / 60000]\n",
            "Loss: 1.1734719276428223 [19968 / 60000]\n",
            "Loss: 1.143214225769043 [20224 / 60000]\n",
            "Loss: 1.1931405067443848 [20480 / 60000]\n",
            "Loss: 1.1293303966522217 [20736 / 60000]\n",
            "Loss: 1.0653855800628662 [20992 / 60000]\n",
            "Loss: 1.1830660104751587 [21248 / 60000]\n",
            "Loss: 1.137143611907959 [21504 / 60000]\n",
            "Loss: 1.1702053546905518 [21760 / 60000]\n",
            "Loss: 1.22791588306427 [22016 / 60000]\n",
            "Loss: 1.1543136835098267 [22272 / 60000]\n",
            "Loss: 1.1636102199554443 [22528 / 60000]\n",
            "Loss: 1.185495376586914 [22784 / 60000]\n",
            "Loss: 1.1316649913787842 [23040 / 60000]\n",
            "Loss: 1.180543303489685 [23296 / 60000]\n",
            "Loss: 1.1787396669387817 [23552 / 60000]\n",
            "Loss: 1.1410075426101685 [23808 / 60000]\n",
            "Loss: 1.1650457382202148 [24064 / 60000]\n",
            "Loss: 1.1907265186309814 [24320 / 60000]\n",
            "Loss: 1.1278245449066162 [24576 / 60000]\n",
            "Loss: 1.100151538848877 [24832 / 60000]\n",
            "Loss: 1.1583865880966187 [25088 / 60000]\n",
            "Loss: 1.170534610748291 [25344 / 60000]\n",
            "Loss: 1.1785906553268433 [25600 / 60000]\n",
            "Loss: 1.0816885232925415 [25856 / 60000]\n",
            "Loss: 1.1649028062820435 [26112 / 60000]\n",
            "Loss: 1.2458136081695557 [26368 / 60000]\n",
            "Loss: 1.1740764379501343 [26624 / 60000]\n",
            "Loss: 1.1639043092727661 [26880 / 60000]\n",
            "Loss: 1.118962049484253 [27136 / 60000]\n",
            "Loss: 1.1377429962158203 [27392 / 60000]\n",
            "Loss: 1.1330405473709106 [27648 / 60000]\n",
            "Loss: 1.100889801979065 [27904 / 60000]\n",
            "Loss: 1.1164077520370483 [28160 / 60000]\n",
            "Loss: 1.1055049896240234 [28416 / 60000]\n",
            "Loss: 1.1668875217437744 [28672 / 60000]\n",
            "Loss: 1.1508938074111938 [28928 / 60000]\n",
            "Loss: 1.0933998823165894 [29184 / 60000]\n",
            "Loss: 1.1972532272338867 [29440 / 60000]\n",
            "Loss: 1.1393064260482788 [29696 / 60000]\n",
            "Loss: 1.1660584211349487 [29952 / 60000]\n",
            "Loss: 1.1343932151794434 [30208 / 60000]\n",
            "Loss: 1.135579228401184 [30464 / 60000]\n",
            "Loss: 1.1536839008331299 [30720 / 60000]\n",
            "Loss: 1.1998722553253174 [30976 / 60000]\n",
            "Loss: 1.1125617027282715 [31232 / 60000]\n",
            "Loss: 1.1258423328399658 [31488 / 60000]\n",
            "Loss: 1.118546724319458 [31744 / 60000]\n",
            "Loss: 1.1161514520645142 [32000 / 60000]\n",
            "Loss: 1.215907096862793 [32256 / 60000]\n",
            "Loss: 1.169840931892395 [32512 / 60000]\n",
            "Loss: 1.0951826572418213 [32768 / 60000]\n",
            "Loss: 1.1247750520706177 [33024 / 60000]\n",
            "Loss: 1.181990385055542 [33280 / 60000]\n",
            "Loss: 1.1354780197143555 [33536 / 60000]\n",
            "Loss: 1.160383701324463 [33792 / 60000]\n",
            "Loss: 1.1750273704528809 [34048 / 60000]\n",
            "Loss: 1.1310596466064453 [34304 / 60000]\n",
            "Loss: 1.1576241254806519 [34560 / 60000]\n",
            "Loss: 1.1090316772460938 [34816 / 60000]\n",
            "Loss: 1.1410768032073975 [35072 / 60000]\n",
            "Loss: 1.1944884061813354 [35328 / 60000]\n",
            "Loss: 1.1386404037475586 [35584 / 60000]\n",
            "Loss: 1.1894185543060303 [35840 / 60000]\n",
            "Loss: 1.1288275718688965 [36096 / 60000]\n",
            "Loss: 1.0842140913009644 [36352 / 60000]\n",
            "Loss: 1.0804976224899292 [36608 / 60000]\n",
            "Loss: 1.1078468561172485 [36864 / 60000]\n",
            "Loss: 1.121472954750061 [37120 / 60000]\n",
            "Loss: 1.1376737356185913 [37376 / 60000]\n",
            "Loss: 1.1406644582748413 [37632 / 60000]\n",
            "Loss: 1.1575547456741333 [37888 / 60000]\n",
            "Loss: 1.1916393041610718 [38144 / 60000]\n",
            "Loss: 1.1302684545516968 [38400 / 60000]\n",
            "Loss: 1.1575514078140259 [38656 / 60000]\n",
            "Loss: 1.1510510444641113 [38912 / 60000]\n",
            "Loss: 1.0805498361587524 [39168 / 60000]\n",
            "Loss: 1.1236045360565186 [39424 / 60000]\n",
            "Loss: 1.1752421855926514 [39680 / 60000]\n",
            "Loss: 1.075829267501831 [39936 / 60000]\n",
            "Loss: 1.1874302625656128 [40192 / 60000]\n",
            "Loss: 1.1310982704162598 [40448 / 60000]\n",
            "Loss: 1.1408052444458008 [40704 / 60000]\n",
            "Loss: 1.0914461612701416 [40960 / 60000]\n",
            "Loss: 1.0830113887786865 [41216 / 60000]\n",
            "Loss: 1.1085697412490845 [41472 / 60000]\n",
            "Loss: 1.1319860219955444 [41728 / 60000]\n",
            "Loss: 1.123405933380127 [41984 / 60000]\n",
            "Loss: 1.1495945453643799 [42240 / 60000]\n",
            "Loss: 1.1586155891418457 [42496 / 60000]\n",
            "Loss: 1.0755953788757324 [42752 / 60000]\n",
            "Loss: 1.1559950113296509 [43008 / 60000]\n",
            "Loss: 1.134737491607666 [43264 / 60000]\n",
            "Loss: 1.1864248514175415 [43520 / 60000]\n",
            "Loss: 1.0976932048797607 [43776 / 60000]\n",
            "Loss: 1.1603575944900513 [44032 / 60000]\n",
            "Loss: 1.159736156463623 [44288 / 60000]\n",
            "Loss: 1.1734305620193481 [44544 / 60000]\n",
            "Loss: 1.1608096361160278 [44800 / 60000]\n",
            "Loss: 1.1814844608306885 [45056 / 60000]\n",
            "Loss: 1.1765044927597046 [45312 / 60000]\n",
            "Loss: 1.0903189182281494 [45568 / 60000]\n",
            "Loss: 1.1327286958694458 [45824 / 60000]\n",
            "Loss: 1.096248984336853 [46080 / 60000]\n",
            "Loss: 1.1199318170547485 [46336 / 60000]\n",
            "Loss: 1.15034019947052 [46592 / 60000]\n",
            "Loss: 1.1255840063095093 [46848 / 60000]\n",
            "Loss: 1.1875197887420654 [47104 / 60000]\n",
            "Loss: 1.080899953842163 [47360 / 60000]\n",
            "Loss: 1.1478824615478516 [47616 / 60000]\n",
            "Loss: 1.1825543642044067 [47872 / 60000]\n",
            "Loss: 1.1409822702407837 [48128 / 60000]\n",
            "Loss: 1.1260855197906494 [48384 / 60000]\n",
            "Loss: 1.159380316734314 [48640 / 60000]\n",
            "Loss: 1.1215629577636719 [48896 / 60000]\n",
            "Loss: 1.1259117126464844 [49152 / 60000]\n",
            "Loss: 1.1551322937011719 [49408 / 60000]\n",
            "Loss: 1.1479440927505493 [49664 / 60000]\n",
            "Loss: 1.1105438470840454 [49920 / 60000]\n",
            "Loss: 1.1463959217071533 [50176 / 60000]\n",
            "Loss: 1.1711454391479492 [50432 / 60000]\n",
            "Loss: 1.1084719896316528 [50688 / 60000]\n",
            "Loss: 1.0936273336410522 [50944 / 60000]\n",
            "Loss: 1.1268986463546753 [51200 / 60000]\n",
            "Loss: 1.0823553800582886 [51456 / 60000]\n",
            "Loss: 1.060784935951233 [51712 / 60000]\n",
            "Loss: 1.1423300504684448 [51968 / 60000]\n",
            "Loss: 1.1495857238769531 [52224 / 60000]\n",
            "Loss: 1.126505970954895 [52480 / 60000]\n",
            "Loss: 1.1146949529647827 [52736 / 60000]\n",
            "Loss: 1.1008822917938232 [52992 / 60000]\n",
            "Loss: 1.0967880487442017 [53248 / 60000]\n",
            "Loss: 1.1124863624572754 [53504 / 60000]\n",
            "Loss: 1.1692204475402832 [53760 / 60000]\n",
            "Loss: 1.141607642173767 [54016 / 60000]\n",
            "Loss: 1.1032180786132812 [54272 / 60000]\n",
            "Loss: 1.1907178163528442 [54528 / 60000]\n",
            "Loss: 1.0869899988174438 [54784 / 60000]\n",
            "Loss: 1.1163644790649414 [55040 / 60000]\n",
            "Loss: 1.1317214965820312 [55296 / 60000]\n",
            "Loss: 1.0695068836212158 [55552 / 60000]\n",
            "Loss: 1.0943210124969482 [55808 / 60000]\n",
            "Loss: 1.0768728256225586 [56064 / 60000]\n",
            "Loss: 1.1144654750823975 [56320 / 60000]\n",
            "Loss: 1.1184583902359009 [56576 / 60000]\n",
            "Loss: 1.132591724395752 [56832 / 60000]\n",
            "Loss: 1.1528258323669434 [57088 / 60000]\n",
            "Loss: 1.216091513633728 [57344 / 60000]\n",
            "Loss: 1.0487399101257324 [57600 / 60000]\n",
            "Loss: 1.153095006942749 [57856 / 60000]\n",
            "Loss: 1.0772759914398193 [58112 / 60000]\n",
            "Loss: 1.1288461685180664 [58368 / 60000]\n",
            "Loss: 1.1923503875732422 [58624 / 60000]\n",
            "Loss: 1.1153960227966309 [58880 / 60000]\n",
            "Loss: 1.0916215181350708 [59136 / 60000]\n",
            "Loss: 1.1675224304199219 [59392 / 60000]\n",
            "Loss: 1.1331653594970703 [59648 / 60000]\n",
            "Loss: 1.0569008588790894 [22464 / 60000]\n",
            "Test Loss: 1.0947356462478637 Accuracy:81.13\n",
            "epoch:18=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.118461012840271 [0 / 60000]\n",
            "Loss: 1.1197088956832886 [256 / 60000]\n",
            "Loss: 1.1121259927749634 [512 / 60000]\n",
            "Loss: 1.1466079950332642 [768 / 60000]\n",
            "Loss: 1.1773490905761719 [1024 / 60000]\n",
            "Loss: 1.1634899377822876 [1280 / 60000]\n",
            "Loss: 1.1512014865875244 [1536 / 60000]\n",
            "Loss: 1.0854743719100952 [1792 / 60000]\n",
            "Loss: 1.1226294040679932 [2048 / 60000]\n",
            "Loss: 1.1393672227859497 [2304 / 60000]\n",
            "Loss: 1.111397385597229 [2560 / 60000]\n",
            "Loss: 1.1659791469573975 [2816 / 60000]\n",
            "Loss: 1.0574849843978882 [3072 / 60000]\n",
            "Loss: 1.1761189699172974 [3328 / 60000]\n",
            "Loss: 1.1296566724777222 [3584 / 60000]\n",
            "Loss: 1.135789394378662 [3840 / 60000]\n",
            "Loss: 1.1678071022033691 [4096 / 60000]\n",
            "Loss: 1.1011940240859985 [4352 / 60000]\n",
            "Loss: 1.0611473321914673 [4608 / 60000]\n",
            "Loss: 1.0540270805358887 [4864 / 60000]\n",
            "Loss: 1.1057590246200562 [5120 / 60000]\n",
            "Loss: 1.1298179626464844 [5376 / 60000]\n",
            "Loss: 1.1070945262908936 [5632 / 60000]\n",
            "Loss: 1.146546721458435 [5888 / 60000]\n",
            "Loss: 1.0732656717300415 [6144 / 60000]\n",
            "Loss: 1.0768197774887085 [6400 / 60000]\n",
            "Loss: 1.0968106985092163 [6656 / 60000]\n",
            "Loss: 1.1174086332321167 [6912 / 60000]\n",
            "Loss: 1.133487582206726 [7168 / 60000]\n",
            "Loss: 1.1125389337539673 [7424 / 60000]\n",
            "Loss: 1.0664547681808472 [7680 / 60000]\n",
            "Loss: 1.104200005531311 [7936 / 60000]\n",
            "Loss: 1.09865403175354 [8192 / 60000]\n",
            "Loss: 1.0953766107559204 [8448 / 60000]\n",
            "Loss: 1.1982616186141968 [8704 / 60000]\n",
            "Loss: 1.0846503973007202 [8960 / 60000]\n",
            "Loss: 1.0854628086090088 [9216 / 60000]\n",
            "Loss: 1.1173418760299683 [9472 / 60000]\n",
            "Loss: 1.162984013557434 [9728 / 60000]\n",
            "Loss: 1.1122068166732788 [9984 / 60000]\n",
            "Loss: 1.1080775260925293 [10240 / 60000]\n",
            "Loss: 1.1054649353027344 [10496 / 60000]\n",
            "Loss: 1.1069400310516357 [10752 / 60000]\n",
            "Loss: 1.103011965751648 [11008 / 60000]\n",
            "Loss: 1.1242672204971313 [11264 / 60000]\n",
            "Loss: 1.1053799390792847 [11520 / 60000]\n",
            "Loss: 1.0806478261947632 [11776 / 60000]\n",
            "Loss: 1.1471822261810303 [12032 / 60000]\n",
            "Loss: 1.1459805965423584 [12288 / 60000]\n",
            "Loss: 1.1841920614242554 [12544 / 60000]\n",
            "Loss: 1.1721795797348022 [12800 / 60000]\n",
            "Loss: 1.0833218097686768 [13056 / 60000]\n",
            "Loss: 1.1362836360931396 [13312 / 60000]\n",
            "Loss: 1.1302703619003296 [13568 / 60000]\n",
            "Loss: 1.1404597759246826 [13824 / 60000]\n",
            "Loss: 1.1315479278564453 [14080 / 60000]\n",
            "Loss: 1.1016100645065308 [14336 / 60000]\n",
            "Loss: 1.137569785118103 [14592 / 60000]\n",
            "Loss: 1.1315579414367676 [14848 / 60000]\n",
            "Loss: 1.0887771844863892 [15104 / 60000]\n",
            "Loss: 1.0603617429733276 [15360 / 60000]\n",
            "Loss: 1.0801641941070557 [15616 / 60000]\n",
            "Loss: 1.1136319637298584 [15872 / 60000]\n",
            "Loss: 1.0586761236190796 [16128 / 60000]\n",
            "Loss: 1.1916148662567139 [16384 / 60000]\n",
            "Loss: 1.094069242477417 [16640 / 60000]\n",
            "Loss: 1.0718907117843628 [16896 / 60000]\n",
            "Loss: 1.1154382228851318 [17152 / 60000]\n",
            "Loss: 1.106223464012146 [17408 / 60000]\n",
            "Loss: 1.1492255926132202 [17664 / 60000]\n",
            "Loss: 1.1514555215835571 [17920 / 60000]\n",
            "Loss: 1.0468724966049194 [18176 / 60000]\n",
            "Loss: 1.0750881433486938 [18432 / 60000]\n",
            "Loss: 1.0053527355194092 [18688 / 60000]\n",
            "Loss: 1.0788452625274658 [18944 / 60000]\n",
            "Loss: 1.0961503982543945 [19200 / 60000]\n",
            "Loss: 1.0918164253234863 [19456 / 60000]\n",
            "Loss: 1.109075903892517 [19712 / 60000]\n",
            "Loss: 1.0677576065063477 [19968 / 60000]\n",
            "Loss: 1.0938618183135986 [20224 / 60000]\n",
            "Loss: 1.0484892129898071 [20480 / 60000]\n",
            "Loss: 1.0806434154510498 [20736 / 60000]\n",
            "Loss: 1.165251612663269 [20992 / 60000]\n",
            "Loss: 1.0897451639175415 [21248 / 60000]\n",
            "Loss: 1.1269761323928833 [21504 / 60000]\n",
            "Loss: 1.1354645490646362 [21760 / 60000]\n",
            "Loss: 1.093189001083374 [22016 / 60000]\n",
            "Loss: 1.1641831398010254 [22272 / 60000]\n",
            "Loss: 1.0218902826309204 [22528 / 60000]\n",
            "Loss: 1.0816055536270142 [22784 / 60000]\n",
            "Loss: 1.066757321357727 [23040 / 60000]\n",
            "Loss: 1.1246880292892456 [23296 / 60000]\n",
            "Loss: 1.1273338794708252 [23552 / 60000]\n",
            "Loss: 1.0956045389175415 [23808 / 60000]\n",
            "Loss: 1.1239957809448242 [24064 / 60000]\n",
            "Loss: 1.1393284797668457 [24320 / 60000]\n",
            "Loss: 1.1266140937805176 [24576 / 60000]\n",
            "Loss: 1.080350637435913 [24832 / 60000]\n",
            "Loss: 1.09246027469635 [25088 / 60000]\n",
            "Loss: 1.0609006881713867 [25344 / 60000]\n",
            "Loss: 1.1228337287902832 [25600 / 60000]\n",
            "Loss: 1.097855806350708 [25856 / 60000]\n",
            "Loss: 1.0912054777145386 [26112 / 60000]\n",
            "Loss: 1.0946180820465088 [26368 / 60000]\n",
            "Loss: 1.1240923404693604 [26624 / 60000]\n",
            "Loss: 1.0579661130905151 [26880 / 60000]\n",
            "Loss: 1.123798131942749 [27136 / 60000]\n",
            "Loss: 1.0152547359466553 [27392 / 60000]\n",
            "Loss: 1.0748027563095093 [27648 / 60000]\n",
            "Loss: 1.102457046508789 [27904 / 60000]\n",
            "Loss: 1.0431417226791382 [28160 / 60000]\n",
            "Loss: 1.096458077430725 [28416 / 60000]\n",
            "Loss: 1.1111112833023071 [28672 / 60000]\n",
            "Loss: 1.0731054544448853 [28928 / 60000]\n",
            "Loss: 1.0868158340454102 [29184 / 60000]\n",
            "Loss: 1.1689202785491943 [29440 / 60000]\n",
            "Loss: 1.1446325778961182 [29696 / 60000]\n",
            "Loss: 1.1521000862121582 [29952 / 60000]\n",
            "Loss: 1.0530638694763184 [30208 / 60000]\n",
            "Loss: 1.1257930994033813 [30464 / 60000]\n",
            "Loss: 1.0769460201263428 [30720 / 60000]\n",
            "Loss: 1.0218232870101929 [30976 / 60000]\n",
            "Loss: 1.0892319679260254 [31232 / 60000]\n",
            "Loss: 1.1185836791992188 [31488 / 60000]\n",
            "Loss: 1.130642056465149 [31744 / 60000]\n",
            "Loss: 1.1282581090927124 [32000 / 60000]\n",
            "Loss: 1.026689052581787 [32256 / 60000]\n",
            "Loss: 1.0994528532028198 [32512 / 60000]\n",
            "Loss: 1.0566420555114746 [32768 / 60000]\n",
            "Loss: 1.0670149326324463 [33024 / 60000]\n",
            "Loss: 1.1001607179641724 [33280 / 60000]\n",
            "Loss: 1.0186082124710083 [33536 / 60000]\n",
            "Loss: 1.0365893840789795 [33792 / 60000]\n",
            "Loss: 1.1283036470413208 [34048 / 60000]\n",
            "Loss: 1.0272506475448608 [34304 / 60000]\n",
            "Loss: 1.1425044536590576 [34560 / 60000]\n",
            "Loss: 1.05621337890625 [34816 / 60000]\n",
            "Loss: 1.1344562768936157 [35072 / 60000]\n",
            "Loss: 1.1003812551498413 [35328 / 60000]\n",
            "Loss: 1.0651873350143433 [35584 / 60000]\n",
            "Loss: 1.0825613737106323 [35840 / 60000]\n",
            "Loss: 1.1330616474151611 [36096 / 60000]\n",
            "Loss: 1.0577417612075806 [36352 / 60000]\n",
            "Loss: 1.080754280090332 [36608 / 60000]\n",
            "Loss: 1.0928518772125244 [36864 / 60000]\n",
            "Loss: 1.161709189414978 [37120 / 60000]\n",
            "Loss: 1.086031436920166 [37376 / 60000]\n",
            "Loss: 1.0496633052825928 [37632 / 60000]\n",
            "Loss: 1.0683704614639282 [37888 / 60000]\n",
            "Loss: 1.0154430866241455 [38144 / 60000]\n",
            "Loss: 1.1110985279083252 [38400 / 60000]\n",
            "Loss: 1.0300871133804321 [38656 / 60000]\n",
            "Loss: 1.1265391111373901 [38912 / 60000]\n",
            "Loss: 1.0549116134643555 [39168 / 60000]\n",
            "Loss: 1.0467225313186646 [39424 / 60000]\n",
            "Loss: 1.105067253112793 [39680 / 60000]\n",
            "Loss: 1.0673972368240356 [39936 / 60000]\n",
            "Loss: 1.1310161352157593 [40192 / 60000]\n",
            "Loss: 1.0978684425354004 [40448 / 60000]\n",
            "Loss: 1.0952039957046509 [40704 / 60000]\n",
            "Loss: 1.0564377307891846 [40960 / 60000]\n",
            "Loss: 1.0665568113327026 [41216 / 60000]\n",
            "Loss: 1.1077293157577515 [41472 / 60000]\n",
            "Loss: 1.0610955953598022 [41728 / 60000]\n",
            "Loss: 1.051459550857544 [41984 / 60000]\n",
            "Loss: 1.0562673807144165 [42240 / 60000]\n",
            "Loss: 1.0365064144134521 [42496 / 60000]\n",
            "Loss: 1.1464359760284424 [42752 / 60000]\n",
            "Loss: 1.0478605031967163 [43008 / 60000]\n",
            "Loss: 1.0629655122756958 [43264 / 60000]\n",
            "Loss: 1.0888444185256958 [43520 / 60000]\n",
            "Loss: 1.1714011430740356 [43776 / 60000]\n",
            "Loss: 1.035158395767212 [44032 / 60000]\n",
            "Loss: 1.1069029569625854 [44288 / 60000]\n",
            "Loss: 1.000302791595459 [44544 / 60000]\n",
            "Loss: 1.0363070964813232 [44800 / 60000]\n",
            "Loss: 1.0777298212051392 [45056 / 60000]\n",
            "Loss: 1.112295150756836 [45312 / 60000]\n",
            "Loss: 1.0407313108444214 [45568 / 60000]\n",
            "Loss: 1.089228868484497 [45824 / 60000]\n",
            "Loss: 1.03563392162323 [46080 / 60000]\n",
            "Loss: 1.0930540561676025 [46336 / 60000]\n",
            "Loss: 1.0732100009918213 [46592 / 60000]\n",
            "Loss: 1.0602625608444214 [46848 / 60000]\n",
            "Loss: 1.0279300212860107 [47104 / 60000]\n",
            "Loss: 1.0956099033355713 [47360 / 60000]\n",
            "Loss: 1.0704708099365234 [47616 / 60000]\n",
            "Loss: 1.0979074239730835 [47872 / 60000]\n",
            "Loss: 1.0954090356826782 [48128 / 60000]\n",
            "Loss: 0.9967284202575684 [48384 / 60000]\n",
            "Loss: 1.0513161420822144 [48640 / 60000]\n",
            "Loss: 1.1318806409835815 [48896 / 60000]\n",
            "Loss: 1.0742847919464111 [49152 / 60000]\n",
            "Loss: 1.1250447034835815 [49408 / 60000]\n",
            "Loss: 1.1267681121826172 [49664 / 60000]\n",
            "Loss: 1.095911979675293 [49920 / 60000]\n",
            "Loss: 1.1176015138626099 [50176 / 60000]\n",
            "Loss: 1.0435556173324585 [50432 / 60000]\n",
            "Loss: 1.0932902097702026 [50688 / 60000]\n",
            "Loss: 1.1188697814941406 [50944 / 60000]\n",
            "Loss: 1.1339136362075806 [51200 / 60000]\n",
            "Loss: 1.091944932937622 [51456 / 60000]\n",
            "Loss: 1.1074870824813843 [51712 / 60000]\n",
            "Loss: 1.0490249395370483 [51968 / 60000]\n",
            "Loss: 1.0339548587799072 [52224 / 60000]\n",
            "Loss: 1.0492916107177734 [52480 / 60000]\n",
            "Loss: 1.1484295129776 [52736 / 60000]\n",
            "Loss: 1.0418776273727417 [52992 / 60000]\n",
            "Loss: 1.1214869022369385 [53248 / 60000]\n",
            "Loss: 1.066630482673645 [53504 / 60000]\n",
            "Loss: 1.0746426582336426 [53760 / 60000]\n",
            "Loss: 1.1115317344665527 [54016 / 60000]\n",
            "Loss: 1.0551393032073975 [54272 / 60000]\n",
            "Loss: 1.0258601903915405 [54528 / 60000]\n",
            "Loss: 1.109798550605774 [54784 / 60000]\n",
            "Loss: 1.0991696119308472 [55040 / 60000]\n",
            "Loss: 1.0389759540557861 [55296 / 60000]\n",
            "Loss: 1.0929697751998901 [55552 / 60000]\n",
            "Loss: 1.0250046253204346 [55808 / 60000]\n",
            "Loss: 1.065981149673462 [56064 / 60000]\n",
            "Loss: 1.0830007791519165 [56320 / 60000]\n",
            "Loss: 1.0432276725769043 [56576 / 60000]\n",
            "Loss: 1.1161081790924072 [56832 / 60000]\n",
            "Loss: 1.0043776035308838 [57088 / 60000]\n",
            "Loss: 1.0246968269348145 [57344 / 60000]\n",
            "Loss: 1.0637099742889404 [57600 / 60000]\n",
            "Loss: 1.008494257926941 [57856 / 60000]\n",
            "Loss: 1.0554354190826416 [58112 / 60000]\n",
            "Loss: 1.0650941133499146 [58368 / 60000]\n",
            "Loss: 1.1110641956329346 [58624 / 60000]\n",
            "Loss: 1.111763596534729 [58880 / 60000]\n",
            "Loss: 1.018486738204956 [59136 / 60000]\n",
            "Loss: 1.0411158800125122 [59392 / 60000]\n",
            "Loss: 1.0370209217071533 [59648 / 60000]\n",
            "Loss: 1.1710628271102905 [22464 / 60000]\n",
            "Test Loss: 1.041326567530632 Accuracy:81.64\n",
            "epoch:19=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 1.0991296768188477 [0 / 60000]\n",
            "Loss: 1.0709670782089233 [256 / 60000]\n",
            "Loss: 1.0815000534057617 [512 / 60000]\n",
            "Loss: 1.0352768898010254 [768 / 60000]\n",
            "Loss: 1.091303825378418 [1024 / 60000]\n",
            "Loss: 1.061111330986023 [1280 / 60000]\n",
            "Loss: 1.1032270193099976 [1536 / 60000]\n",
            "Loss: 1.069118618965149 [1792 / 60000]\n",
            "Loss: 1.0821083784103394 [2048 / 60000]\n",
            "Loss: 1.0268985033035278 [2304 / 60000]\n",
            "Loss: 1.0645661354064941 [2560 / 60000]\n",
            "Loss: 1.0409280061721802 [2816 / 60000]\n",
            "Loss: 1.0731722116470337 [3072 / 60000]\n",
            "Loss: 1.087935447692871 [3328 / 60000]\n",
            "Loss: 1.0985703468322754 [3584 / 60000]\n",
            "Loss: 1.087976098060608 [3840 / 60000]\n",
            "Loss: 1.1310317516326904 [4096 / 60000]\n",
            "Loss: 1.0336495637893677 [4352 / 60000]\n",
            "Loss: 1.0744774341583252 [4608 / 60000]\n",
            "Loss: 1.044785499572754 [4864 / 60000]\n",
            "Loss: 1.0174341201782227 [5120 / 60000]\n",
            "Loss: 1.0869481563568115 [5376 / 60000]\n",
            "Loss: 1.103116512298584 [5632 / 60000]\n",
            "Loss: 1.1148382425308228 [5888 / 60000]\n",
            "Loss: 1.080871343612671 [6144 / 60000]\n",
            "Loss: 1.0053023099899292 [6400 / 60000]\n",
            "Loss: 0.9935762286186218 [6656 / 60000]\n",
            "Loss: 1.0589460134506226 [6912 / 60000]\n",
            "Loss: 1.142469048500061 [7168 / 60000]\n",
            "Loss: 1.0963045358657837 [7424 / 60000]\n",
            "Loss: 1.0745488405227661 [7680 / 60000]\n",
            "Loss: 1.0500493049621582 [7936 / 60000]\n",
            "Loss: 1.1004921197891235 [8192 / 60000]\n",
            "Loss: 0.9765785336494446 [8448 / 60000]\n",
            "Loss: 1.047051191329956 [8704 / 60000]\n",
            "Loss: 1.0218473672866821 [8960 / 60000]\n",
            "Loss: 1.0683242082595825 [9216 / 60000]\n",
            "Loss: 1.0918883085250854 [9472 / 60000]\n",
            "Loss: 1.0472700595855713 [9728 / 60000]\n",
            "Loss: 1.0357764959335327 [9984 / 60000]\n",
            "Loss: 1.0565308332443237 [10240 / 60000]\n",
            "Loss: 1.0705235004425049 [10496 / 60000]\n",
            "Loss: 1.0340795516967773 [10752 / 60000]\n",
            "Loss: 1.1119425296783447 [11008 / 60000]\n",
            "Loss: 1.0708667039871216 [11264 / 60000]\n",
            "Loss: 1.0676243305206299 [11520 / 60000]\n",
            "Loss: 1.0343317985534668 [11776 / 60000]\n",
            "Loss: 1.040993332862854 [12032 / 60000]\n",
            "Loss: 1.0426356792449951 [12288 / 60000]\n",
            "Loss: 1.109304428100586 [12544 / 60000]\n",
            "Loss: 1.0757408142089844 [12800 / 60000]\n",
            "Loss: 1.088065266609192 [13056 / 60000]\n",
            "Loss: 1.011826515197754 [13312 / 60000]\n",
            "Loss: 1.016281008720398 [13568 / 60000]\n",
            "Loss: 1.0484206676483154 [13824 / 60000]\n",
            "Loss: 1.042000651359558 [14080 / 60000]\n",
            "Loss: 1.1394587755203247 [14336 / 60000]\n",
            "Loss: 1.0233837366104126 [14592 / 60000]\n",
            "Loss: 1.0425556898117065 [14848 / 60000]\n",
            "Loss: 1.0598984956741333 [15104 / 60000]\n",
            "Loss: 1.0487040281295776 [15360 / 60000]\n",
            "Loss: 0.9974473118782043 [15616 / 60000]\n",
            "Loss: 1.0985585451126099 [15872 / 60000]\n",
            "Loss: 1.026120901107788 [16128 / 60000]\n",
            "Loss: 0.9839515686035156 [16384 / 60000]\n",
            "Loss: 0.9747664332389832 [16640 / 60000]\n",
            "Loss: 1.0038620233535767 [16896 / 60000]\n",
            "Loss: 1.0115160942077637 [17152 / 60000]\n",
            "Loss: 1.0479813814163208 [17408 / 60000]\n",
            "Loss: 1.0844420194625854 [17664 / 60000]\n",
            "Loss: 1.0597790479660034 [17920 / 60000]\n",
            "Loss: 1.0253621339797974 [18176 / 60000]\n",
            "Loss: 1.049192190170288 [18432 / 60000]\n",
            "Loss: 1.0335729122161865 [18688 / 60000]\n",
            "Loss: 1.0762618780136108 [18944 / 60000]\n",
            "Loss: 0.9842612743377686 [19200 / 60000]\n",
            "Loss: 1.0398244857788086 [19456 / 60000]\n",
            "Loss: 1.0855802297592163 [19712 / 60000]\n",
            "Loss: 1.0600351095199585 [19968 / 60000]\n",
            "Loss: 1.0070186853408813 [20224 / 60000]\n",
            "Loss: 1.1076796054840088 [20480 / 60000]\n",
            "Loss: 1.0851995944976807 [20736 / 60000]\n",
            "Loss: 1.053763747215271 [20992 / 60000]\n",
            "Loss: 1.0723962783813477 [21248 / 60000]\n",
            "Loss: 1.0253313779830933 [21504 / 60000]\n",
            "Loss: 1.0004957914352417 [21760 / 60000]\n",
            "Loss: 1.0562347173690796 [22016 / 60000]\n",
            "Loss: 1.0614616870880127 [22272 / 60000]\n",
            "Loss: 1.0665901899337769 [22528 / 60000]\n",
            "Loss: 1.0396950244903564 [22784 / 60000]\n",
            "Loss: 1.0857611894607544 [23040 / 60000]\n",
            "Loss: 1.0421777963638306 [23296 / 60000]\n",
            "Loss: 0.9693048000335693 [23552 / 60000]\n",
            "Loss: 1.0663310289382935 [23808 / 60000]\n",
            "Loss: 1.0375244617462158 [24064 / 60000]\n",
            "Loss: 1.0376911163330078 [24320 / 60000]\n",
            "Loss: 1.0528316497802734 [24576 / 60000]\n",
            "Loss: 1.0268346071243286 [24832 / 60000]\n",
            "Loss: 1.134710431098938 [25088 / 60000]\n",
            "Loss: 1.0188872814178467 [25344 / 60000]\n",
            "Loss: 1.0348434448242188 [25600 / 60000]\n",
            "Loss: 0.9929508566856384 [25856 / 60000]\n",
            "Loss: 1.0649574995040894 [26112 / 60000]\n",
            "Loss: 1.1258972883224487 [26368 / 60000]\n",
            "Loss: 1.0728962421417236 [26624 / 60000]\n",
            "Loss: 0.9977421760559082 [26880 / 60000]\n",
            "Loss: 1.0799548625946045 [27136 / 60000]\n",
            "Loss: 1.0583964586257935 [27392 / 60000]\n",
            "Loss: 1.0592247247695923 [27648 / 60000]\n",
            "Loss: 1.101570963859558 [27904 / 60000]\n",
            "Loss: 1.00486421585083 [28160 / 60000]\n",
            "Loss: 1.047802448272705 [28416 / 60000]\n",
            "Loss: 0.9927811622619629 [28672 / 60000]\n",
            "Loss: 1.0722525119781494 [28928 / 60000]\n",
            "Loss: 1.036212682723999 [29184 / 60000]\n",
            "Loss: 1.0494093894958496 [29440 / 60000]\n",
            "Loss: 0.9951843023300171 [29696 / 60000]\n",
            "Loss: 1.0449976921081543 [29952 / 60000]\n",
            "Loss: 1.0034418106079102 [30208 / 60000]\n",
            "Loss: 1.0589040517807007 [30464 / 60000]\n",
            "Loss: 1.0660227537155151 [30720 / 60000]\n",
            "Loss: 1.0644108057022095 [30976 / 60000]\n",
            "Loss: 1.0256597995758057 [31232 / 60000]\n",
            "Loss: 1.0370622873306274 [31488 / 60000]\n",
            "Loss: 1.0935410261154175 [31744 / 60000]\n",
            "Loss: 0.9662907719612122 [32000 / 60000]\n",
            "Loss: 1.0083627700805664 [32256 / 60000]\n",
            "Loss: 1.0568790435791016 [32512 / 60000]\n",
            "Loss: 1.053220510482788 [32768 / 60000]\n",
            "Loss: 1.049338459968567 [33024 / 60000]\n",
            "Loss: 1.0555487871170044 [33280 / 60000]\n",
            "Loss: 1.0997110605239868 [33536 / 60000]\n",
            "Loss: 0.9952112436294556 [33792 / 60000]\n",
            "Loss: 1.0520427227020264 [34048 / 60000]\n",
            "Loss: 1.0505809783935547 [34304 / 60000]\n",
            "Loss: 1.0446794033050537 [34560 / 60000]\n",
            "Loss: 1.045868158340454 [34816 / 60000]\n",
            "Loss: 1.0580345392227173 [35072 / 60000]\n",
            "Loss: 0.9227153062820435 [35328 / 60000]\n",
            "Loss: 1.0660862922668457 [35584 / 60000]\n",
            "Loss: 1.027332067489624 [35840 / 60000]\n",
            "Loss: 1.0582597255706787 [36096 / 60000]\n",
            "Loss: 1.0295376777648926 [36352 / 60000]\n",
            "Loss: 1.0155121088027954 [36608 / 60000]\n",
            "Loss: 1.0859163999557495 [36864 / 60000]\n",
            "Loss: 1.1217176914215088 [37120 / 60000]\n",
            "Loss: 0.9894638657569885 [37376 / 60000]\n",
            "Loss: 1.021026611328125 [37632 / 60000]\n",
            "Loss: 1.0430563688278198 [37888 / 60000]\n",
            "Loss: 0.9924322366714478 [38144 / 60000]\n",
            "Loss: 1.0320886373519897 [38400 / 60000]\n",
            "Loss: 1.0651129484176636 [38656 / 60000]\n",
            "Loss: 1.0252100229263306 [38912 / 60000]\n",
            "Loss: 1.1076382398605347 [39168 / 60000]\n",
            "Loss: 1.0116753578186035 [39424 / 60000]\n",
            "Loss: 1.0395811796188354 [39680 / 60000]\n",
            "Loss: 1.0537725687026978 [39936 / 60000]\n",
            "Loss: 0.9852137565612793 [40192 / 60000]\n",
            "Loss: 1.0443209409713745 [40448 / 60000]\n",
            "Loss: 0.9892621040344238 [40704 / 60000]\n",
            "Loss: 0.9829807877540588 [40960 / 60000]\n",
            "Loss: 1.0212149620056152 [41216 / 60000]\n",
            "Loss: 1.1188961267471313 [41472 / 60000]\n",
            "Loss: 1.0092484951019287 [41728 / 60000]\n",
            "Loss: 1.081439733505249 [41984 / 60000]\n",
            "Loss: 1.054910659790039 [42240 / 60000]\n",
            "Loss: 1.0230250358581543 [42496 / 60000]\n",
            "Loss: 1.0001076459884644 [42752 / 60000]\n",
            "Loss: 1.0269081592559814 [43008 / 60000]\n",
            "Loss: 1.0392669439315796 [43264 / 60000]\n",
            "Loss: 0.9779101014137268 [43520 / 60000]\n",
            "Loss: 0.9879239201545715 [43776 / 60000]\n",
            "Loss: 0.9835383296012878 [44032 / 60000]\n",
            "Loss: 1.0775043964385986 [44288 / 60000]\n",
            "Loss: 1.0328277349472046 [44544 / 60000]\n",
            "Loss: 1.0440047979354858 [44800 / 60000]\n",
            "Loss: 1.0248894691467285 [45056 / 60000]\n",
            "Loss: 1.0350812673568726 [45312 / 60000]\n",
            "Loss: 1.0161453485488892 [45568 / 60000]\n",
            "Loss: 1.0008121728897095 [45824 / 60000]\n",
            "Loss: 0.9933673739433289 [46080 / 60000]\n",
            "Loss: 1.0660271644592285 [46336 / 60000]\n",
            "Loss: 0.9927772283554077 [46592 / 60000]\n",
            "Loss: 1.0442129373550415 [46848 / 60000]\n",
            "Loss: 1.0379927158355713 [47104 / 60000]\n",
            "Loss: 1.0449291467666626 [47360 / 60000]\n",
            "Loss: 1.0359866619110107 [47616 / 60000]\n",
            "Loss: 1.0931938886642456 [47872 / 60000]\n",
            "Loss: 1.0485188961029053 [48128 / 60000]\n",
            "Loss: 1.0550963878631592 [48384 / 60000]\n",
            "Loss: 1.0747196674346924 [48640 / 60000]\n",
            "Loss: 1.0310899019241333 [48896 / 60000]\n",
            "Loss: 1.0059690475463867 [49152 / 60000]\n",
            "Loss: 1.0115265846252441 [49408 / 60000]\n",
            "Loss: 1.0554300546646118 [49664 / 60000]\n",
            "Loss: 0.9734660983085632 [49920 / 60000]\n",
            "Loss: 1.033590316772461 [50176 / 60000]\n",
            "Loss: 0.9916308522224426 [50432 / 60000]\n",
            "Loss: 1.0026216506958008 [50688 / 60000]\n",
            "Loss: 0.9764896631240845 [50944 / 60000]\n",
            "Loss: 1.001028060913086 [51200 / 60000]\n",
            "Loss: 1.076557993888855 [51456 / 60000]\n",
            "Loss: 1.0636802911758423 [51712 / 60000]\n",
            "Loss: 1.0483392477035522 [51968 / 60000]\n",
            "Loss: 1.0607165098190308 [52224 / 60000]\n",
            "Loss: 0.96037358045578 [52480 / 60000]\n",
            "Loss: 0.9900214672088623 [52736 / 60000]\n",
            "Loss: 1.001579999923706 [52992 / 60000]\n",
            "Loss: 1.0219902992248535 [53248 / 60000]\n",
            "Loss: 0.9856753349304199 [53504 / 60000]\n",
            "Loss: 0.9853633046150208 [53760 / 60000]\n",
            "Loss: 1.020874261856079 [54016 / 60000]\n",
            "Loss: 0.9466406106948853 [54272 / 60000]\n",
            "Loss: 1.0073167085647583 [54528 / 60000]\n",
            "Loss: 1.0139178037643433 [54784 / 60000]\n",
            "Loss: 1.026348352432251 [55040 / 60000]\n",
            "Loss: 0.9881845116615295 [55296 / 60000]\n",
            "Loss: 1.0964970588684082 [55552 / 60000]\n",
            "Loss: 1.0621119737625122 [55808 / 60000]\n",
            "Loss: 0.9832503199577332 [56064 / 60000]\n",
            "Loss: 1.0922271013259888 [56320 / 60000]\n",
            "Loss: 1.0267348289489746 [56576 / 60000]\n",
            "Loss: 0.980301558971405 [56832 / 60000]\n",
            "Loss: 1.0409187078475952 [57088 / 60000]\n",
            "Loss: 1.0217311382293701 [57344 / 60000]\n",
            "Loss: 1.0187342166900635 [57600 / 60000]\n",
            "Loss: 1.0051491260528564 [57856 / 60000]\n",
            "Loss: 1.037984013557434 [58112 / 60000]\n",
            "Loss: 1.0209163427352905 [58368 / 60000]\n",
            "Loss: 1.0357192754745483 [58624 / 60000]\n",
            "Loss: 1.036782145500183 [58880 / 60000]\n",
            "Loss: 1.0998170375823975 [59136 / 60000]\n",
            "Loss: 1.0073174238204956 [59392 / 60000]\n",
            "Loss: 1.0252728462219238 [59648 / 60000]\n",
            "Loss: 1.1242576837539673 [22464 / 60000]\n",
            "Test Loss: 0.992861358821392 Accuracy:82.16\n",
            "epoch:20=-=-=-=-=-=--=-==--=-=-=\n",
            "Loss: 0.9845198392868042 [0 / 60000]\n",
            "Loss: 1.0299978256225586 [256 / 60000]\n",
            "Loss: 1.02540123462677 [512 / 60000]\n",
            "Loss: 1.0190706253051758 [768 / 60000]\n",
            "Loss: 0.9982242584228516 [1024 / 60000]\n",
            "Loss: 1.0434483289718628 [1280 / 60000]\n",
            "Loss: 1.0259451866149902 [1536 / 60000]\n",
            "Loss: 1.0663139820098877 [1792 / 60000]\n",
            "Loss: 0.9737677574157715 [2048 / 60000]\n",
            "Loss: 1.0274722576141357 [2304 / 60000]\n",
            "Loss: 1.0468579530715942 [2560 / 60000]\n",
            "Loss: 1.0301154851913452 [2816 / 60000]\n",
            "Loss: 1.0174411535263062 [3072 / 60000]\n",
            "Loss: 1.0018529891967773 [3328 / 60000]\n",
            "Loss: 0.9383624792098999 [3584 / 60000]\n",
            "Loss: 1.0173983573913574 [3840 / 60000]\n",
            "Loss: 0.9627037048339844 [4096 / 60000]\n",
            "Loss: 1.0229151248931885 [4352 / 60000]\n",
            "Loss: 0.9716131687164307 [4608 / 60000]\n",
            "Loss: 0.9448149800300598 [4864 / 60000]\n",
            "Loss: 0.984394907951355 [5120 / 60000]\n",
            "Loss: 1.0468279123306274 [5376 / 60000]\n",
            "Loss: 0.9620513916015625 [5632 / 60000]\n",
            "Loss: 1.0696008205413818 [5888 / 60000]\n",
            "Loss: 1.0481927394866943 [6144 / 60000]\n",
            "Loss: 1.0064560174942017 [6400 / 60000]\n",
            "Loss: 0.9444920420646667 [6656 / 60000]\n",
            "Loss: 0.9440368413925171 [6912 / 60000]\n",
            "Loss: 1.062099575996399 [7168 / 60000]\n",
            "Loss: 1.0213074684143066 [7424 / 60000]\n",
            "Loss: 1.0148905515670776 [7680 / 60000]\n",
            "Loss: 1.0147120952606201 [7936 / 60000]\n",
            "Loss: 1.1054463386535645 [8192 / 60000]\n",
            "Loss: 1.0446596145629883 [8448 / 60000]\n",
            "Loss: 1.0716700553894043 [8704 / 60000]\n",
            "Loss: 1.0705230236053467 [8960 / 60000]\n",
            "Loss: 1.0223147869110107 [9216 / 60000]\n",
            "Loss: 0.9976902008056641 [9472 / 60000]\n",
            "Loss: 1.0478750467300415 [9728 / 60000]\n",
            "Loss: 1.0116479396820068 [9984 / 60000]\n",
            "Loss: 0.9981642365455627 [10240 / 60000]\n",
            "Loss: 0.9973183274269104 [10496 / 60000]\n",
            "Loss: 1.0680263042449951 [10752 / 60000]\n",
            "Loss: 0.9396271109580994 [11008 / 60000]\n",
            "Loss: 1.0408238172531128 [11264 / 60000]\n",
            "Loss: 1.0161008834838867 [11520 / 60000]\n",
            "Loss: 1.0147181749343872 [11776 / 60000]\n",
            "Loss: 1.0241785049438477 [12032 / 60000]\n",
            "Loss: 0.9800374507904053 [12288 / 60000]\n",
            "Loss: 0.9755306839942932 [12544 / 60000]\n",
            "Loss: 1.0321201086044312 [12800 / 60000]\n",
            "Loss: 1.0574380159378052 [13056 / 60000]\n",
            "Loss: 0.9525719881057739 [13312 / 60000]\n",
            "Loss: 1.0182965993881226 [13568 / 60000]\n",
            "Loss: 1.015270709991455 [13824 / 60000]\n",
            "Loss: 0.9927390813827515 [14080 / 60000]\n",
            "Loss: 1.006047248840332 [14336 / 60000]\n",
            "Loss: 0.9883030652999878 [14592 / 60000]\n",
            "Loss: 0.9677540063858032 [14848 / 60000]\n",
            "Loss: 1.0048747062683105 [15104 / 60000]\n",
            "Loss: 0.9914787411689758 [15360 / 60000]\n",
            "Loss: 1.0459519624710083 [15616 / 60000]\n",
            "Loss: 0.9730374217033386 [15872 / 60000]\n",
            "Loss: 1.048893690109253 [16128 / 60000]\n",
            "Loss: 1.0129996538162231 [16384 / 60000]\n",
            "Loss: 0.9553825855255127 [16640 / 60000]\n",
            "Loss: 0.9518956542015076 [16896 / 60000]\n",
            "Loss: 0.9445732831954956 [17152 / 60000]\n",
            "Loss: 1.0386407375335693 [17408 / 60000]\n",
            "Loss: 1.0118521451950073 [17664 / 60000]\n",
            "Loss: 1.0711703300476074 [17920 / 60000]\n",
            "Loss: 1.0281453132629395 [18176 / 60000]\n",
            "Loss: 1.0197341442108154 [18432 / 60000]\n",
            "Loss: 0.9927152395248413 [18688 / 60000]\n",
            "Loss: 1.0025348663330078 [18944 / 60000]\n",
            "Loss: 0.9708632826805115 [19200 / 60000]\n",
            "Loss: 0.939728319644928 [19456 / 60000]\n",
            "Loss: 1.0301411151885986 [19712 / 60000]\n",
            "Loss: 1.026872158050537 [19968 / 60000]\n",
            "Loss: 1.0082405805587769 [20224 / 60000]\n",
            "Loss: 1.056204915046692 [20480 / 60000]\n",
            "Loss: 1.057698369026184 [20736 / 60000]\n",
            "Loss: 1.0183876752853394 [20992 / 60000]\n",
            "Loss: 1.0002565383911133 [21248 / 60000]\n",
            "Loss: 1.01438307762146 [21504 / 60000]\n",
            "Loss: 1.0354784727096558 [21760 / 60000]\n",
            "Loss: 0.9582933783531189 [22016 / 60000]\n",
            "Loss: 0.9929341673851013 [22272 / 60000]\n",
            "Loss: 1.0673813819885254 [22528 / 60000]\n",
            "Loss: 0.972202718257904 [22784 / 60000]\n",
            "Loss: 0.9745272397994995 [23040 / 60000]\n",
            "Loss: 0.9407106041908264 [23296 / 60000]\n",
            "Loss: 0.9947604537010193 [23552 / 60000]\n",
            "Loss: 1.0315918922424316 [23808 / 60000]\n",
            "Loss: 1.022949457168579 [24064 / 60000]\n",
            "Loss: 0.9643019437789917 [24320 / 60000]\n",
            "Loss: 1.0609499216079712 [24576 / 60000]\n",
            "Loss: 1.0106216669082642 [24832 / 60000]\n",
            "Loss: 0.9866039752960205 [25088 / 60000]\n",
            "Loss: 1.009157419204712 [25344 / 60000]\n",
            "Loss: 0.9953187704086304 [25600 / 60000]\n",
            "Loss: 0.9748128652572632 [25856 / 60000]\n",
            "Loss: 0.9858342409133911 [26112 / 60000]\n",
            "Loss: 1.0210366249084473 [26368 / 60000]\n",
            "Loss: 0.9914875626564026 [26624 / 60000]\n",
            "Loss: 1.0263434648513794 [26880 / 60000]\n",
            "Loss: 0.9592495560646057 [27136 / 60000]\n",
            "Loss: 0.9580511450767517 [27392 / 60000]\n",
            "Loss: 1.0112308263778687 [27648 / 60000]\n",
            "Loss: 1.003633737564087 [27904 / 60000]\n",
            "Loss: 1.0284898281097412 [28160 / 60000]\n",
            "Loss: 0.945464551448822 [28416 / 60000]\n",
            "Loss: 1.012635350227356 [28672 / 60000]\n",
            "Loss: 1.0190919637680054 [28928 / 60000]\n",
            "Loss: 1.062753677368164 [29184 / 60000]\n",
            "Loss: 0.9629053473472595 [29440 / 60000]\n",
            "Loss: 1.039951205253601 [29696 / 60000]\n",
            "Loss: 0.9479756355285645 [29952 / 60000]\n",
            "Loss: 0.9836887717247009 [30208 / 60000]\n",
            "Loss: 1.0431488752365112 [30464 / 60000]\n",
            "Loss: 0.9504561424255371 [30720 / 60000]\n",
            "Loss: 1.0110440254211426 [30976 / 60000]\n",
            "Loss: 1.0164566040039062 [31232 / 60000]\n",
            "Loss: 1.032691240310669 [31488 / 60000]\n",
            "Loss: 0.9880704283714294 [31744 / 60000]\n",
            "Loss: 1.0102981328964233 [32000 / 60000]\n",
            "Loss: 0.9810039401054382 [32256 / 60000]\n",
            "Loss: 0.9792737364768982 [32512 / 60000]\n",
            "Loss: 0.964540958404541 [32768 / 60000]\n",
            "Loss: 0.9533884525299072 [33024 / 60000]\n",
            "Loss: 0.9451990723609924 [33280 / 60000]\n",
            "Loss: 1.0116873979568481 [33536 / 60000]\n",
            "Loss: 1.0345616340637207 [33792 / 60000]\n",
            "Loss: 1.047149896621704 [34048 / 60000]\n",
            "Loss: 0.9604056477546692 [34304 / 60000]\n",
            "Loss: 0.981287956237793 [34560 / 60000]\n",
            "Loss: 1.0226149559020996 [34816 / 60000]\n",
            "Loss: 0.9923006892204285 [35072 / 60000]\n",
            "Loss: 1.006556510925293 [35328 / 60000]\n",
            "Loss: 0.9972503185272217 [35584 / 60000]\n",
            "Loss: 0.967896044254303 [35840 / 60000]\n",
            "Loss: 0.9842773675918579 [36096 / 60000]\n",
            "Loss: 0.9478105306625366 [36352 / 60000]\n",
            "Loss: 0.9982145428657532 [36608 / 60000]\n",
            "Loss: 1.003812551498413 [36864 / 60000]\n",
            "Loss: 1.0063436031341553 [37120 / 60000]\n",
            "Loss: 0.9994058012962341 [37376 / 60000]\n",
            "Loss: 1.0544778108596802 [37632 / 60000]\n",
            "Loss: 1.0802736282348633 [37888 / 60000]\n",
            "Loss: 0.9555615782737732 [38144 / 60000]\n",
            "Loss: 1.0038973093032837 [38400 / 60000]\n",
            "Loss: 0.9904642105102539 [38656 / 60000]\n",
            "Loss: 0.9760157465934753 [38912 / 60000]\n",
            "Loss: 1.001328468322754 [39168 / 60000]\n",
            "Loss: 1.0341601371765137 [39424 / 60000]\n",
            "Loss: 1.0282258987426758 [39680 / 60000]\n",
            "Loss: 0.9903020858764648 [39936 / 60000]\n",
            "Loss: 0.9756221771240234 [40192 / 60000]\n",
            "Loss: 0.9279536008834839 [40448 / 60000]\n",
            "Loss: 0.9812420010566711 [40704 / 60000]\n",
            "Loss: 0.9689985513687134 [40960 / 60000]\n",
            "Loss: 0.987792432308197 [41216 / 60000]\n",
            "Loss: 0.9709240794181824 [41472 / 60000]\n",
            "Loss: 0.9679988026618958 [41728 / 60000]\n",
            "Loss: 0.9730226993560791 [41984 / 60000]\n",
            "Loss: 0.9502852559089661 [42240 / 60000]\n",
            "Loss: 0.9926242828369141 [42496 / 60000]\n",
            "Loss: 0.9991189241409302 [42752 / 60000]\n",
            "Loss: 0.9631139039993286 [43008 / 60000]\n",
            "Loss: 0.9748698472976685 [43264 / 60000]\n",
            "Loss: 0.9537802934646606 [43520 / 60000]\n",
            "Loss: 1.0084757804870605 [43776 / 60000]\n",
            "Loss: 1.0358879566192627 [44032 / 60000]\n",
            "Loss: 0.9745311141014099 [44288 / 60000]\n",
            "Loss: 0.9964720606803894 [44544 / 60000]\n",
            "Loss: 0.966215968132019 [44800 / 60000]\n",
            "Loss: 0.966428816318512 [45056 / 60000]\n",
            "Loss: 0.9744105935096741 [45312 / 60000]\n",
            "Loss: 0.920609712600708 [45568 / 60000]\n",
            "Loss: 0.9935502409934998 [45824 / 60000]\n",
            "Loss: 0.9283545017242432 [46080 / 60000]\n",
            "Loss: 0.9584198594093323 [46336 / 60000]\n",
            "Loss: 1.0089941024780273 [46592 / 60000]\n",
            "Loss: 0.9746901392936707 [46848 / 60000]\n",
            "Loss: 0.9068723320960999 [47104 / 60000]\n",
            "Loss: 0.9748797416687012 [47360 / 60000]\n",
            "Loss: 0.9659312963485718 [47616 / 60000]\n",
            "Loss: 1.0577319860458374 [47872 / 60000]\n",
            "Loss: 1.0474398136138916 [48128 / 60000]\n",
            "Loss: 0.9033688902854919 [48384 / 60000]\n",
            "Loss: 1.0084903240203857 [48640 / 60000]\n",
            "Loss: 1.020522117614746 [48896 / 60000]\n",
            "Loss: 1.003432273864746 [49152 / 60000]\n",
            "Loss: 0.9863545894622803 [49408 / 60000]\n",
            "Loss: 0.997588574886322 [49664 / 60000]\n",
            "Loss: 1.0177737474441528 [49920 / 60000]\n",
            "Loss: 0.9769783616065979 [50176 / 60000]\n",
            "Loss: 1.04792058467865 [50432 / 60000]\n",
            "Loss: 0.992657482624054 [50688 / 60000]\n",
            "Loss: 1.0140959024429321 [50944 / 60000]\n",
            "Loss: 0.9572409987449646 [51200 / 60000]\n",
            "Loss: 0.991136372089386 [51456 / 60000]\n",
            "Loss: 0.9055296778678894 [51712 / 60000]\n",
            "Loss: 1.0486853122711182 [51968 / 60000]\n",
            "Loss: 0.992145836353302 [52224 / 60000]\n",
            "Loss: 0.9975029230117798 [52480 / 60000]\n",
            "Loss: 0.991249144077301 [52736 / 60000]\n",
            "Loss: 0.9506149888038635 [52992 / 60000]\n",
            "Loss: 1.0011855363845825 [53248 / 60000]\n",
            "Loss: 0.9604433178901672 [53504 / 60000]\n",
            "Loss: 1.0378113985061646 [53760 / 60000]\n",
            "Loss: 1.0026882886886597 [54016 / 60000]\n",
            "Loss: 0.9794758558273315 [54272 / 60000]\n",
            "Loss: 0.922334611415863 [54528 / 60000]\n",
            "Loss: 1.0384550094604492 [54784 / 60000]\n",
            "Loss: 1.0135232210159302 [55040 / 60000]\n",
            "Loss: 0.9795183539390564 [55296 / 60000]\n",
            "Loss: 0.9867453575134277 [55552 / 60000]\n",
            "Loss: 0.8851025104522705 [55808 / 60000]\n",
            "Loss: 0.9517824649810791 [56064 / 60000]\n",
            "Loss: 0.940129816532135 [56320 / 60000]\n",
            "Loss: 0.9998831748962402 [56576 / 60000]\n",
            "Loss: 0.9477270245552063 [56832 / 60000]\n",
            "Loss: 0.9523608088493347 [57088 / 60000]\n",
            "Loss: 0.9822067618370056 [57344 / 60000]\n",
            "Loss: 0.9210801720619202 [57600 / 60000]\n",
            "Loss: 0.9971891045570374 [57856 / 60000]\n",
            "Loss: 0.9973435401916504 [58112 / 60000]\n",
            "Loss: 0.9660775661468506 [58368 / 60000]\n",
            "Loss: 0.9982210993766785 [58624 / 60000]\n",
            "Loss: 0.9908542037010193 [58880 / 60000]\n",
            "Loss: 0.9320493936538696 [59136 / 60000]\n",
            "Loss: 0.9626802802085876 [59392 / 60000]\n",
            "Loss: 1.0133657455444336 [59648 / 60000]\n",
            "Loss: 0.947711706161499 [22464 / 60000]\n",
            "Test Loss: 0.9490706980228424 Accuracy:82.54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#`Autoencoder`"
      ],
      "metadata": {
        "id": "s1dRITqUWupl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAZEAAAB+CAIAAABqJrvjAAAgAElEQVR4nO2de1hTV7r/15ZCyAYTMOEmF7lLGx8buc3IqaCtpc1pQX0Yb/Ac6viAHbHTGaU6DhWnhR5tq1J7TsXTYq2Hc1S0ooV2RHicp4aZwUFAYUqUAgHBhIsEQ7jsJETYvz/en/tsw0WCAYKszx88Ye+111r79t1rvetd7yJomkYYDAYzS5g30xXAYDAYE8CahcFgZhNYszAYzGwCaxYGg5lNYM3CYDCzCaxZGAxmNoE1C4PBzCawZmEwmNkE1iwMBjObwJqFwWBmE1izMBjMbAJrFgaDmU08g5qFZ33PUvCNw0yE52a6AmaGee5v3bp15cqVma2MZfL6668vW7aMpmmCIGa6Lv8Hc+Owco2PRd21GeFZ0yz06KZevHixpKQkOjp6pqtjQdjY2Fy9epWiqGXLls10XR4DdIogCIqitm/fXlZWNtM1sjh8fX2zs7P9/PxmuiIzzzOoWYCNjU1ycnJSUtJMV8SyeOGFF8rLy2e6FmNiMBhyc3Nra2ttbW1nui6WxcaNG9VqNULI0hrI08+zplkEQTA3tbe3Fzbi28ygVqs5HM5M1+IJ+Pj4kCQ507WwIGiatrKygt/4SX7WNAuNdlPxbbZwmBs0b948hJDBYJjR6lgcWq0WIcTI1hwHjxtiMLMJ/Hg/g5qFwWCeYbBmYTCY2QTWLAwGM5vAmoXBYGYTWLMwGMxsAmsWBoOZTWDNwmAwswmsWRgMZjaBNQuDwcwmsGZhMJjZBNYsDAYzm8CahcFgZhMmxHVQKpXt7e1TVxUjrKysxGIxwlEZLAP21Fz2HYE4P8xeZheO/2MhMPEUp7O4KS3RBM06e/bs7t27p6geozIwMECSJH76LQR2bLKRG0cGR8Y3bsZhbsH03At2cVNXigmaNc1h2AQCwXQWhxkH5lkc9bk32oV1ygIZ9XszFUxDqJxnMOYfxuxApParV6+Outfa2nrU7WFhYUKhcCrrhXkCbKmaBsFiWwamrpQp0SwXFxcbGxsrKys7OzuBQGBra6vT6dra2vR6vUajYUIeTxD83bYE2tvbf/jhh4SEhJG74uLiuru7R26/fPmyRCKZ+qphxgQE68yZM21tbdNQ3Lx58/R6fWxsrEgkmrpSzK9ZPB4vNjbWx8fH2trax8dHLBbz+XyNRlNeXn7nzp0bN26UlJSYvVDMVGNraysWi6Oiokbu8vb2HlWzcEz3GQe+94WFhefOnZu2Ql999dUpzd/MmuXp6ZmcnCyRSBwdHW1tbTkcDvQOhEIhn89/8cUXSZL8xz/+YWpTCzPj6HQ6nU431q5prgzGJCx/1RKTMKdmiUSidevWxcfH+/r6MsMHSqUSIWRvby8UCoVCoUqlioqK+v77781YLmZ6wOt3YSwBc2rWunXrtmzZ4uvrixDSaDQqlUqhUMhkMp1O5+/vv2rVqvnz53t4eCxfvrysrGzU3gTGYgGj5EzXAjOFiESikJAQf39/gUBAUVRHR0dXVxdCSK1W//TTT3fv3p3pCv5/zKZZIpFILBaDYHV3d3/33XfffvttT0/PgwcP1Gp1RESEp6cn+Ig+Yy3VOQIWrGcYgUAQEhISHh4eERGxdOlSHo+n1+s7OztVKhVCSKVSXb9+/cqVKzKZbKZripAZNcvd3V0oFBIEodFocnNzCwoKSktLYZe/v39QUJCzszNBEL29vXV1dXgBu9kI7hs+k/B4vJCQkG3btonFYqFQCJ4rJEl6eXm5uLiQJDk0NLRo0SIul6vX6xsbG2e6vubTrO7u7oqKCoqilErlyZMn2ZIcEhLyxhtvODo6IoSGhoZaWlqwDX7WYWtrq9FoNBqN0fcGt5pnO1FRUZs2bQLBmjdvHkVRGo0GmtUODg4kSdrb2wcEBKxevbqmpuaZ0qyqqqqGhgZwazCSpICAAJFIxOVyEUIDAwO4lzFLSUtLO3LkiJFm9fb2NjY2+vr6wlrHCCG40RcvXpyBKmJMRCQShYWFBQYGIoT6+/vb29vr6+s7Ojr0er2TkxPz5vJ4PA8PD2h2zDjmtMH39vaObEAlJiZGRETARByVStXQ0FBdXW3GQucaMzuJb6yRE4IgjLyxrK2tKYqalkrNJLN9TqVAIOByuQMDAwMDAyqV6vbt23/729+6u7utrKzAM8nf3x8hxHyQLIGpnbsTGRm5adOmsLAwhJBWqy0rK7t69eps6RiyH0dmcjx7PjD7BzsZe5fRbyax0YFFRUXNzc1btmyBN58pxehYo7oZFTFtc8pmkJFRCqYhkMBYNZnO6cdThE6n6+joqKio0Gg0DQ0NtbW19+7d6+3t5fF4tra2QUFB0CsyGAwKhaKjo2Om64vQlMbPEggEYWFhS5cuBbfS5ubm4uLioqKiqSvRJGiapml606ZNxAhOnDjBpDE6hK0LRqLGntgFv43kyWjKOySDfysrK7Ozs5lul5H6MMcysOVy7ggWYik+nP40B1oxYmZLNwstLS1//etfv//++5KSkrKyMplMBk0KZ2fnsLCwyMhIGFibN28eshjb5VS1s3g8nkgkCgoKgvNsa2u7fv16eXm55TSy4CXncDjR0dGXLl1i7+JyuUaPo1FYqJGyNWpIKbbWjNUEY45iRuVGFm3UTDPKmZ3mGXiLxsdIl2cwmARcc61We/v2bTc3N3d392mugFno7OzUarVcLvfhw4fMJ5PH4/n6+gYFBbm5uVlbW9M0DTOF9Xr9zNYWmCrNWrt27datW0UikUAgUCqVUqk0Jyenqqpqiop7SoxsMcyLAf4pQqGwr68PGszz58836h4ihGA0zdrams/ns7WMpmnGAASNTaaBAJESwOIjFAqfe+7/bgSTAxwL2Rq9q+yK6fV6a2trHo83VkfyGQMuxcGDB+vq6jgcjl6vX7FihUQimWbVgGoYDIawsLDTp0/Hx8dPZ+lmBMzQPB4P2hM8Hs/T09Pf39/d3Z3D4cCIikajuX//fk9Pz0xXFqGp6xsGBQWJxWIwvTc2NhYWFt64cWOKylKpVDBDyFytDObld3JyiouLk8vlMG6yfPnyW7duMQ0rgiD6+vrS09MdHBycnJwcHByysrJAhgiCUKlUu3fvdnrEwYMHmfYReLFt2rQJdp04ccKo1a1Sqd555x3Y6+DgcPjwYaN+pZOTU1pamlwuF4lEkAbG6SZ+7qOmZEseu+E2CcYKUDNqWROHaWDW1NTk5ubm5OTk5uYmJyd7eHhAj55+BGJ9IUaWyD47ozMdp2ITqfOosQ8nmP9EGOcWazQa9riHSQWxO0C+vr4ikcjLywseS61Wq9Ppent7h4aGJlVlMzMl7ayNGzeKxWIej4cQamtrq6qqmtJp5dbW1k5OTpOziXI4HKVSmZ+fz94YHR09f/58hJCnp+fPP/+ckpKSk5PT3t6+f//+V199VaVSMU/D0qVL7969m5iYuGLFip6entTU1FWrVi1btgwhtHXr1u+//3737t2BgYFXr15NS0tTKBTHjh2DY1955ZWqqqqMjAw3N7dPPvlk4cKFdnZ2kKdGowkKCoqIiIAwrXK5/F/+5V8QQu+99x77vH7++ec1a9a89dZbixYtamlpaWlpMfXcx0Iulzs7O7NblOPnqVAojPxXbG1tm5ubJxKQpLKyMjQ0dHLVjomJKSwshApnZGQkJyeLxWJ2bmMZAZldI/+OVZPxrQFGpRj9QCzZGmmaNMr/iYyaEjb29/dv374drsmknwQ+nx8YGOjj4+Pg4GBlZaXVatvb2+vq6tra2izFS4meMMeOHZtIhgKB4PLlyz09PcPDwwMDAwUFBRs3bpxExVxcXAYGBiZYt8TExMTExJ6eHpqmh4eHaZo+cODAkSNHxkoPaYaHh5OTkwUCQQqL5OTkrq4uSObp6eni4tLY2Aj/ZmRkIISYvadPn0YIFRQUMNkqFArYe/PmTYQQU4Hh4eGUlBSEEJRbUVHBPnBgYAAhFBkZCfXPycnx9vaG38C1a9cQQszVGB4ehu/BtWvXjM6I/WNUcnJy9u3bN2rK4eFh2HLt2rXk5GSjjQqFwtQ7yK7eWMTExEAyKAW+9uxzH4uNGzfGxMQw16SrqyskJASqzdzcCxcugG5GR0dXVFSwTwfO6MCBA/7+/iKR6MCBA+yHp7Gxcffu3f7+/v7+/gcOHIB7yhxYW1u7ceNGFxeXjIwMuCynT59mKgbHwhU4cuQIcy5weEFBAUJIoVBcvnw5MjJSIBAcO3bsic/5wMBAeHj4zZs3n5gsOjqa/dTBj8TExInftZCQkCNHjly7dq2xsVGhUDQ2NhYUFKSmpkZHR08wdDBc6qnDzO0sgUCwevVqLy8vPp9PUVRzc3N+fv7kxgofPnwIP/r6+sZPaWVlFRcXt2bNmo6OjqysLHhMHz58OM4wB/PJ1ev1ERERI+WYpmmCIAYHB1988UU/Pz/498UXX0QIaTQaME5VV1d7e3vHxMQwhzAmlaamJoTQmjVrmKzefPPN7Ozs27dvi0Si+vp6hNDq1athL0mSqampf/3rX+HYq1evPnjw4O2332YqAw3+9vZ2Pz8/2NLb2xsdHR0VFUU/PliJnqJ7yPwbFha2Y8eO9PT0zMzMqbORQc1TU1NXrlx58+ZNaJxOOiuhUPiLX/wiOzv7q6++gjrv3r07JycnLy8vMDCwvLw8LCwMYhDC6chkMolEcu/evfDwcFtb248//hhawdCpB6ekyMhIhFBaWtqVK1fy8/Phpsvl8iVLliCEoqOjc3Jy7t27x66JTCZbsmRJampqbW0tQmjXrl03btzIycmZP38+XF6wc587dy41NTU8PFwkEu3YsWPDhg0TiTU2NDRE0/RYrlJcLpckybfeeishIaGjo2PPnj2TCBLr7e0tEolcXV3t7Owg+pBcLq+oqKioqKiurraQATQza5afn9/69es9PDzg39bW1rt37z7Nqfb19UGbYnwgTUlJyZIlS1JSUr744osnHsKWrYkkNtoCnzVwIEaPO0wRBAF5wlAgHOvk5MQcCxoEy3PAFoFAwBgL1Gq1WCzevn072zaxfft2I0v8okWLmLMw7/AZSZJbt25NTU1taGj44osvpig+MlRVLBaHhIQEBweDGdvKymrSGTKCjhC6cuXKkSNHFAoFfEX8/Px6enrS09OjoqLgsmdmZt67d+/atWsQxVCj0Xz77bdwJXNychBCzK4TJ04kJyeXlJSAlf2TTz5BCFVUVISGhlIUxV7VhabpXbt2JScnHz58GLZcunTJzs5u/fr1cXFx7Nt05syZ2tpa+LjK5fKJLNRia2u7Y8cOFxeXcdKQJAnPzKFDhy5durRz587t27eb9EgEBgauWLHi+eefd3Z21ul0CoWivr6+oaGhu7vbQgQLmVez/P39o6KioqKieDweRVHt7e0//vhja2vrJGv23HMIISsrK3iGxgeMTQihyMjIDRs2EATBHol7ekY+UgRBLFiwoKSkRKvVMh9JJo29vT1CqKenx93dHY6FsB4AzIHQaDQgQwih5uZm5nV1dXVVq9WjRgQdtWLsKj0xPUmSH3300UcffTSRzM+dO3fu3LmMjIx9+/ZNJL0RK1eunHjihISEXbt2ffPNN6auXcJcAfhCwNW+dOmSp6fnP//5z3/84x8IIWtra51OV1VVBVa2pqamc+fOHTt2DC4yTdN8Pj8pKQkhRFHUlStXNm7cyFz/pKSkffv2FRYWxsfH048616GhoQghkiR37dqVnZ0NKZuamkpKSjIyMgoLC6E9RZJkeHh4YWFhXFwc++5kZmYyxj621I6Pi4uLq6vrOAmsra2ZvoVarUYIsR/OJxITExMbGyuRSODp1el0SqXyp59+amxstBzBQubVrFWrVr3xxhsCgYAgiPb29uLi4pMnT046Thb0DUmShIdpfM6cOYMQOnbsGONKPn7f8ImftYmwevXqjz766PPPP3/nnXfAZl9ZWQmuOvBEfvPNN4cOHYLuxvHjxxFCsB1i8pw+fTolJYUgCLlcnpOTAz0RhFBsbOyvfvUruVzOfprpx43HDMQjx9QJng5FUZGRkU+0MMpkMngVU1JS4uPjJ3et9u3b5+bmNtZenU4HKnPy5MmqqioXF5esrKylS5ea9MBwuVymbnAgXJDe3l4Oh/PDDz8wPkfW1tYpKSlQItiSfXx80OPWcYSQwWAYGBiA28S0mhcvXgwS0N/fjx61cAF2IxRebJlMxu4whoaGwn1k3yNonpv0EOp0uqNHjy5btmycrjpBEGfOnMnNzU1JSfnwww8n3kDm8Xhisfj1119ftWqVo6Pj0NBQY2MjOFRWVVWpVKpndu6OWCyGaToIoerq6uzsbLME9nviraVpOisrKycnB9RtEnqk0WiY3waDgSTJiXydoqKiEhMTwd6xZMmS/v7+3Nzc2tpad3d3Pz+/3bt3Hzp0qL6+3tPTs7Ky8saNG0eOHIEDfX19k5OTd+zYUVpayuPxCgsLExMT6+rqYK9EIomJiVmzZs2pU6cCAgL6+/uLiopu3boFY45GZ2eqDQshFBkZCQMC47Bjxw6E0IULF+Li4iaesxGrV69+YmtRqVT+8Y9/jIyMBIPRE22XRoA/JOiUTCYLDw+H7RwOJyQkZKxRI1CuUadDzps3z87ODiapMJ397u5u6GNCWxi0iX7kn8Uca2NjgxBKT08fOWA66jM5uS/BqGYK9MjjLy8vb9++fRkZGRMZ8GXg8/mrV69evnw5fGNaW1v/8pe/5OXlWaZDpTk1y8bGhrHR6PX6iUcIi46OBqsn0N3dfe7cuYnH2GprawsODk5KSqLHHks2gkmj1+tLSkocHBzYezMyMtLT0xFCnZ2dbD86qBJ8paGsU6dOrVix4pNPPiktLfX09Lxw4cILL7wAuz744ANXV9fU1FSEEI/HY7sdEgRx9OhRHo8HKnb58uXKysri4mLYS5JkXl7eqVOnmA/Axo0b4ViCNd3nKZvrI8WOecRVKlV+fj7bYm0WM9nIChAEUVRUJBaLi4qK4CMxPDxsUiZMOwv8aUCkYLQkNTU1Ly8PPa4XcDpCodDf3/8//uM/2F02pVK5cOFCe3t7b2/v7OzsL774Ap4lpVIpk8lgwSHo6xUXF+/atQsOLCsrYyrj5eWFEKqpqWGaaU/81prlwjKZqNVqjUaTmZlpahFubm6RkZH+/v5cLretra2wsDA/P98yBQtNnR98YGBgRkbG4OAgQsjGxmZwcNDGxqaxsbGjo0On0/H5fE9PT1dXV9geGhoK9xtobW2lKIoJ//DEi+7o6PjVV18xKSd4nyBZXl4ePNljpUGsfhlYUtlFEASRlJRk1HuFNGDs2LVr16jZcrncw4cPM8ZaiUQCKgmQJAmOFyOPZc7RaIupjHOUTCbLysoCwTJK5unpCfeUwcbGxmjszKQK3Lhxo7CwkGnVwry28WFq1dbWdvHiRZIkL126lJOTIxKJtmzZAmkSExNTU1Pfe++9vXv3QhdJJpPV1NRs3ryZIAg+n//hhx8mJCQsWbIkLS3N0dGxrKzsxo0bV65cIQjit7/9bW5u7sqVK1NTUw0GAzQ5Y2NjIecPPvjgX//1X1977bW0tLSqqiro8gPz58/PyclJSEhwd3ePiooiCKKvr6+kpCQoKEgkEo1qEjXpio3l0sl8V+zt7Y3G6CdShL+/f3R0tIeHh729fXd39z//+c/r169brGAhs2sW80qHhISEhIQY7b19+3Zra6vBYODz+TCth72X6TPb2tpGRkaCu8BEmNyaVJP2CXjisU/M2YzNlklnNeorBLdPLBaDcY2dQKfTHTlyZOfOnUamDS6XGxYWNulH/MCBA3w+36Rqww+Koqqqqn71q18hhDw9PVNSUtLS0phmvlAobGxs3Llz55IlS2xsbMBNNzo6mukxbd68GSGUlZUFDShvb+9Tp07B+YaGhubk5Pz7v/87uKqEhIR88803TNPp9ddfP3bs2I4dO0pKSvz9/U+cOMEeakhKShocHExNTVUoFDY2NhwOZ8GCBSdPnmQSwIAyu6k+8XOHnuk42md0JSeYua+vL6yey+58TLxW0485NaulpaWpqQlCwqPRJvf6+PhAe8rKygrmMTH09vY2NDSA41JPT09lZaVFDVXMBZjPNUxdNNrL+G2wzd7A5Nyj4d0TCoWT6yJ99tlnBw8ehN8ODg5gb2I8Tmia9vPzKygoaGpqguq5uLgY2aTj4+PfeOMNcAqFvUxNkpKS1q5d29nZiRDy8PAALWCuT0pKymuvvabT6VxcXAQCQW1tLbggwOEpKSkbNmyAY21tbd3c3Njf1Ojo6Js3b4L5fyp63JPA1dXVz88P5lpxOBwnJ6cnxvbz9PTk8Xgwc2NoaEin0+n1+vv370/PO2tOzSoqKtLr9W+88QZ7o1Ao9PLygmE18B9pb2+HUZiurq7m5ub29naEkFarVSgU8HtgYOD+/fuMidRc3X7MRDDq/DIwwmSue2GSkdgIkCSjLWzLADNYMXLslf0p5fP5TNuE7WGHEIKl7UYeaJQtTdPsQUZGiNn6yOQJvq8gjlNnKDQVDofDjvQ/MDAA06TH6vKHh4evWLEiODjY3d0d5k63tLTU1NRAHIFpqLA5Nauqqqqqqqq1tRWcDPR6Pch2UFAQ4yCuVqtv3rwJzkodHR3jrCnN9Bwt4b7OKYjRfN/ZoXLMckcYjTA1t1Etekwm7MqzHUTYksT8MMrEKL3RgWyVYVfeaPtIZTQ6xKSTnR50Oh042VtZWfn7+69bt04sFstkstra2sWLFwcFBbm6utrb2zP67uHhAatdUBQFLcqGhgZm4spUY34b/HSuso0xI2ztGKkjZp8fywxiTOKosQ4cJ0+jXaOmHPUKjJNy5K5xruHkznca6OjoaG1thRnRYLGBCPEikejll18WCoWMQiGEuFwu+JdotVq1Wt3e3t7S0lJXV9fc3DxtK5ZObWxlDAZj4bS1tZWVlZEkCctVAAKBwMgw3d/fr9fr1Wo1zEOEAFB///vf792719TUNLmx48mBNQszIWxtbdnTJzHPDFVVVRqNprOz880334yIiGBGDJjBFpqmYYisra2tvr6+vr5er9ffvXu3ra1NrVYbDIZpHi7DmoWZKHfv3pXJZCP9s2aqPhhz0djY2NfX19LSUlpaunDhwuHhYXCf5nA4arX6wYMH3d3darWaWRZ++nWKjQmaNc0Rv6ate4yZIIcOHTp06NBM1wIzJXR2dpaUlFRVVcE0bMZFa9o8GCaOCZrl6uoaEhLChNOcavBK6xjMU2LqsGx3d7fltxVM0Kz4+PgZCdSP/bMwmMnxTL44JqxhMdLtZUoxcm/BYDAYNAkb/LQ5xY30ysNgMJPAwucPmooJmvU0ky0mjcW6DmMws4XY2NiAgIDpGeEdHBx84nTFp8S0dtb0C9b0F4rBPEvQND39Zugpbdxg/yyMCUwkWPtzzz338OFDyx9+mlNMQw9p5ETLKQJrFmZCLFy4sKur64mrQzMYDAZTV6PAmJ3ptKtM21RwrFmYiTJFi4ZhphTGwDJtTS00xfYc03wd2PLJRPNgtrMTsGN9sBOwsxr/WIxFga2Ks5HxY3VMRXHTUIoJmoUQghDXRUVFUqmUHTmI0XKCIIqKik6cOGEUrIORealUevDgQeJx6NHWnoAVNNkr4mAwGIxpmoUQyszMJElSpVKdOXOGIAitVqtUKuVyuUwmo2lapVI5OTlJJBJI3NfXJ5PJ5HK5SqUiCEKlUtnZ2W3YsAEhRNM0HCiXy4lHy73cunVLqVTC3szMTG9vb3OeKwaDmf2Yplk0TXd2dkZFRUkkEljbSq1We3h4VFdXw8KNer2+qKiIWfzjnXfekcvlO3fuPH/+PEJIr9dfu3YNDuzu7l6zZo1MJnv//fflcjlFUd999x1C6MCBA9XV1aBiz5gvHAaDeXpM8ylVqVQQN5nL5TKrNCcnJzMrd5IkGRkZCW0luVzO4XBgkSVYKcDd3T0sLIxZ9zA4OBj21tfX+/n5hYSEXL9+vb+/v6qqatmyZQihcRaCxmAwE8EoUv5Ul/L0hzwx6LYJmkXTtEAggMAU0KRCCOl0Oh6Ph8ZY0Hj8oXGmGUWSpFwu37t3b3FxsYODA7N6BW5nYTBPw8iw9+bKE40YizRVtoxC6RsVMY5smWyDF4lERUVF3333HbNKJTu8Diy6C1YqX1/f7u7uysrKH374AeYNKJXKhoaGpqYmpVKp1+tBkgwGA0VRtra2HA5HLpf/93//N5Obl5dXWVmZSqUyqZIYDIaBPdj19LmNlD8jd4KnyQp4YqvQZBt8amoqSZKBgYHQH3Rzc/vd737H7L1//75IJIqMjITVwGBtd7FYzOwNCAiIiYlpbGx0dHSElZOjoqLCwsLc3d0PHjyoUChgBWNI//7775MkiYcOMZhJQ1FUX1+fuZpa7KU9jLyxTNJEI5eDUQvKysoatdqm+ZTSNE2SZFRUFPtfWNcbigE7FENRUdHg4GBpaSmIl9FeOBA8FelH68Sxy+Lz+UxZGAzGVKqrq/fu3bt+/frCwsLTp0/DMqNPCaNWx48f9/HxYVoYJvGHP/xhw4YNoaGho7anYEtBQcHOnTtHHmtyXAc0xuJuRk1E2JKUlETTdEpKCnp8OtJIn6yx6o3BYJ6GV199NSkpyd3d/fvvv4+Pjz948CAsSnjo0CE+n19UVPTDDz/Y2dm9/fbbfn5+27Ztc3Bw6OnpOXr06M8///z73/8+MjKyoaEhLy+PoqiDBw+SJGltbZ2amlpYWFhQUODo6Hjnzp3k5OT58+dv27att7dXJBJJJJKBgQGlUhkfH79p06aTJ09yudyPP/6YoihHR8ff/OY3RUVF5eXlPT09N27c2L59u0wm+/zzzx0cHMRicXx8vFQqPXnypL29vZub26hnZJoNHn5otVqKomA5XDRiAUsmMVt02Ma2GYlpg8HMTTo6OuRy+Y8//vj2229LpVKFQpGWlnb27Nlvv/02PoCwdw8AAA8BSURBVD7+6NGjxcXFNE1rtdpbt245ODh8+umn2dnZUqnU1dU1MjIyMzNzx44dKpVKr9d3dnYePXoUhshiY2PB1BMVFQVvt16vT09Ph96SVCqFQX+Korhc7pUrV/h8/h//+Ef4Ny4urrq6es2aNaGhoQihzMzM7du3C4XCt956a/PmzSdPnjxw4IC9vf2//du/jXpGJtizmGZRe3v78ePH2VuMfozaaCIIgqKoEydOIBwVC4OZFmxsbHp7e+vr62tqavz8/FQqFZfLbWxsDAsLk0gkFEXBihUEQZAk2dHRAV7cPj4+4K4EkbAWLFig1+vd3d0TEhJ+//vfw7uPEBocHGSG+BFCHA7HwcHBqAIkSRIEoVQqQcvgX6M0tbW1CCGVSvVf//VfBEGo1WpYs3ostwGTYyvL5fLi4uI333wTNhYVFeXn52dnZ1MURVFUfn7+mTNn8vPzIX1RURFBEHK5/NatWxqN5vPPPz969GhWVlZlZSVuZ2EwU83g4CCYnLZt23bmzJmgoCCtVhsVFRUcHMzj8UiSvHv3bl9fHyywunTp0itXriCE/vKXvwQEBAwNDQ0PDzNZaTQaoVD41VdfVVRUgKuAjY0NaNao73JzczNC6Ny5czRNL1++/PTp0wghmUzW19eHEOLz+QMDA5Dy9ddft7Ozi4qKAokMDAxsampivDhHYlo7CyHk6+u7fPnyixcvwsajR48KhUKRSHTq1CmE0J/+9KeoqCi9Xv/1118TBJGXl4cQUigU169f5/P5iYmJISEhv/nNb0JCQiZe7lOCG3SzAnybJohJH3tnZ2cYtY+Li2traxOJRL/+9a/T09O/+OILvV5PkmR+fv7Ro0dzc3Pt7e3d3d2TkpLS09ODg4NBQaBxFBYWZm9vz+Px6urq0tPTP/zwQz6fjxCKj4+/fft2VlYWyNDKlSvt7e2h3LCwMJIks7KyYHE5kUi0bdu29PR0uVwOabZs2VJaWpqVlUVRVEZGxo0bN9LT06urqxFCe/fuvXLlilQqXb9+/ehnRU+Y4eHh4eFhmqYbGxv37dsHG5OTk7u6uhQKRUZGxsDAQGRkJCRITU2FvTRNX7t27dixYzRNd3V1JSYmQiZTBFPJjIwMKBTD5vTp03DvpvQuTAKoT09PD0Kop6eHHq2GzBb2Lov9ba4MaZoeGBgIDw+vqKigJ4upt9vSHg82kxk3RE9aPVin082bNw89cjdld3rRdA0IDg4OajQasB1OQ3GzAg6H09HRMdO1GB32UwEzK8YZSmbvstjf5soQIcTlcm1tba2srJCJ7ub0U0Tgox93v7IcTBs3JAhCKpVevXq1tLRUIBCkpKQwTvDQPtTpdAcPHqyrq9uzZw9CKCAg4ODBg9euXVuzZg1CSCAQ2NvbZ2Zmvvnmm0a+WuaCuUPPP/98QkJCWlraVJQyq8nJyZnpKhhDPz6HY//+/SRJzmiNLIuHDx+WlpYODQ1N4lh6UvMNLXlw3wQNhpRarRYhxOVytVotSZI0y9lKo9HExsZKpVL2Rhjd/P+FEQRN01qtFrZMxRVhP/30435klnkDphn6ca+6ma7O/8FU7OLFiy0tLTNdHYuDx+OtX78eDEkTB67qmTNn7O3tmcl2Ewe6Ke7u7qYeOKWY1jdECDEfQPjBdtufN2/ekiVLjDYafTBhVNUsVR+nkuzfk5hY8AwzjhPvzMLcJiZGCObpIQhCpVI9//zzzs7OCCGKotRqtU6n4/P54F/Z1tbG/re7u1soFGo0Gmtra4PBcP78+bt377799tu+vr6W8+E3eb7hqIBtbP78+WD2NkuemKnDEp48zPSg0WhOnDhRVVWFEGpvb3/ppZdkMtnWrVv7+vra2to8PDzq6+tTU1OVSmV3d/ef/vQnhNDXX39dUVHR39/f3t4+MDBQX18PvStkGcO75lzDwgJ7HBjMXIamaT8/v5dffpnZEhcXFxsbq1AoGhsbnZ2dk5OTYcJgUVHR+vXr7ezsEEK2trboUbQ7hUIBCSzn7TZPO4uNJSgxBoNhw0SygwF9QKfTsUfVDQYD24l0VCzh7TaPZo2cxIPBYGYcmDfT0tLS3NysUql0Oh1blWxtbXNzc+Vy+aVLl5YvXy4UCn/66SeZTFZQUAAJPD09IdodY8myhLfb6oMPPjBXXpZwPpgJQo8xv514PC7SEzeO3DVyOxujUUu5XP7TTz/hxUqmjlu3bi1cuNDe3t5gMPj4+Pj5+Tk7Oy9YsMDT01Oj0XR2di5evPjll18Gl/fg4ODW1taYmJjAwECSJF1dXQUCQX19vZeXF7TULOEdx3OV5xwURR05cuS5557r7u7OyMiY+DDu+MNG9IhIHhN5visrK2/cuAHRijBmZ/y7IJfL33//fZhgN/L2occVynJcTLFmzTkYbx14TJVKZVpamr29/bJly+Lj49etW+fq6srhcHp7e8+ePfvxxx/X1dW5uLhAbCOVSpWQkPDiiy9yOJw9e/a0trZmZ2dzudxXXnlFIpHI5fLjx49zudyIiAiJRCKVSv/85z8jhDZv3rxs2TKpVArLL/36178ODQ09ceJEfX19XV1dbGxsUlLSTF+VZxP2223UIkaPt7XHcZcfeeDMYs5xQ8ys4M6dO1u2bGH+PXv27KZNmyQSyaZNm1atWoUQ2rNnD6zn1tTURFEU7I2Kitq8eTMc8sknn8Bz/Omnn/72t791dHRMSUl56aWXiouLX3nllaioKJit9ec//3nDhg2LFy9GCFEUdfz48T/84Q88Hm/nzp2fffZZYWFhYWHhmTNnjKZ2YaYItjCN2owyki1G1yxKsNBUjBtiLBwbGxudTocePYIdHR1eXl4IIYFAoFarIR4IzPgDnJycEEJubm7gpOPq6goH9vf3d3R0dHV11dfXp6WlWVlZbdmypaysDCJPIoR+97vf5eXlvfPOO/fv34dQRR0dHfX19fv374d8EELu7u6Dg4PTfg3mCsTjsLegx7t7bGfjUf8dpyE2zWDNmnNIJBIwYcDS30FBQdevX0cIwcIiFEUNDg4ODg4ODw/rdDobG5v6+nqappubm8HyxYyO29vb+/v7BwYGSiQSDw8PkiSbm5szMzP3798PXcL79+8fPnx469atP/74o1AodHR0XLp0qUQicXNzc3Z2htnaZWVlM3Yh5jajDvGP35KykHaWOccNMZYPTdPu7u5WVlZff/11X19fcHBwcHBweXn5t99+++677/r6+vb394tEIltbW0dHx0WLFpWXlyuVyvLy8l27drm5uQ0NDWm12uDgYIQQQRAvvfTS2bNnL1++bG1tvXjxYoPB8Pnnn9+5c+fdd98lSfLBgwdffvnl/fv3t27dam1tHR4enpubW1JS4uDgEBgYGBAQcPz4cZIkxWIxHjfETBxLae9hpgdTp4zt2LEDTOYj80GjGUQm+C8GM2mwZs05xhokGuldRRCETCZzcXFh1isZFSOXq1GtuSOHqCxnzi1mdoE1a44yweHtkcbaUYeWxh9HH5mYqQN7FzursYa0xs8QMxfANvi5BTN0zX7PiRGgEdLDFgijgSe2yhhla3SsTCZjj6AbZWW0bEFlZeXhw4eNKjnqb/oRZr5YGIsEa9acgyAIiDqtVCq1Wi38q1QqwU+KWYWFoiiQBpVKxU4MvxnFUSqVsK4UehStidnLlAJq0tTUtHXrVrlcrlKpCILQarVKpVKlUiGEaJq+fft2SkrK7du3NRoNQRB9fX1cLnfdunWQLUVRTGKEEGSrUqkY7UMWM6qFmWrwuOGcgyCIDRs23Lx5kyRJR0fHrq6uvXv32tnZXbhwYdWqVR999NH//u//6nS6oqKil19+maZpsVg8MDBAEISnp+ePP/544cKFrq6u8vLysLAwqVT63Xff9fb2NjQ0vPDCC0VFRadPn1ar1VKpdPny5a+99tqDBw/Ky8v7+/sXL178t7/97dq1awEBAQghNze33NxcrVb7P//zPxwOx9nZWSqV/v3vf/f29iZJ0tnZWaFQnD9/vrGxMSIigqKo/fv39/T0fPrpp+Hh4ba2tg4ODiEhIdnZ2YGBgc7OzpYTJgUzDeB21twCXmxPT88PP/wwNjbW3d39yy+/9Pf3t7a2Li0tra6utrGxSU9PT0lJaW1tVSqVBEFERETs2bMnNjaWx+MdPXp00aJFzs7On332WV9fH7SwfvGLX8TFxdE0/cEHH4jFYpIk//M//5OiKG9v73ffffe9994rLi4mCCImJmbhwoVxcXGhoaEEQaxatYqiKD6fX1paSpLk2rVrfXx81q5dKxKJIOqTRCIxGAwIoebmZq1Wm5SUlJqaev78eS6XGx0dHR8fv3Xr1pqamhm+oJhpB8/dmVswPSnGNXR4eFgikSxevHj16tUkSTIrV8I6dNCEgQAmWq3W0dFRIpHY29vX1NSQJBkfHy+VSr/88kuYjTg0NBQTEzM8PLx27VpwQDUYDNbW1hD+X6vVMoWqVKqUlJTz58+TJAmr2o26QAMT9Qmws7N7+PAhQgjWVWeDG1lzB9zOmnPQNN3f38/8+8orrxQUFOj1eqlUSlGUjY1NYWFhZWVlf38/LF4AQkPTNEmS4eHhUqnUYDBUVFQghKRSqVAofOmllyCr+Pj4kpKS/v5+2MsoFLPuiaurq0wm02g0CCGdTmcwGC5dugQyNH/+fIqimpubwZ6lUqna2tru37+v0WhcXFy6u7vlcvk333wTGRmp1WohB6OTwjb4OQK2Z805wDLFREQKCAjg8Xg1NTV8Pt/f37+0tNTFxWV4eDgpKQnaSj4+Pq6urrCi5S9/+Uu1Wn3nzh0vLy8nJyeBQHDz5k1ra+t169YRBPHLX/4Sph+6urq6ubl5e3svXLjQyspq0aJFbm5uBEGEh4dXV1cPDQ35+/uHh4dXVVVFR0cvWbIEVlgIDw+vra21sbFxdnauqKjQ6/V+fn5NTU1isTgiIqKysnLlypURERHPPfecr6+vt7e3tbW1q6vrggUL2Kc2QxcVM31g/6w5B9M9HNV0/d577yUkJMDqkyNdtMYRhfETmOQlP47LFbvyRt5hWLDmCFizMI8B61Hi9x9jsWDNwmAwswlsg8dgMLMJrFkYDGY2gTULg8HMJrBmYTCY2QTWLAwGM5v4f6XnfC3TNAOKAAAAAElFTkSuQmCC)"
      ],
      "metadata": {
        "id": "PHHHNAXk8iiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Building an linear encoder with Linear\n",
        "        # layer followed by Relu activation function\n",
        "        # 784 ==> 9\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(28 * 28, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 36),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(36, 18),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(18, 9)\n",
        "        )\n",
        "\n",
        "        # Building an linear decoder with Linear\n",
        "        # layer followed by Relu activation function\n",
        "        # The Sigmoid activation function\n",
        "        # outputs the value between 0 and 1\n",
        "        # 9 ==> 784\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(9, 18),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(18, 36),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(36, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 28 * 28),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "      x = x.view((-1,28*28))\n",
        "      encoded = self.encoder(x)\n",
        "      decoded = self.decoder(encoded)\n",
        "      decoded = decoded.view((-1,1,28,28))\n",
        "      return decoded"
      ],
      "metadata": {
        "id": "zHXUlpZhvQ8N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "auto = AE()"
      ],
      "metadata": {
        "id": "nYUFKYGT92TJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()"
      ],
      "metadata": {
        "id": "X5tfeRhw951H"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(params = auto.parameters(),lr = 1e-3)\n"
      ],
      "metadata": {
        "id": "m5Ck1w8e-LHO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(Data, Model, Loss_fn, Optimizer):\n",
        "\n",
        "  size = len(Data.dataset)\n",
        "\n",
        "  Model.train()\n",
        "\n",
        "  for batch, (x,y) in enumerate(Data):\n",
        "    print(x.shape)\n",
        "\n",
        "    y_pred = Model(x)\n",
        "    print(y_pred.shape)\n",
        "    loss = Loss_fn(y_pred, x)\n",
        "    Optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    Optimizer.step()\n",
        "\n",
        "    if size % 100 ==0 :\n",
        "      loss, current = loss.item(), batch * len(x)\n",
        "      print(f\"Loss: {loss} [{current} / {size}]\")"
      ],
      "metadata": {
        "id": "c0_1o_KiAKpT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(25):\n",
        "  print(f\"epoch: {epoch+1}=-=-=-=-=-=--=\")\n",
        "  train(train_dl, auto, loss_fn, opt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ohnx12g-xes",
        "outputId": "5bd3d48b-31c5-463a-c8a1-2e8b9cfddd4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Loss: 0.025700783357024193 [55040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02381570264697075 [55296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026922255754470825 [55552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02409624494612217 [55808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02751312032341957 [56064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02539006806910038 [56320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0267231073230505 [56576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02395472303032875 [56832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02337847463786602 [57088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0250261090695858 [57344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025756118819117546 [57600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024321740493178368 [57856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02414880506694317 [58112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02393271028995514 [58368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025665346533060074 [58624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02506791055202484 [58880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0258170235902071 [59136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024789389222860336 [59392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02533745765686035 [59648 / 60000]\n",
            "torch.Size([96, 1, 28, 28])\n",
            "torch.Size([96, 1, 28, 28])\n",
            "Loss: 0.026774650439620018 [22464 / 60000]\n",
            "epoch: 19=-=-=-=-=-=--=\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024368589743971825 [0 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02506176568567753 [256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025382954627275467 [512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02689376473426819 [768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025311104953289032 [1024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02513911761343479 [1280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025287214666604996 [1536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026307331398129463 [1792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025128165259957314 [2048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024400854483246803 [2304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024776117876172066 [2560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025624603033065796 [2816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024759525433182716 [3072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023388102650642395 [3328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025050949305295944 [3584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024919813498854637 [3840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023955877870321274 [4096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023796584457159042 [4352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0251545999199152 [4608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025328587740659714 [4864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024094877764582634 [5120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02499454841017723 [5376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026049034669995308 [5632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02419988438487053 [5888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024978438392281532 [6144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025640297681093216 [6400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024192962795495987 [6656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025340639054775238 [6912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024421222507953644 [7168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02476908639073372 [7424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025133825838565826 [7680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02614513784646988 [7936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024734849110245705 [8192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025463253259658813 [8448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026565028354525566 [8704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025592779740691185 [8960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024912897497415543 [9216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024962669238448143 [9472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025865014642477036 [9728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024893896654248238 [9984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024490172043442726 [10240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024886267259716988 [10496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023845402523875237 [10752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02648075670003891 [11008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024950949475169182 [11264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02543550916016102 [11520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02588258869946003 [11776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024335838854312897 [12032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02473319135606289 [12288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02505001798272133 [12544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02441004291176796 [12800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024390878155827522 [13056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025843048468232155 [13312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024604327976703644 [13568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025877565145492554 [13824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025367245078086853 [14080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025034651160240173 [14336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024642182514071465 [14592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024713924154639244 [14848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02426287531852722 [15104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024126531556248665 [15360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024064254015684128 [15616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02386634238064289 [15872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025852952152490616 [16128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024979354813694954 [16384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02444806694984436 [16640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023448461666703224 [16896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0242866650223732 [17152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024107741191983223 [17408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025827942416071892 [17664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024831220507621765 [17920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02423207089304924 [18176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02486453764140606 [18432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023689275607466698 [18688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025916138663887978 [18944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02408623695373535 [19200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024764133617281914 [19456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024830276146531105 [19712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024177227169275284 [19968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02564447931945324 [20224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02573489584028721 [20480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024426665157079697 [20736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024985816329717636 [20992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02410707250237465 [21248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02447269856929779 [21504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02339142933487892 [21760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025148386135697365 [22016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02525797113776207 [22272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025232618674635887 [22528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023541226983070374 [22784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02471087872982025 [23040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024329280480742455 [23296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025206247344613075 [23552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0253281369805336 [23808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025435954332351685 [24064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026205716654658318 [24320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02397848665714264 [24576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02549099177122116 [24832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0244359839707613 [25088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025132805109024048 [25344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023747827857732773 [25600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026629919186234474 [25856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024189043790102005 [26112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025039901956915855 [26368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02562520280480385 [26624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024372227489948273 [26880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02584903873503208 [27136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025640180334448814 [27392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02548365667462349 [27648 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02536681294441223 [27904 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023318743333220482 [28160 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024556078016757965 [28416 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0249774232506752 [28672 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024286018684506416 [28928 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02434271015226841 [29184 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023708049207925797 [29440 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0252198688685894 [29696 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02610926702618599 [29952 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024132022634148598 [30208 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024073531851172447 [30464 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025141095742583275 [30720 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02575424313545227 [30976 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02594202570617199 [31232 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02448650263249874 [31488 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02493605762720108 [31744 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02479909174144268 [32000 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02537064626812935 [32256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024325665086507797 [32512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025237053632736206 [32768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025005802512168884 [33024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026198554784059525 [33280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025503406301140785 [33536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02532866783440113 [33792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02516387403011322 [34048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02509233169257641 [34304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025232402607798576 [34560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024654682725667953 [34816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026438267901539803 [35072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025516288354992867 [35328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023794421926140785 [35584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025104723870754242 [35840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0248123649507761 [36096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024135636165738106 [36352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024916093796491623 [36608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02393209934234619 [36864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025684811174869537 [37120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024283647537231445 [37376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024573642760515213 [37632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02541486546397209 [37888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026097526773810387 [38144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025561565533280373 [38400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02585657685995102 [38656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02543063275516033 [38912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025035317987203598 [39168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02429071255028248 [39424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024743515998125076 [39680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02571495994925499 [39936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024590419605374336 [40192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0259803906083107 [40448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02480281889438629 [40704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026198649778962135 [40960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024670785292983055 [41216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02461794763803482 [41472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02469475194811821 [41728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02459966205060482 [41984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02398761361837387 [42240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02520895004272461 [42496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02526046708226204 [42752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026471156626939774 [43008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024827387183904648 [43264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024601271376013756 [43520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02499961107969284 [43776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02389138750731945 [44032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02401171252131462 [44288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026030581444501877 [44544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02401557005941868 [44800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024454865604639053 [45056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024484267458319664 [45312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024715417996048927 [45568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025162409991025925 [45824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025379672646522522 [46080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.027007542550563812 [46336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026006203144788742 [46592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02415432222187519 [46848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024486597627401352 [47104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02475273422896862 [47360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025022435933351517 [47616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025754302740097046 [47872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025169506669044495 [48128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025700971484184265 [48384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024759888648986816 [48640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023280994966626167 [48896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02398846670985222 [49152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024051016196608543 [49408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02434806153178215 [49664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02389334701001644 [49920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024824783205986023 [50176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.027187468484044075 [50432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02528003789484501 [50688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02464800886809826 [50944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024136299267411232 [51200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02407311648130417 [51456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02471749112010002 [51712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023614121600985527 [51968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025078369304537773 [52224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023579536005854607 [52480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02582818642258644 [52736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025851687416434288 [52992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0261272843927145 [53248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023351330310106277 [53504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024249203503131866 [53760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022804265841841698 [54016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025752123445272446 [54272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02459518238902092 [54528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02411094307899475 [54784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024214506149291992 [55040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024209104478359222 [55296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025188352912664413 [55552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02512340620160103 [55808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026558643206954002 [56064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025584984570741653 [56320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024640172719955444 [56576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024044202640652657 [56832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02414056472480297 [57088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024118874222040176 [57344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02534700185060501 [57600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023732811212539673 [57856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024537190794944763 [58112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024373633787035942 [58368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02616472914814949 [58624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026369500905275345 [58880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02450220286846161 [59136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024257048964500427 [59392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02441970258951187 [59648 / 60000]\n",
            "torch.Size([96, 1, 28, 28])\n",
            "torch.Size([96, 1, 28, 28])\n",
            "Loss: 0.02355354279279709 [22464 / 60000]\n",
            "epoch: 20=-=-=-=-=-=--=\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02385353483259678 [0 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024836856871843338 [256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025544730946421623 [512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025016799569129944 [768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026126503944396973 [1024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02531551755964756 [1280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0245033111423254 [1536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02301379293203354 [1792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024845685809850693 [2048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024419961497187614 [2304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024785948917269707 [2560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024929914623498917 [2816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024418959394097328 [3072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02448517270386219 [3328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023314006626605988 [3584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025436971336603165 [3840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023528268560767174 [4096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024715954437851906 [4352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02396586909890175 [4608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024004841223359108 [4864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025450505316257477 [5120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024206606671214104 [5376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023823736235499382 [5632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024576542899012566 [5888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024940215051174164 [6144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024787012487649918 [6400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024178586900234222 [6656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023965584114193916 [6912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025864940136671066 [7168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02542586252093315 [7424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02473454363644123 [7680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024800069630146027 [7936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025986207649111748 [8192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023928867653012276 [8448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02514241263270378 [8704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024209333583712578 [8960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025632617995142937 [9216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02357986755669117 [9472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02458607591688633 [9728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022625571116805077 [9984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02660868875682354 [10240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02572055719792843 [10496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025337742641568184 [10752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02401266247034073 [11008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024259040132164955 [11264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024942943826317787 [11520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024065662175416946 [11776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023463506251573563 [12032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02582869678735733 [12288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026137923821806908 [12544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.027163490653038025 [12800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025027571246027946 [13056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02436484582722187 [13312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024003099650144577 [13568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023919064551591873 [13824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024674393236637115 [14080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024673882871866226 [14336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024367107078433037 [14592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023985344916582108 [14848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02527475915849209 [15104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02570170722901821 [15360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02414444461464882 [15616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02474389225244522 [15872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024492621421813965 [16128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024419190362095833 [16384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02487117238342762 [16640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024481382220983505 [16896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024320125579833984 [17152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024761224165558815 [17408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023728886619210243 [17664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02416211925446987 [17920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024874409660696983 [18176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02452680841088295 [18432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0255286768078804 [18688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0256227757781744 [18944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024343429133296013 [19200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025278493762016296 [19456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02366551384329796 [19712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02530357614159584 [19968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02406899631023407 [20224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025128036737442017 [20480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025111772119998932 [20736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02302495762705803 [20992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025473522022366524 [21248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02395608462393284 [21504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024688761681318283 [21760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02474273554980755 [22016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024809082970023155 [22272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025670679286122322 [22528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023755313828587532 [22784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025805678218603134 [23040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022609861567616463 [23296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02416154555976391 [23552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025014549493789673 [23808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024059297516942024 [24064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024535521864891052 [24320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024023130536079407 [24576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023227199912071228 [24832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023867186158895493 [25088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02556988224387169 [25344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02611343003809452 [25600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02431284449994564 [25856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02351824939250946 [26112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025491463020443916 [26368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025869084522128105 [26624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023750927299261093 [26880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025115519762039185 [27136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025239059701561928 [27392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02553730085492134 [27648 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02430708520114422 [27904 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025015292689204216 [28160 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024330904707312584 [28416 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025022927671670914 [28672 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025734594091773033 [28928 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023344572633504868 [29184 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024144679307937622 [29440 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024300457909703255 [29696 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023958006873726845 [29952 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024794384837150574 [30208 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025218861177563667 [30464 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023071328178048134 [30720 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025527268648147583 [30976 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023284895345568657 [31232 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02289649471640587 [31488 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024543436244130135 [31744 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02422250248491764 [32000 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026212310418486595 [32256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02352934703230858 [32512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025433791801333427 [32768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024507610127329826 [33024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024179348722100258 [33280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02463441900908947 [33536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024334684014320374 [33792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024963442236185074 [34048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024903781712055206 [34304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023213619366288185 [34560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0229269340634346 [34816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024895088747143745 [35072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025383399799466133 [35328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0258681271225214 [35584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023848455399274826 [35840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02402551844716072 [36096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02440793812274933 [36352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024668684229254723 [36608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02395130880177021 [36864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023846834897994995 [37120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02399740181863308 [37376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02525492012500763 [37632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024540448561310768 [37888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02382870577275753 [38144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02441675402224064 [38400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024515796452760696 [38656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022382840514183044 [38912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02542700804769993 [39168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02481936477124691 [39424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02570800669491291 [39680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023484697565436363 [39936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02470366843044758 [40192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024090582504868507 [40448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02515396848320961 [40704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023898830637335777 [40960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024467580020427704 [41216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02492685429751873 [41472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024187635630369186 [41728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025231217965483665 [41984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02400773949921131 [42240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0247822143137455 [42496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024871446192264557 [42752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02294631116092205 [43008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024639947339892387 [43264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024699462577700615 [43520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02466627024114132 [43776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024309977889060974 [44032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024931158870458603 [44288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023754224181175232 [44544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02356414683163166 [44800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024361902847886086 [45056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02383577637374401 [45312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025015616789460182 [45568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024476155638694763 [45824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025084150955080986 [46080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023941297084093094 [46336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023698488250374794 [46592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02363397926092148 [46848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02344001643359661 [47104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025122953578829765 [47360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02339453436434269 [47616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02488783933222294 [47872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02529405988752842 [48128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023226791992783546 [48384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02466547302901745 [48640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023843415081501007 [48896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02508397214114666 [49152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024533692747354507 [49408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024652354419231415 [49664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024898039177060127 [49920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023948874324560165 [50176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024275222793221474 [50432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0246855728328228 [50688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02653645910322666 [50944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02493521384894848 [51200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024950236082077026 [51456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02415347658097744 [51712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024454567581415176 [51968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024337437003850937 [52224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023937376216053963 [52480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02317190170288086 [52736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02418231964111328 [52992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02499011531472206 [53248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024505898356437683 [53504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024536853656172752 [53760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022962898015975952 [54016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023242881521582603 [54272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024142010137438774 [54528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025187848135828972 [54784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024655383080244064 [55040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02402561530470848 [55296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02454114891588688 [55552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023603376001119614 [55808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025417795404791832 [56064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023616058751940727 [56320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0260316114872694 [56576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024813437834382057 [56832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02424514852464199 [57088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025353487581014633 [57344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02420821785926819 [57600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023713745176792145 [57856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023571748286485672 [58112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025280481204390526 [58368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024630896747112274 [58624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024976784363389015 [58880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023810019716620445 [59136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024800650775432587 [59392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024548467248678207 [59648 / 60000]\n",
            "torch.Size([96, 1, 28, 28])\n",
            "torch.Size([96, 1, 28, 28])\n",
            "Loss: 0.023517174646258354 [22464 / 60000]\n",
            "epoch: 21=-=-=-=-=-=--=\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02401077002286911 [0 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025339258834719658 [256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024460315704345703 [512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02319488488137722 [768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023654349148273468 [1024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024430498480796814 [1280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02307271771132946 [1536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02457374520599842 [1792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025645054876804352 [2048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025537850335240364 [2304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024655906483530998 [2560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02469451166689396 [2816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02361411415040493 [3072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02497987076640129 [3328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025006916373968124 [3584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024974975734949112 [3840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024866977706551552 [4096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024780480191111565 [4352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023447319865226746 [4608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02469477429986 [4864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023763546720147133 [5120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024584876373410225 [5376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02434021420776844 [5632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02585379034280777 [5888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0241901483386755 [6144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023843932896852493 [6400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023358330130577087 [6656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02485189586877823 [6912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023852212354540825 [7168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023644164204597473 [7424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023276804015040398 [7680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024371398612856865 [7936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024075204506516457 [8192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024075979366898537 [8448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02286314219236374 [8704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023061426356434822 [8960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02451140806078911 [9216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023707184940576553 [9472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024901708588004112 [9728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024399615824222565 [9984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02266012318432331 [10240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024335389956831932 [10496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0244671031832695 [10752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023560499772429466 [11008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023952467367053032 [11264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024350792169570923 [11520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025333229452371597 [11776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023423582315444946 [12032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02390000969171524 [12288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025974981486797333 [12544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02326035685837269 [12800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025578713044524193 [13056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024746060371398926 [13312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02483162097632885 [13568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02420956827700138 [13824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02359064668416977 [14080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02515062503516674 [14336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02432871051132679 [14592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024459054693579674 [14848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024208448827266693 [15104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024808309972286224 [15360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023613277822732925 [15616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02598661370575428 [15872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02408025413751602 [16128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02437150664627552 [16384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024942604824900627 [16640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023219211027026176 [16896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024924587458372116 [17152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02320837415754795 [17408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0250967126339674 [17664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02357669174671173 [17920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023165863007307053 [18176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024539954960346222 [18432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024099094793200493 [18688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023909714072942734 [18944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02448127791285515 [19200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024562448263168335 [19456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022777730599045753 [19712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024631602689623833 [19968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0225991141051054 [20224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02312476374208927 [20480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023266257718205452 [20736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022740285843610764 [20992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023444674909114838 [21248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022775720804929733 [21504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025026608258485794 [21760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024200940504670143 [22016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024563677608966827 [22272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022725969552993774 [22528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02373640425503254 [22784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02575780637562275 [23040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022662833333015442 [23296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024302823469042778 [23552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023988105356693268 [23808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025097934529185295 [24064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023515846580266953 [24320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024240046739578247 [24576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023757584393024445 [24832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025640001520514488 [25088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02461608499288559 [25344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024685777723789215 [25600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024208256974816322 [25856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024022353813052177 [26112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024754134938120842 [26368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025174790993332863 [26624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024523800238966942 [26880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023540928959846497 [27136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026066459715366364 [27392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022588009014725685 [27648 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025683840736746788 [27904 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024635495617985725 [28160 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023960739374160767 [28416 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024417445063591003 [28672 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022934705018997192 [28928 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023288648575544357 [29184 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025077372789382935 [29440 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024482814595103264 [29696 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023867884650826454 [29952 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02458074688911438 [30208 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024888165295124054 [30464 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024236485362052917 [30720 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022952161729335785 [30976 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024879679083824158 [31232 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024134619161486626 [31488 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024644898250699043 [31744 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024570537731051445 [32000 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02442042902112007 [32256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02516983076930046 [32512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024114206433296204 [32768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02453746274113655 [33024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025616062805056572 [33280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024926787242293358 [33536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02462255209684372 [33792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023909179493784904 [34048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024501314386725426 [34304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023559147492051125 [34560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02550254575908184 [34816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023741433396935463 [35072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024984749034047127 [35328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025071309879422188 [35584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024059806019067764 [35840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024755606427788734 [36096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024369796738028526 [36352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02388734742999077 [36608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02365986257791519 [36864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02552434429526329 [37120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023858625441789627 [37376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025740575045347214 [37632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02409731037914753 [37888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023464396595954895 [38144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02427813783288002 [38400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023918412625789642 [38656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02542118728160858 [38912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02441639080643654 [39168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023881854489445686 [39424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025123611092567444 [39680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02235562913119793 [39936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025079673156142235 [40192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02580016292631626 [40448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02382137067615986 [40704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025097448378801346 [40960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02325323596596718 [41216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025177322328090668 [41472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023999804630875587 [41728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02279050648212433 [41984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02358015440404415 [42240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023856522515416145 [42496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0224270261824131 [42752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025094956159591675 [43008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023795826360583305 [43264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02319265343248844 [43520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024230558425188065 [43776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024338342249393463 [44032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024083243682980537 [44288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022185983136296272 [44544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024265766143798828 [44800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024963155388832092 [45056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025118885561823845 [45312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023665886372327805 [45568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025477219372987747 [45824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02490192838013172 [46080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02527581714093685 [46336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02520807832479477 [46592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023621028289198875 [46848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021946055814623833 [47104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023069286718964577 [47360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02382689341902733 [47616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024570036679506302 [47872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023973481729626656 [48128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024346861988306046 [48384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02531701698899269 [48640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02336285077035427 [48896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024783333763480186 [49152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024828210473060608 [49408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025321822613477707 [49664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023271964862942696 [49920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024044713005423546 [50176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0223894901573658 [50432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025080587714910507 [50688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023668304085731506 [50944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02461567521095276 [51200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024663656949996948 [51456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02465713769197464 [51712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02334751933813095 [51968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02579394355416298 [52224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023722194135189056 [52480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02355143055319786 [52736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024692736566066742 [52992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024594036862254143 [53248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024730414152145386 [53504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025114601477980614 [53760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023339781910181046 [54016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022310540080070496 [54272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02347535826265812 [54528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02431233786046505 [54784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02432297170162201 [55040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025176851078867912 [55296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02457370236515999 [55552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02370016649365425 [55808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023428265005350113 [56064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02398025244474411 [56320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023237794637680054 [56576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02402009256184101 [56832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02350991778075695 [57088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022287871688604355 [57344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023207316175103188 [57600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025067271664738655 [57856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02416720613837242 [58112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022964103147387505 [58368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024192139506340027 [58624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024593574926257133 [58880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023711321875452995 [59136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02307470515370369 [59392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024110998958349228 [59648 / 60000]\n",
            "torch.Size([96, 1, 28, 28])\n",
            "torch.Size([96, 1, 28, 28])\n",
            "Loss: 0.02658758871257305 [22464 / 60000]\n",
            "epoch: 22=-=-=-=-=-=--=\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02334021031856537 [0 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02345660701394081 [256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024170849472284317 [512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0239175483584404 [768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023392798379063606 [1024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024218646809458733 [1280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023740321397781372 [1536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023723727092146873 [1792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02343752607703209 [2048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024442456662654877 [2304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023798659443855286 [2560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024798553436994553 [2816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023728173226118088 [3072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023033585399389267 [3328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02433994598686695 [3584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023047104477882385 [3840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02359679713845253 [4096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023370962589979172 [4352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023028921335935593 [4608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02350090630352497 [4864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02522037923336029 [5120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023739978671073914 [5376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024045273661613464 [5632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022932443767786026 [5888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02419137954711914 [6144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023039203137159348 [6400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024034084752202034 [6656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025340652093291283 [6912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024774575605988503 [7168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025496428832411766 [7424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023191722109913826 [7680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023801978677511215 [7936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025452861562371254 [8192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024692893028259277 [8448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02312946878373623 [8704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023279134184122086 [8960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022461364045739174 [9216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023925255984067917 [9472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023577656596899033 [9728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02435452863574028 [9984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024654818698763847 [10240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024567287415266037 [10496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023967402055859566 [10752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023769082501530647 [11008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023525500670075417 [11264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0248833280056715 [11520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024007828906178474 [11776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024057505652308464 [12032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023207474499940872 [12288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023404715582728386 [12544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023702483624219894 [12800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022895239293575287 [13056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024231428280472755 [13312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02393413335084915 [13568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02437276765704155 [13824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024779025465250015 [14080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02456248365342617 [14336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025361381471157074 [14592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02266531065106392 [14848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024116167798638344 [15104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02260928973555565 [15360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024166719987988472 [15616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023893926292657852 [15872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023457074537873268 [16128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023826511576771736 [16384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024261854588985443 [16640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023963747546076775 [16896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022716965526342392 [17152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02238258719444275 [17408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023905858397483826 [17664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024722600355744362 [17920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02327561378479004 [18176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023221498355269432 [18432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025237340480089188 [18688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02387019619345665 [18944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02323230914771557 [19200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024343177676200867 [19456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024419061839580536 [19712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023828905075788498 [19968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023897448554635048 [20224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023317748680710793 [20480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021889060735702515 [20736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024580055847764015 [20992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02353174425661564 [21248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02399894967675209 [21504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025220301002264023 [21760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023814750835299492 [22016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02504556067287922 [22272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024236172437667847 [22528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02306179702281952 [22784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024203157052397728 [23040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02356083318591118 [23296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024774545803666115 [23552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023388801142573357 [23808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023739410564303398 [24064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024101009592413902 [24320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024664470925927162 [24576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.026666810736060143 [24832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023447023704648018 [25088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025493094697594643 [25344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02361254021525383 [25600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025032494217157364 [25856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02538357302546501 [26112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02272791974246502 [26368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023896990343928337 [26624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02276063896715641 [26880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02447419799864292 [27136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02415131963789463 [27392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024057142436504364 [27648 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023563027381896973 [27904 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023631256073713303 [28160 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024938980117440224 [28416 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02369300276041031 [28672 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02304333820939064 [28928 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024019727483391762 [29184 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024004211649298668 [29440 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024314068257808685 [29696 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02476816065609455 [29952 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023381082341074944 [30208 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023212676867842674 [30464 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02368277497589588 [30720 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02517586015164852 [30976 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02277684025466442 [31232 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023658527061343193 [31488 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02262081205844879 [31744 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023809082806110382 [32000 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02509860135614872 [32256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024220723658800125 [32512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024875251576304436 [32768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024947389960289 [33024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024004938080906868 [33280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024108584970235825 [33536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023529961705207825 [33792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022901002317667007 [34048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023721370846033096 [34304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025740116834640503 [34560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023859413340687752 [34816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022911470383405685 [35072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02555861696600914 [35328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024540314450860023 [35584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02587278187274933 [35840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02424321137368679 [36096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02409691922366619 [36352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02429354377090931 [36608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023470645770430565 [36864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024567849934101105 [37120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02420772612094879 [37376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02558770403265953 [37632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02613607794046402 [37888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02336316928267479 [38144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024356475099921227 [38400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022135837003588676 [38656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02418629080057144 [38912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02335958555340767 [39168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02397306077182293 [39424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023570073768496513 [39680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024937881156802177 [39936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02434397302567959 [40192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023725297302007675 [40448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024334102869033813 [40704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024468660354614258 [40960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023613205179572105 [41216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025318996980786324 [41472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023946626111865044 [41728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024027956649661064 [41984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022612927481532097 [42240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02270735614001751 [42496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0239611454308033 [42752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02335752546787262 [43008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023531127721071243 [43264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023926137015223503 [43520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023513304069638252 [43776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023660622537136078 [44032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023762917146086693 [44288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025415081530809402 [44544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023148994892835617 [44800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024567533284425735 [45056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023338038474321365 [45312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022937148809432983 [45568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02360200695693493 [45824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02432378940284252 [46080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02273287996649742 [46336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02381722815334797 [46592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022696182131767273 [46848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024424925446510315 [47104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02526788040995598 [47360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024357590824365616 [47616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023699238896369934 [47872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02303268015384674 [48128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024200990796089172 [48384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024624070152640343 [48640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024230940267443657 [48896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022616665810346603 [49152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024129977449774742 [49408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02377857081592083 [49664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024542920291423798 [49920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02478400059044361 [50176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023932021111249924 [50432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02396472543478012 [50688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023180944845080376 [50944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02463616617023945 [51200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02511802315711975 [51456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024290328845381737 [51712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02379448339343071 [51968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02331659570336342 [52224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023402953520417213 [52480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023801416158676147 [52736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022436052560806274 [52992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024740876629948616 [53248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023669766262173653 [53504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024788299575448036 [53760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02338726446032524 [54016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023483578115701675 [54272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025314124301075935 [54528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02358965575695038 [54784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023522375151515007 [55040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022546475753188133 [55296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023268355056643486 [55552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023090137168765068 [55808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02332964725792408 [56064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023763099685311317 [56320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023151157423853874 [56576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024747902527451515 [56832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022985298186540604 [57088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023877104744315147 [57344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02296951413154602 [57600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023996958509087563 [57856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024389376863837242 [58112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023170271888375282 [58368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02404242567718029 [58624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023932553827762604 [58880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02436540275812149 [59136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022924477234482765 [59392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023503867909312248 [59648 / 60000]\n",
            "torch.Size([96, 1, 28, 28])\n",
            "torch.Size([96, 1, 28, 28])\n",
            "Loss: 0.022036734968423843 [22464 / 60000]\n",
            "epoch: 23=-=-=-=-=-=--=\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02405858226120472 [0 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023935848847031593 [256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023972464725375175 [512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02486739680171013 [768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022981731221079826 [1024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023772859945893288 [1280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023582378402352333 [1536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022935165092349052 [1792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02366936206817627 [2048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023691270500421524 [2304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022524366155266762 [2560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02422703057527542 [2816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02393922582268715 [3072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023054316639900208 [3328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023626597598195076 [3584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023637007921934128 [3840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023858491331338882 [4096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02390105649828911 [4352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02256663516163826 [4608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02424762211740017 [4864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02266094647347927 [5120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024025343358516693 [5376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02313184179365635 [5632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023508509621024132 [5888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02236127108335495 [6144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024580905213952065 [6400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02347490005195141 [6656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022652024403214455 [6912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023877574130892754 [7168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023678477853536606 [7424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02541080117225647 [7680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024960452690720558 [7936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024692891165614128 [8192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023158973082900047 [8448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02351047843694687 [8704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023586496710777283 [8960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024658378213644028 [9216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02308020554482937 [9472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02379775047302246 [9728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02316971682012081 [9984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02209869958460331 [10240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02281995490193367 [10496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024139652028679848 [10752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024509739130735397 [11008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02643914334475994 [11264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023109206929802895 [11520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024457097053527832 [11776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023717062547802925 [12032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02360357716679573 [12288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023357033729553223 [12544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024350866675376892 [12800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02407441847026348 [13056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023212406784296036 [13312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024059409275650978 [13568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022879011929035187 [13824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022993041202425957 [14080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022520732134580612 [14336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023383695632219315 [14592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02234520949423313 [14848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02317173220217228 [15104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022487182170152664 [15360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022821106016635895 [15616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023974943906068802 [15872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02314259111881256 [16128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025596650317311287 [16384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023481592535972595 [16640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02360204979777336 [16896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02361067198216915 [17152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022189322859048843 [17408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024801630526781082 [17664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02495073713362217 [17920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024106420576572418 [18176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024402588605880737 [18432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02399761974811554 [18688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023149609565734863 [18944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02228880487382412 [19200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02376604825258255 [19456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024270161986351013 [19712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024644896388053894 [19968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022265996783971786 [20224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023775026202201843 [20480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023180842399597168 [20736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024095823988318443 [20992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023102767765522003 [21248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024666985496878624 [21504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02338148094713688 [21760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02412799373269081 [22016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024099092930555344 [22272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02422110177576542 [22528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024285322055220604 [22784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024773890152573586 [23040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023603759706020355 [23296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023567233234643936 [23552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0224481001496315 [23808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0246885996311903 [24064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02448035031557083 [24320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02320164069533348 [24576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022992921993136406 [24832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024030130356550217 [25088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022781237959861755 [25344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02273477427661419 [25600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02429409697651863 [25856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023275358602404594 [26112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023318352177739143 [26368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02375275455415249 [26624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02388627827167511 [26880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023666098713874817 [27136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023219512775540352 [27392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024067150428891182 [27648 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024648277088999748 [27904 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022431204095482826 [28160 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02225516550242901 [28416 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024884507060050964 [28672 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023379117250442505 [28928 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023028390482068062 [29184 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02416643127799034 [29440 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023646079003810883 [29696 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02464320696890354 [29952 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0223959032446146 [30208 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02305312268435955 [30464 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022603241726756096 [30720 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02494756132364273 [30976 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024733593687415123 [31232 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02345346473157406 [31488 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02417689748108387 [31744 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02352393977344036 [32000 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024322154000401497 [32256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023524800315499306 [32512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024797670543193817 [32768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023483602330088615 [33024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02347555384039879 [33280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022661099210381508 [33536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023096276447176933 [33792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021843256428837776 [34048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023322049528360367 [34304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022839874029159546 [34560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022302931174635887 [34816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025939885526895523 [35072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022974610328674316 [35328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023461882025003433 [35584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024204691872000694 [35840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023771239444613457 [36096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02303616888821125 [36352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02489817887544632 [36608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02455526404082775 [36864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023495499044656754 [37120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02232453040778637 [37376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023854225873947144 [37632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02472727559506893 [37888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023997070267796516 [38144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022900722920894623 [38400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023296287283301353 [38656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024203618988394737 [38912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025072097778320312 [39168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024298757314682007 [39424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023339590057730675 [39680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024239690974354744 [39936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02282453328371048 [40192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0234203077852726 [40448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024974321946501732 [40704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024643080309033394 [40960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0229836143553257 [41216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023442575708031654 [41472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02391592413187027 [41728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024964576587080956 [41984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023336108773946762 [42240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023703597486019135 [42496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023000648245215416 [42752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02411700040102005 [43008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02310917340219021 [43264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023732716217637062 [43520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025974847376346588 [43776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02260596677660942 [44032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023303307592868805 [44288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02225455269217491 [44544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022673707455396652 [44800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024604106321930885 [45056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024260442703962326 [45312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02433290332555771 [45568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02375640720129013 [45824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02393394149839878 [46080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022895172238349915 [46336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023183133453130722 [46592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024911077693104744 [46848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022449597716331482 [47104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024043630808591843 [47360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024288760498166084 [47616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02235705778002739 [47872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02354954183101654 [48128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02512981742620468 [48384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023553458973765373 [48640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022906102240085602 [48896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02369314804673195 [49152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023234304040670395 [49408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022280192002654076 [49664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02387009747326374 [49920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022796589881181717 [50176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021787241101264954 [50432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023631978780031204 [50688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022132715210318565 [50944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02189406380057335 [51200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024733291938900948 [51456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023800084367394447 [51712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02360381931066513 [51968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021988099440932274 [52224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0227675661444664 [52480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023485708981752396 [52736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02467881329357624 [52992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02298637293279171 [53248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02418413758277893 [53504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024170469492673874 [53760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02353397198021412 [54016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023747308179736137 [54272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023382313549518585 [54528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02459772303700447 [54784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02422758936882019 [55040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023117369040846825 [55296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02225828357040882 [55552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0242654699832201 [55808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022821353748440742 [56064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023602768778800964 [56320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02305774576961994 [56576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02382681332528591 [56832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.025166351348161697 [57088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022252127528190613 [57344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02492661215364933 [57600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02464151196181774 [57856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023660676553845406 [58112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023465413600206375 [58368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02208179049193859 [58624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02357194945216179 [58880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023121142759919167 [59136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023306680843234062 [59392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024755312129855156 [59648 / 60000]\n",
            "torch.Size([96, 1, 28, 28])\n",
            "torch.Size([96, 1, 28, 28])\n",
            "Loss: 0.022899506613612175 [22464 / 60000]\n",
            "epoch: 24=-=-=-=-=-=--=\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023447932675480843 [0 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023205822333693504 [256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024325555190443993 [512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02349032275378704 [768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023509511724114418 [1024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024045879021286964 [1280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02338884398341179 [1536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02368062175810337 [1792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022763323038816452 [2048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02408507466316223 [2304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024542972445487976 [2560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02427913248538971 [2816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021865656599402428 [3072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022881953045725822 [3328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022692132741212845 [3584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023374246433377266 [3840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022304214537143707 [4096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024478986859321594 [4352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0236250851303339 [4608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024432381615042686 [4864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02339530549943447 [5120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02334900014102459 [5376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022162601351737976 [5632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024305913597345352 [5888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024428581818938255 [6144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023563196882605553 [6400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024915719404816628 [6656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021674370393157005 [6912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02210729382932186 [7168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023303702473640442 [7424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02489001862704754 [7680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024152111262083054 [7936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02282045967876911 [8192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02330300770699978 [8448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024106740951538086 [8704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02334892749786377 [8960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023321673274040222 [9216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023587582632899284 [9472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022932974621653557 [9728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023933157324790955 [9984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024732200428843498 [10240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0241989903151989 [10496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02282136119902134 [10752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023675015196204185 [11008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023549452424049377 [11264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02304738573729992 [11520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02267622761428356 [11776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023288950324058533 [12032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024231012910604477 [12288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02320130541920662 [12544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02199673466384411 [12800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023351669311523438 [13056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023351464420557022 [13312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0225676242262125 [13568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021286489441990852 [13824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022215792909264565 [14080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02206806093454361 [14336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02412177063524723 [14592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024272991344332695 [14848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023376187309622765 [15104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022760648280382156 [15360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02363794669508934 [15616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022788261994719505 [15872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02476249821484089 [16128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02349412441253662 [16384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023851336911320686 [16640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024269063025712967 [16896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022812001407146454 [17152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022679561749100685 [17408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0238661952316761 [17664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02395591139793396 [17920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023505784571170807 [18176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023815041407942772 [18432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023252412676811218 [18688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023918868973851204 [18944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024701397866010666 [19200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0245909933000803 [19456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023532459512352943 [19712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022876637056469917 [19968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021658001467585564 [20224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022812483832240105 [20480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022518722340464592 [20736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023621398955583572 [20992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023969966918230057 [21248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023757006973028183 [21504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023465856909751892 [21760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02486143819987774 [22016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024022426456212997 [22272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024017753079533577 [22528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02214423194527626 [22784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02327277697622776 [23040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022620175033807755 [23296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02280714362859726 [23552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024434980005025864 [23808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02516292594373226 [24064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023726869374513626 [24320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024359606206417084 [24576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02276449091732502 [24832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023338284343481064 [25088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023140475153923035 [25344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02366045117378235 [25600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02361954189836979 [25856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02469535544514656 [26112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022991688922047615 [26368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02340441383421421 [26624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02179826982319355 [26880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02322709560394287 [27136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02490261197090149 [27392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02329828590154648 [27648 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023318760097026825 [27904 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02332138456404209 [28160 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023543016985058784 [28416 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02385922335088253 [28672 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024684062227606773 [28928 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022707447409629822 [29184 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021693246439099312 [29440 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022947248071432114 [29696 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023778237402439117 [29952 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022938281297683716 [30208 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02346285805106163 [30464 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02286565490067005 [30720 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023095911368727684 [30976 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022371800616383553 [31232 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023367304354906082 [31488 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023128928616642952 [31744 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022084586322307587 [32000 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023205066099762917 [32256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021718496456742287 [32512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023509301245212555 [32768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022729070857167244 [33024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024182932451367378 [33280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023630505427718163 [33536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02429141476750374 [33792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02256382815539837 [34048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024024376645684242 [34304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024170702323317528 [34560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02281765080988407 [34816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024446599185466766 [35072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02471563220024109 [35328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02373463846743107 [35584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02321760356426239 [35840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023578960448503494 [36096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0234551802277565 [36352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022552719339728355 [36608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023194096982479095 [36864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02336660772562027 [37120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02421279065310955 [37376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02342776395380497 [37632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023014014586806297 [37888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024069784209132195 [38144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02318461239337921 [38400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022794192656874657 [38656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022504769265651703 [38912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02340879663825035 [39168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023330137133598328 [39424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023647882044315338 [39680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023925146088004112 [39936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023494917899370193 [40192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022527184337377548 [40448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02422088198363781 [40704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02240842394530773 [40960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023651298135519028 [41216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024103567004203796 [41472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02306324802339077 [41728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023143751546740532 [41984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02269066870212555 [42240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024500098079442978 [42496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02470909059047699 [42752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023159096017479897 [43008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023588186129927635 [43264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023080220445990562 [43520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023574432358145714 [43776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023062262684106827 [44032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023135103285312653 [44288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02373548410832882 [44544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02370769903063774 [44800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021629516035318375 [45056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02334713190793991 [45312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021913306787610054 [45568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023114696145057678 [45824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022758642211556435 [46080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02319605089724064 [46336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0239308699965477 [46592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023062366992235184 [46848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0229705311357975 [47104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022943837568163872 [47360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023331986740231514 [47616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02493598498404026 [47872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02233997918665409 [48128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02339296042919159 [48384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02371547929942608 [48640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02288808859884739 [48896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02302495390176773 [49152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023179104551672935 [49408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022935613989830017 [49664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024309540167450905 [49920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024044465273618698 [50176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0235199686139822 [50432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023490969091653824 [50688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023048073053359985 [50944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023741092532873154 [51200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022024432197213173 [51456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023259298875927925 [51712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02271091379225254 [51968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022479325532913208 [52224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02389531210064888 [52480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02145560085773468 [52736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02272256277501583 [52992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024608928710222244 [53248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021995194256305695 [53504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022462008520960808 [53760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024291295558214188 [54016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02451927773654461 [54272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023812677711248398 [54528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024059509858489037 [54784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024007301777601242 [55040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024369938299059868 [55296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023586470633745193 [55552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023126468062400818 [55808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022108973935246468 [56064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022400222718715668 [56320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022757513448596 [56576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023787127807736397 [56832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0241861529648304 [57088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023142395541071892 [57344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022656982764601707 [57600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02340368926525116 [57856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02382851578295231 [58112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022671645507216454 [58368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022652262821793556 [58624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02300897426903248 [58880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022617120295763016 [59136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02494574338197708 [59392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02349064126610756 [59648 / 60000]\n",
            "torch.Size([96, 1, 28, 28])\n",
            "torch.Size([96, 1, 28, 28])\n",
            "Loss: 0.02260638400912285 [22464 / 60000]\n",
            "epoch: 25=-=-=-=-=-=--=\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022087229415774345 [0 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023082250729203224 [256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02241598814725876 [512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024110255762934685 [768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021979093551635742 [1024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02316978946328163 [1280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02255025878548622 [1536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02275010198354721 [1792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022357866168022156 [2048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023458678275346756 [2304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023249616846442223 [2560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023530056700110435 [2816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023803887888789177 [3072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023322027176618576 [3328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023290207609534264 [3584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02301924303174019 [3840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023726271465420723 [4096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022729113698005676 [4352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0233051385730505 [4608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02295285277068615 [4864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022548483684659004 [5120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02363451197743416 [5376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02458159811794758 [5632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021596306934952736 [5888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024001972749829292 [6144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022763384506106377 [6400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02228664420545101 [6656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02363397926092148 [6912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021404191851615906 [7168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02426765486598015 [7424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023041032254695892 [7680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021939389407634735 [7936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022448008880019188 [8192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024610066786408424 [8448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022852500900626183 [8704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021917778998613358 [8960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02418517880141735 [9216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024082796648144722 [9472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02238125540316105 [9728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022598952054977417 [9984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02529696561396122 [10240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023788388818502426 [10496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023187536746263504 [10752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023025084286928177 [11008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022467825561761856 [11264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024103283882141113 [11520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022179346531629562 [11776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022645235061645508 [12032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02455161139369011 [12288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0237136073410511 [12544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022327253594994545 [12800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023928390815854073 [13056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023586267605423927 [13312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023208066821098328 [13568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02340030111372471 [13824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02359633706510067 [14080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023510068655014038 [14336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02284517139196396 [14592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0237116739153862 [14848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023580854758620262 [15104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022382451221346855 [15360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022855695337057114 [15616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024814819917082787 [15872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022981995716691017 [16128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.020832862704992294 [16384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024447765201330185 [16640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02351045049726963 [16896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0234425887465477 [17152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023903073742985725 [17408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023483021184802055 [17664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021890781819820404 [17920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022157056257128716 [18176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023387206718325615 [18432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02261730283498764 [18688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023665236309170723 [18944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023965828120708466 [19200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024395814165472984 [19456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02374725602567196 [19712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023883994668722153 [19968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023590287193655968 [20224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023510649800300598 [20480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024142488837242126 [20736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0232531800866127 [20992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022139832377433777 [21248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024584347382187843 [21504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02453838288784027 [21760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022721951827406883 [22016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02345230057835579 [22272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022427644580602646 [22528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024235526099801064 [22784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02228248305618763 [23040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02216091938316822 [23296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02424495480954647 [23552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023639313876628876 [23808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022588426247239113 [24064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022587306797504425 [24320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02254055254161358 [24576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02339514158666134 [24832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02349645271897316 [25088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02231132797896862 [25344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02266852930188179 [25600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023158740252256393 [25856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023102959617972374 [26112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0236340444535017 [26368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02361426129937172 [26624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02282728999853134 [26880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022564468905329704 [27136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022811060771346092 [27392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024349898099899292 [27648 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022921591997146606 [27904 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02304254099726677 [28160 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02329200692474842 [28416 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023031625896692276 [28672 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02195574715733528 [28928 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02288592793047428 [29184 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024509403854608536 [29440 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023149928078055382 [29696 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022760137915611267 [29952 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022586068138480186 [30208 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024142591282725334 [30464 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02323509193956852 [30720 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023496272042393684 [30976 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024023400619626045 [31232 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02351156622171402 [31488 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02234753966331482 [31744 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023556161671876907 [32000 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023940173909068108 [32256 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022357137873768806 [32512 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02333747409284115 [32768 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023699527606368065 [33024 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022973939776420593 [33280 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02332102507352829 [33536 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024291230365633965 [33792 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023010876029729843 [34048 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023497041314840317 [34304 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024496886879205704 [34560 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024398041889071465 [34816 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02298956736922264 [35072 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022026782855391502 [35328 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02286982350051403 [35584 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023532750084996223 [35840 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022427931427955627 [36096 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021985935047268867 [36352 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024006245657801628 [36608 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02325074002146721 [36864 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02291315421462059 [37120 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024257633835077286 [37376 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024197064340114594 [37632 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024317855015397072 [37888 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022951975464820862 [38144 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022374821826815605 [38400 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024140309542417526 [38656 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.020934294909238815 [38912 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02263338305056095 [39168 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022496329620480537 [39424 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02319677732884884 [39680 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022763118147850037 [39936 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022294776514172554 [40192 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02265973947942257 [40448 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023876415565609932 [40704 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023044323548674583 [40960 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023832131177186966 [41216 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021599330008029938 [41472 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02185118943452835 [41728 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023443857207894325 [41984 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022872956469655037 [42240 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02474813535809517 [42496 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02416469343006611 [42752 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022840222343802452 [43008 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023766448721289635 [43264 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023002222180366516 [43520 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023876860737800598 [43776 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023019403219223022 [44032 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02159925550222397 [44288 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023978127166628838 [44544 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021998470649123192 [44800 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024808183312416077 [45056 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022340450435876846 [45312 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023612579330801964 [45568 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022662462666630745 [45824 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02198839746415615 [46080 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022394409403204918 [46336 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023549776524305344 [46592 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02133624628186226 [46848 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02366807498037815 [47104 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022672206163406372 [47360 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022714676335453987 [47616 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02351641096174717 [47872 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021608291193842888 [48128 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023359661921858788 [48384 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02375059947371483 [48640 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022998064756393433 [48896 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021838512271642685 [49152 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021795818582177162 [49408 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021996071562170982 [49664 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0228858795017004 [49920 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022327709943056107 [50176 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02378924936056137 [50432 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022098351269960403 [50688 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022339921444654465 [50944 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02450576424598694 [51200 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022040003910660744 [51456 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024696486070752144 [51712 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023371253162622452 [51968 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024037010967731476 [52224 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02302134968340397 [52480 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021976757794618607 [52736 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023714059963822365 [52992 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02392299473285675 [53248 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024071654304862022 [53504 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02254556678235531 [53760 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023503003641963005 [54016 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023185474798083305 [54272 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023433567956089973 [54528 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.024162739515304565 [54784 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.0227535218000412 [55040 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02250935137271881 [55296 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02416110970079899 [55552 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02209779992699623 [55808 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02267915941774845 [56064 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023378310725092888 [56320 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02145083248615265 [56576 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02380983904004097 [56832 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02304091304540634 [57088 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02226620726287365 [57344 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.021907884627580643 [57600 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02258654311299324 [57856 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02336377091705799 [58112 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.02218247950077057 [58368 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022353902459144592 [58624 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.023746514692902565 [58880 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022611401975154877 [59136 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022633880376815796 [59392 / 60000]\n",
            "torch.Size([256, 1, 28, 28])\n",
            "torch.Size([256, 1, 28, 28])\n",
            "Loss: 0.022898096591234207 [59648 / 60000]\n",
            "torch.Size([96, 1, 28, 28])\n",
            "torch.Size([96, 1, 28, 28])\n",
            "Loss: 0.022001715376973152 [22464 / 60000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_ds[1]"
      ],
      "metadata": {
        "id": "YggbpRfu_GNO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.reshape(28,28))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "9UUbzZ6pDHZ1",
        "outputId": "59246f78-92e0-4303-b45f-1a5661bb235f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = auto(image)"
      ],
      "metadata": {
        "id": "7TquMW4tDMM5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(sample.reshape(28,28).detach().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "eLoRx5NADQ-r",
        "outputId": "2e758b13-1bab-4697-ca32-5fd967c6b8d3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yu_XoO8FDVtv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}